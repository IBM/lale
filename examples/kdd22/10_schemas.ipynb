{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDD 2022 Hands-on Tutorial on \"Gradual AutoML using Lale\"\n",
    "\n",
    "# 10. Schemas and Their Uses\n",
    "\n",
    "Lale operators come with JSON Schemas that describe the shape of their inputs, outputs, and hyperparameters.\n",
    "\n",
    "The [New Operators](08_newops.ipynb) notebook contains an introduction to writing these schemas.\n",
    "In this notebook, we explore how Lale uses the schemas.\n",
    "In particular, Lale uses the schemas to generate early, informative error messages, generate documentation, and generate search spaces for AutoML.\n",
    "\n",
    "This notebook has the following sections:\n",
    "\n",
    "- [10.1 Validation / Error Messages](#10.1-Validation-/-Error-Messages)\n",
    "- [10.2 Automatically Generating Documentation](#10.2-Automatically-Generating-Documentation)\n",
    "- [10.3 AutoML: Generating Search Spaces](#10.3-AutoML:-Generating-Search-Spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Validation / Error Messages\n",
    "\n",
    "Schemas are used to validate both the pipeline shape and the hyperparameters for each operator.\n",
    "\n",
    "### 10.1.1 Data Schemas\n",
    "\n",
    "Each operator comes with schemas that describe the operator's allowed input and output types.  In addition to simple static JSON schemas, Lale operators can also include a `transform_schema` method that can dynamically compute the output schema based on the actual shape of the input schema.\n",
    "Given these schemas, Lale can check that a pipeline is well-formed:  the output of each operator must be compatible with the input schema of the operator it is piped to.\n",
    "This is done using a _subschema_ entailment check.\n",
    "If one of the required entailments fails, an error can be raised, making it easy to find the problem.  More information about this check can be found in [the bibliograpy below](#Bibliography).\n",
    "\n",
    "As an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer.fit() invalid X, the schema of the actual data is not a subschema of the expected schema of the argument.\n",
      "actual_schema = {\n",
      "    \"description\": \"Features; the outer array is over samples.\",\n",
      "    \"type\": \"array\",\n",
      "    \"items\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}},\n",
      "}\n",
      "expected_schema = {\n",
      "    \"description\": \"Features; the outer array is over samples.\",\n",
      "    \"anyOf\": [\n",
      "        {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
      "        {\n",
      "            \"type\": \"array\",\n",
      "            \"items\": {\n",
      "                \"type\": \"array\",\n",
      "                \"minItems\": 1,\n",
      "                \"maxItems\": 1,\n",
      "                \"items\": {\"type\": \"string\"},\n",
      "            },\n",
      "        },\n",
      "    ],\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from lale.lib.sklearn import PCA\n",
    "from lale.lib.sklearn import TfidfVectorizer\n",
    "from lale.settings import set_disable_data_schema_validation, set_disable_hyperparams_schema_validation\n",
    "import lale.datasets.openml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# enable schema validation explicitly for the notebook, in case it was disabled for performance\n",
    "set_disable_data_schema_validation(False)\n",
    "set_disable_hyperparams_schema_validation(False)\n",
    "\n",
    "# Load sample data\n",
    "(train_X, train_y), (test_X, test_y) = lale.datasets.openml.fetch(\n",
    "    \"credit-g\", \"classification\", preprocess=True\n",
    ")\n",
    "\n",
    "# create a pipeline that tries to extract TF-IDF vector information from strings,\n",
    "# but erroneously applies it to the result of principal component analysis (which returns numbers)\n",
    "\n",
    "pipeline = PCA() >> TfidfVectorizer()\n",
    "try:\n",
    "    pipeline.fit(np.array(train_X), np.array(train_y))\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.2 Hyperparameters\n",
    "\n",
    "Schemas are also used to describe the hyperparameters for an individual operator, as well as their allowed values.\n",
    "These schemas are typically more complex than input/output schemas, and are heavily used in Lale.\n",
    "The input schema can also include data dependencies (the schema can refer to the shape of the actual data at runtime).  \n",
    "When an operator is configured, the specified hyperparameter values are checked against the schema.\n",
    "Note that Lale hyperparameter schemas often include side constraints, enabling the verification to validate that the entire hyperparameter set is valid, not just the individual values.\n",
    "Additionally, if the configuration is invalid, Lale uses the schema to automatically generate suggested fixes (alternative, similar, configurations that are valid).\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid configuration for RandomForestClassifier(bootstrap=False, oob_score=True) due to constraint out of bag estimation only available if bootstrap=True.\n",
      "Some possible fixes include:\n",
      "- set bootstrap=True\n",
      "- set oob_score=False\n",
      "Schema of failing constraint: https://lale.readthedocs.io/en/latest/modules/lale.lib.sklearn.random_forest_classifier.html#constraint-2\n",
      "Invalid value: {'bootstrap': False, 'oob_score': True, 'n_estimators': 100, 'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_weight_fraction_leaf': 0.0, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'n_jobs': None, 'random_state': None, 'verbose': 0, 'warm_start': False, 'class_weight': None, 'ccp_alpha': 0.0, 'max_samples': None}\n"
     ]
    }
   ],
   "source": [
    "from lale.lib.sklearn import RandomForestClassifier\n",
    "import jsonschema\n",
    "\n",
    "try:\n",
    "    rfc = RandomForestClassifier(bootstrap=False, oob_score=True)\n",
    "except jsonschema.ValidationError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Automatically Generating Documentation\n",
    "\n",
    "In addition to using schemas to quickly find problems, schemas are used to automatically generate documentation suitable for sphinx (and for hosting, for example, on [Read the Docs](https://lale.readthedocs.io/)).\n",
    "After an operator is created (by calling `make_operator` and/or `customize_schema`), Lale practice is to call\n",
    "`lale.docstrings.set_docstrings` on the variable the resulting operator is stored in.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "AwesomeOp = lale.operators.make_operator(\n",
    "    _AwesomeOpImpl, _combined_schemas_for_awesome_op\n",
    ")\n",
    "lale.docstrings.set_docstrings(AwesomeOp)\n",
    "```\n",
    "\n",
    "This function, when run by Sphinx (during documentation generation), changes the name (`AwesomeOp` in this example) to instead refer to a dynamically created class with documentation for the `AwesomeOp` operator.\n",
    "This class is created by a compilation process that analyzes the schemas and creates methods as appropriate.\n",
    "For example, the compilation process creates a fake `__init__` method with arguments corresponding to all of the hyperparameters described in the schema, with appropriate descriptions/types.\n",
    "Similarly, if a `fit` method is available, the fit input schema is translated into appropriate documentation for the dynamically created `fit` method, and so on for the other methods.\n",
    "Note that these methods have stubs for their implementations; they exist simply to enable Sphinx to generate appropriate documentation for them.\n",
    "When not running under Sphinx, the call to `lale.docstrings.set_docstrings` has no effect.\n",
    "\n",
    "As an example, the automatically generated documentation for [PCA](https://lale.readthedocs.io/en/latest/modules/lale.lib.sklearn.pca.html) includes: \n",
    "\n",
    "<img src=\"readthedocs_pca.png\" width=\"600\" />\n",
    "\n",
    "Note how the constructor arguments for the generated documented PCA class reflect the schema's hyperparameters, along with defaults if specified.  The description of the hyperparameters is compiled from the schema.  In the example shown, `n_components` is seen to allow four different types of values, each of which is clearly explained.  It also cross-references to the side-constraints that mention `n_components`, which are included further down in the documentation.\n",
    "\n",
    "Similarly, `fit` and `predict` methods are documented, along with compiled information about the shape of their input and output, and any fit parameters (additional arguments that the operator supports passing to the `fit` method).\n",
    "The `description` strings in the schemas are leveraged throughout to provide useful descriptions to the user.\n",
    "\n",
    "Automatically generating information in this fashion ensures that the documentation and the code stay synchronized, and that we are validating exactly what is specified in the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 AutoML: Generating Search Spaces\n",
    "\n",
    "The most sophisticated use Lale makes of schemas is to automatically generate search spaces from the hyperparameter schemas of the operators in a pipeline.\n",
    "This enables AutoML to automatically tune the (unconfigured) hyperparameters of the operators in a pipeline.\n",
    "It also works in conjunction with selecting from different choices encoded in the pipeline using the Lale choice combinator (`|`).\n",
    "The planned pipeline is encoded into an optimizer search space via a compilation process.\n",
    "\n",
    "Each operator schema is turned into a simplified representation that is easier to communicate to search space optimizers.\n",
    "Explicitly configured hyperparameter values are taken into account.\n",
    "This simplified form for each operator is combined into a bigger search space specification that encodes the shape of the pipeline (and the choices it may embed).\n",
    "This representation is then compiled into the form required by the specified backend (such as hyperopt, smac, or grid search).  For example, for scikit-learn's [GridSearchCV](https://lale.readthedocs.io/en/latest/modules/lale.lib.lale.grid_search_cv.html#lale.lib.lale.grid_search_cv.GridSearchCV), a list of grids is produced (with a discretized representation of the encoded search space).\n",
    "\n",
    "Each time the specified AutoML optimizer samples a point in the search space, Lale runs a reverse compilation to decode that point into a trainable pipeline.\n",
    "It fits that pipeline to obtain a trained pipeline, and evaluates that to obtain metrics, which certain optimizers (such as [Hyperopt](https://lale.readthedocs.io/en/latest/modules/lale.lib.lale.hyperopt.html#lale.lib.lale.hyperopt.Hyperopt)) take into consideration when sampling the next point in the search space. \n",
    "\n",
    "<img src=\"workflow_enc_dec.png\" width=\"360\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "The following paper has more information about subschema entailment checks:\n",
    "\n",
    "```bibtex\n",
    "@InProceedings{habib_et_al_2021,\n",
    "  title = \"Finding Data Compatibility Bugs with {JSON} Subschema Checking\",\n",
    "  author = \"Habib, Andrew and Shinnar, Avraham and Hirzel, Martin and Pradel, Michael\",\n",
    "  booktitle = \"International Symposium on Software Testing and Analysis (ISSTA)\",\n",
    "  year = 2021,\n",
    "  pages = \"620--632\",\n",
    "  url = \"https://doi.org/10.1145/3460319.3464796\" }\n",
    "```\n",
    "\n",
    "The following paper has more information about how Lale generated search spaces:\n",
    "\n",
    "```bibtex\n",
    "@InProceedings{baudart_et_al_2021,\n",
    "  title = \"Pipeline Combinators for Gradual {AutoML}\",\n",
    "  author = \"Baudart, Guillaume and Hirzel, Martin and Kate, Kiran and Ram, Parikshit and Shinnar, Avraham and Tsay, Jason\",\n",
    "  booktitle = \"Advances in Neural Information Processing Systems (NeurIPS)\",\n",
    "  year = 2021,\n",
    "  url = \"https://proceedings.neurips.cc/paper/2021/file/a3b36cb25e2e0b93b5f334ffb4e4064e-Paper.pdf\" }\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
