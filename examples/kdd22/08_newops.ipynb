{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDD 2022 Hands-on Tutorial on \"Gradual AutoML using Lale\"\n",
    "\n",
    "# 8. Add a New Operator\n",
    "\n",
    "Lale comes with several library operators, so you do not need to write\n",
    "your own. But if you want to use Lale for AutoML that includes your custom models,\n",
    "you will need to create a new Lale operator that wraps the custom model.\n",
    "\n",
    "This tutorial illustrates this using ResNet-50 for image classification as a running example.\n",
    "The following four steps highlight the process to add a new operator:\n",
    "\n",
    "-   [8.1 Create Implementation Class](#impl)\n",
    "-   [8.2 Add Hyperparameter Schema](#hyperparam_schema)\n",
    "-   [8.3 Register a New Lale Operator](#make_operator)\n",
    "-   [8.4 Test and Use the New operator](#use_operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"impl\"></a>\n",
    "## 8.1 Create Implementation Class\n",
    "\n",
    "The implementation class of an operator needs to have methods `__init__`,\n",
    "`fit`, and `predict` or `transform`. Any other compatibility with\n",
    "scikit-learn such as `get_params` or `set_params` is optional, and so\n",
    "is extending from `sklearn.base.BaseEstimator`.\n",
    "\n",
    "This section illustrates how to implement this class for our new operator `ResNet50`.\n",
    "We call this class `_ResNet50Impl` and it uses  ResNet-50 implementation available in the torchvision library (`torchvision.models.resnet50`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://kakate%40us.ibm.com:****@na.artifactory.swg-devops.com/artifactory/api/pypi/wcp-nlp-pypi-virtual/simple\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/site-packages (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (from torch) (1.19.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 22.2 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://kakate%40us.ibm.com:****@na.artifactory.swg-devops.com/artifactory/api/pypi/wcp-nlp-pypi-virtual/simple\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/site-packages (0.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/site-packages (from torchvision) (7.2.0)\n",
      "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/site-packages (from torchvision) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/site-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 22.2 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Install dependencies required by this tutorial\n",
    "!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "class _ResNet50Impl():\n",
    "    \n",
    "    #__init__ that takes the hyperparameters as keyword arguments.\n",
    "    def __init__(self, num_classes=10,\n",
    "            num_epochs = 2, batch_size = 128, learning_rate_init=0.1,\n",
    "            lr_scheduler = 'constant'):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = resnet50(num_classes).to(self.device)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        \"\"\"Fit method for ResNet50.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Pytorch Dataset object.\n",
    "          Pytorch dataset that contains the training data and targets.\n",
    "        y : optional\n",
    "          This is ignored.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ResNet50Impl\n",
    "          A new object that is trained.\n",
    "        \"\"\"\n",
    "        trainloader = torch.utils.data.DataLoader(X, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "        self.model.train()\n",
    "        \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "            \n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for epoch in range(self.num_epochs):\n",
    "            learning_rate = self.calculate_learning_rate(epoch)\n",
    "            optimizer = optim.SGD(self.model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "            print('\\n=> Training Epoch %d, LR=%.4f' %(epoch, learning_rate))\n",
    "            for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(inputs)         \n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.data.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "                \n",
    "                print('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
    "                        %(epoch, self.num_epochs, batch_idx+1,\n",
    "                            (len(X)//self.batch_size)+1, loss.data.item(), 100.*correct/total))\n",
    "\n",
    "        return _ResNet50Impl(self.num_classes, self.model, self.num_epochs, self.batch_size,\n",
    "                            self.lr_scheduler) \n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        dataloader = torch.utils.data.DataLoader(X, batch_size=self.batch_size, shuffle=False, num_workers=2)\n",
    "        predicted_X = None\n",
    "        for batch_idx, data in enumerate(dataloader):\n",
    "            if isinstance(data, list) or isinstance(data, tuple):\n",
    "                inputs = data[0] #For standard datasets from torchvision, data is a list with X and y\n",
    "            else:\n",
    "                inputs = data\n",
    "            inputs = inputs.to(self.device)\n",
    "            outputs = self.model(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            predicted = predicted.detach().cpu().numpy()\n",
    "            predicted = np.reshape(predicted, (predicted.shape[0], 1))\n",
    "            if predicted_X is None:\n",
    "                predicted_X = predicted\n",
    "            else:\n",
    "                predicted_X = np.vstack((predicted_X, predicted))\n",
    "        self.model.train()\n",
    "        return predicted_X\n",
    "\n",
    "    def calculate_learning_rate(self, epoch):\n",
    "        import math\n",
    "        if self.lr_scheduler == 'constant':\n",
    "            return self.learning_rate_init\n",
    "        elif self.lr_scheduler == 'decay':\n",
    "            optim_factor = 0\n",
    "            if(epoch > 160):\n",
    "                optim_factor = 3\n",
    "            elif(epoch > 120):\n",
    "                optim_factor = 2\n",
    "            elif(epoch > 60):\n",
    "                optim_factor = 1\n",
    "\n",
    "            return self.learning_rate_init*math.pow(0.2, optim_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code first imports the relevant torchvision and torch packages. Then, it declares\n",
    "a new class for wrapping `torchvision.models.resnet50`. The\n",
    "Lale approach for wrapping new operators carefully avoids depending too much\n",
    "on the Python language or any particular Python library. Hence, the\n",
    "`_ResNet50Impl` class does not need to inherit from anything, but it does need to\n",
    "follow certain conventions:\n",
    "\n",
    "* It has a constructor, `__init__`, whose arguments are the\n",
    "  hyperparameters as keyword arguments.\n",
    "\n",
    "* It has a training method, `fit`, with an argument `X` containing the\n",
    "  training examples and, in the case of supervised models, an argument `y`\n",
    "  containing labels. Since the torch Dataset has both data and targets for this case,\n",
    "  the default value of y is None and it is ignored during training.\n",
    "  The `fit` method trains the neural network by looping for epochs and batches, \n",
    "  and returns the wrapper object with the trained model.\n",
    "\n",
    "* It has a prediction method, `predict` for an estimator or `transform` for\n",
    "  a transformer. The method has an argument `X` containing the test examples\n",
    "  and returns the labels for `predict` or the transformed data for\n",
    "  `transform`.\n",
    "\n",
    "These conventions are designed to be similar to those of scikit-learn.\n",
    "However, they avoid a code dependency upon scikit-learn.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"hyperparam_schema\"></a>\n",
    "## 8.2 Add Hyperparameter Schema\n",
    "\n",
    "Lale uses schemas both for error-checking and for generating search\n",
    "spaces for hyperparameter optimization.\n",
    "The schemas of a Lale operator specify the space of valid values for\n",
    "hyperparameters, for the arguments to `fit` and `predict` or `transform`,\n",
    "and for the output of `predict` or `transform`. All of these schemas are optional.\n",
    "\n",
    "In this section, we will focus on creating a hyperparameter schema for our ResNet50 operator.\n",
    "To keep the schemas independent of the Python programming language, they are expressed as\n",
    "[JSON Schema](https://json-schema.org/understanding-json-schema/reference/).\n",
    "JSON Schema is currently a draft standard and is already being widely\n",
    "adopted and implemented, for instance, as part of specifying\n",
    "[Swagger APIs](https://www.openapis.org/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (744100010.py, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [3]\u001b[0;36m\u001b[0m\n\u001b[0;31m    'maximumForOptimizer': 128},\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "_hyperparams_schema = {\n",
    "  '$schema': 'http://json-schema.org/draft-04/schema#',\n",
    "  'description': 'Hyperparameter schema.',\n",
    "  'allOf': [\n",
    "    { 'description':\n",
    "        'This first sub-object lists all constructor arguments with their '\n",
    "        'types, one at a time, omitting cross-argument constraints.',\n",
    "      'type': 'object',\n",
    "      'additionalProperties': False,\n",
    "      'required': ['num_classes', 'num_epochs',\n",
    "       'batch_size', 'lr_scheduler'],\n",
    "      'relevantToOptimizer': ['num_epochs', 'batch_size', 'lr_scheduler'],\n",
    "      'properties': {\n",
    "        'num_classes': {\n",
    "          'description': 'Number of classes.',\n",
    "          'type': 'integer',\n",
    "          'default': 10,\n",
    "          'minimum': 2},\n",
    "        'num_epochs':{\n",
    "          'description': 'The number of epochs used for training.',\n",
    "          'type': 'integer',\n",
    "          'default': 2,\n",
    "          'minimum': 0,\n",
    "          'distribution': 'uniform',\n",
    "          'minimumForOptimizer': 2,\n",
    "          'maximumForOptimizer': 200},\n",
    "        'batch_size':{\n",
    "          'description': 'Batch size used for training and prediction.',\n",
    "          'type': 'integer',\n",
    "          'default': 64,\n",
    "          'minimum': 1,\n",
    "          'distribution': 'uniform',\n",
    "          'minimumForOptimizer': 32\n",
    "          'maximumForOptimizer': 128},\n",
    "        'lr_scheduler':{\n",
    "          'description': 'Learning rate scheduler for training.',\n",
    "          'enum': ['constant', 'decay'],\n",
    "          'default': 'constant',\n",
    "        },           \n",
    "        'learning_rate_init':{\n",
    "          'description': 'Initial value of learning rate for training.',\n",
    "          'type': 'number',\n",
    "          'default': 0.1,\n",
    "          'minimum': 0,\n",
    "          'distribution': 'loguniform',\n",
    "          'minimumForOptimizer': 1e-05,\n",
    "          'maximumForOptimizer': 0.1\n",
    "        }}}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `num_classes`, `num_epochs` and `batch_size` are integer hyperparameters, `lr_scheduler` is a\n",
    "categorical hyperparameter, and `learning_rate_init` is a float. \n",
    "For all the hyperparameters, the schema\n",
    "includes a `type` which indicates the data type, \n",
    "`description` which is used for interactive documentation, and a\n",
    "`default` value. Note that the JSON Schema type for float is `number`. \n",
    "For integer and float hyperparameters, a `minimum` value is specified.\n",
    "The categorical hyperparameter is specified as an enumeration of its allowed values.\n",
    "\n",
    "The schema has a list called `relevantToOptimizer` which includes hyperparameters to be tuned\n",
    "during AutoML.\n",
    "For such hyperparameters, the schema\n",
    "includes additional information such as its `distribution`, `minimumForOptimizer`, and\n",
    "`maximumForOptimizer` guiding the optimizer to limit its search space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"make_operator\"></a>\n",
    "## 8.3 Register a New Lale Operator\n",
    "\n",
    "We can now register `_ResNet50Impl` as a new Lale operator `ResNet50`. This is done by calling `make_operator` and passing the implementation class and a dictionary with information such as the `description` of the operator, optional `tags` which additional information about what kind of operator it is, and the hyperparameter schema given by key `hyperparams`. The other schemas such as `input_fit`, `input_predict`, `output_fit`, and `output_predict` specify data types of input/output of fit and predict. We will keep those empty for this tutorial, but `lale.lib` has many examples to refer to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lale.operators import make_operator\n",
    "\n",
    "_combined_schemas = {\n",
    "  '$schema': 'http://json-schema.org/draft-04/schema#',\n",
    "  'description': 'Combined schema for expected data and hyperparameters for a transformer for'\n",
    "                ' pytorch implementation of ResNet50 for image classification.',\n",
    "  'type': 'object',\n",
    "  'tags': {'pre': ['images'], 'op': ['estimator', 'classifier'], 'post': []},\n",
    "  'properties': {\n",
    "    'hyperparams': _hyperparams_schema,\n",
    "    'input_fit': {},\n",
    "    'input_predict': {},\n",
    "    'output_fit': {},\n",
    "    'output_predict':{}\n",
    "     } }\n",
    "ResNet50 = make_operator(_ResNet50Impl, _combined_schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"use_operator\"></a>\n",
    "## 8.4 Testing and Using the New Operator\n",
    "\n",
    "Once your operator implementation and schema definitions are ready,\n",
    "you can test it with Lale as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the new Operator\n",
    "\n",
    "Before demonstrating the new `ResNet50` operator, the following code loads the\n",
    "CIFAR10 dataset from torchvision.datasets. The tutorial uses only the first 1000 rows for training \n",
    "and 500 rows for test to speed-up the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[n/255.\n",
    "                    for n in [129.3, 124.1, 112.4]], std=[n/255. for n in [68.2,  65.4,  70.4]])\n",
    "]) # meanstd transformation\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[n/255.\n",
    "                    for n in [129.3, 124.1, 112.4]], std=[n/255. for n in [68.2,  65.4,  70.4]])\n",
    "])\n",
    "\n",
    "data_train = datasets.CIFAR10(root = \"/tmp/\", download = True, transform = transform_train)\n",
    "data_train.data = data_train.data[0:1000, :]\n",
    "data_train.targets = data_train.targets[0:1000]\n",
    "\n",
    "data_test = datasets.CIFAR10(root = \"/tmp/\", download = True, train = False, transform = transform_test)\n",
    "data_test.data = data_test.data[0:500, :]\n",
    "data_test.targets = data_test.targets[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate the ResNet50 operator with `num_classes` as 10 and `num_epochs` as 1 and call `fit` to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ResNet50(num_classes=10,num_epochs = 1)\n",
    "fitted_clf = clf.fit(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `predict` to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = fitted_clf.predict(data_test)\n",
    "print(predicted[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lale.lib.lale import Hyperopt\n",
    "\n",
    "opt = Hyperopt(estimator=clf, max_evals=10, cv= None, verbose=True)\n",
    "opt.fit(data_train, y=None, X_valid=data_test, y_valid=data_test.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, we have learned how to write an operator implementation class and hyperparameter\n",
    "schema; how to register the Lale operator; and how to use the Lale operator for\n",
    "manual as well as automated machine-learning.\n",
    "\n",
    "The next section has more details on options in JSON Schema and can be used as a reference when creating Lale schemas.\n",
    "\n",
    "\n",
    "## 8.5 Appendix\n",
    "\n",
    "This section documents features of JSON Schema that Lale uses, as well as\n",
    "extensions that Lale adds to JSON schema for information specific to the\n",
    "machine-learning domain. For a more comprehensive introduction to JSON\n",
    "Schema, refer to its\n",
    "[Reference](https://json-schema.org/understanding-json-schema/reference/).\n",
    "\n",
    "The following table lists kinds of schemas in JSON Schema:\n",
    "\n",
    "| Kind of schema | Corresponding type in Python/Lale |\n",
    "| ---------------| ---------------------------- |\n",
    "| `null`         | `NoneType`, value `None` |\n",
    "| `boolean`      | `bool`, values `True` or `False` |\n",
    "| `string`       | `str` |\n",
    "| `enum`         | See discussion below. |\n",
    "| `number`       | `float`, .e.g, `0.1` |\n",
    "| `integer`      | `int`, e.g., `42` |\n",
    "| `array`        | See discussion below. |\n",
    "| `object`       | `dict` with string keys |\n",
    "| `anyOf`, `allOf`, `not` | See discussion below. |\n",
    "\n",
    "The use of `null`, `boolean`, and `string` is fairly straightforward.  The\n",
    "following paragraphs discuss the other kinds of schemas one by one.\n",
    "\n",
    "### 8.5.1 enum\n",
    "\n",
    "In JSON Schema, an enum can contain assorted values including strings,\n",
    "numbers, or even `null`. Lale uses enums of strings for categorical\n",
    "hyperparameters, such as `'lr_scheduler': {'enum': ['constant', 'decay']}` in the earlier\n",
    "example. In that case, Lale also automatically declares a corresponding\n",
    "Python `enum`.\n",
    "When Lale uses enums of other types, it is usually to restrict a\n",
    "hyperparameter to a single value, such as `'enum': [None]`.\n",
    "\n",
    "### 8.5.2 number, integer\n",
    "\n",
    "In schemas with `type` set to `number` or `integer`, JSON schema lets users\n",
    "specify `minimum`, `maximum`,\n",
    "`exclusiveMinimum`, and `exclusiveMaximum`. Lale further extends JSON schema\n",
    "with `minimumForOptimizer`, `maximumForOptimizer`, and `distribution`.\n",
    "Possible values for the `distribution` are `'uniform'` (the default) and\n",
    "`'loguniform'`. In the case of integers, Lale quantizes the distributions\n",
    "accordingly.\n",
    "\n",
    "### 8.5.3 array\n",
    "\n",
    "Lale schemas for input and output data make heavy use of the JSON Schema\n",
    "`array` type. In this case, Lale schemas are intended to capture logical\n",
    "schemas, not physical representations, similar to how relational databases\n",
    "hide physical representations behind a well-formalized abstraction layer.\n",
    "Therefore, Lale uses arrays from JSON Schema for several types in Python.\n",
    "The most obvious one is a Python `list`. Another common one is a numpy\n",
    "[ndarray](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html),\n",
    "where Lale uses nested arrays to represent each of the dimensions of a\n",
    "multi-dimensional array. Lale also has support for `pandas.DataFrame` and\n",
    "`pandas.Series`, for which it again uses JSON Schema arrays.\n",
    "\n",
    "For arrays, JSON schema lets users specify `items`, `minItems`, and\n",
    "`maxItems`. Lale further extends JSON schema with `minItemsForOptimizer` and\n",
    "`maxItemsForOptimizer`. Furthermore, Lale supports a `laleType`,\n",
    "which can be `'Any'` to locally disable a subschema check, or`'tuple'` to support cases where the Python code requires a\n",
    "tuple instead of a list.\n",
    "\n",
    "### 8.5.4 object\n",
    "\n",
    "For objects, JSON schema lets users specify a list `required` of properties\n",
    "that must be present, a dictionary `properties` of sub-schemas, and a flag\n",
    "`additionalProperties` to indicate whether the object can have additional\n",
    "properties beyond the keys of the `properties` dictionary. Lale further\n",
    "extends JSON schema with a `relevantToOptimizer` list of properties that\n",
    "hyperparameter optimizers should search over.\n",
    "\n",
    "For individual properties, Lale supports a `default`, which is inspired by\n",
    "and consistent with web API specification practice. It also supports a\n",
    "`forOptimizer` flag which defaults to `True` but can be set to `False` to\n",
    "hide a particular subschema from the hyperparameter optimizer. For example,\n",
    "the number of components for PCA in scikit-learn can be specified as an\n",
    "integer or a floating point number, but an optimizer should only explore one\n",
    "of these choices. Lale supports a Boolean flag `transient` that, if true,\n",
    "elides a hyperparameter during pretty-printing, visualization, or in JSON.\n",
    "\n",
    "### 8.5.5 allOf, anyOf, not\n",
    "\n",
    "In JSON schema, `allOf` is a logical \"and\", `anyOf` is\n",
    "a logical \"or\", and `not` is a logical negation. The running example from\n",
    "earlier illustrated `allOf` at the top level in the hyperparameter schema.\n",
    "A use-case that takes advantage of `anyOf` is for\n",
    "expressing union types, which arise frequently in scikit-learn. For example,\n",
    "here is the schema for `n_components` from PCA:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"n_components\": {\n",
    "    \"anyOf\": [\n",
    "        {\n",
    "            \"description\": \"If not set, keep all components.\",\n",
    "            \"enum\": [None],\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"Use Minka's MLE to guess the dimension.\",\n",
    "            \"enum\": [\"mle\"],\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"\"\"Select the number of components such that the amount of variance that needs to be explained is greater than the specified percentage.\"\"\",\n",
    "            \"type\": \"number\",\n",
    "            \"minimum\": 0.0,\n",
    "            \"exclusiveMinimum\": True,\n",
    "            \"maximum\": 1.0,\n",
    "            \"exclusiveMaximum\": True,\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"Number of components to keep.\",\n",
    "            \"type\": \"integer\",\n",
    "            \"minimum\": 1,\n",
    "            \"laleMaximum\": \"X/items/maxItems\",  # number of columns\n",
    "            \"forOptimizer\": False,\n",
    "        },\n",
    "    ],\n",
    "    \"default\": None,\n",
    "},"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lale39",
   "language": "python",
   "name": "lale39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
