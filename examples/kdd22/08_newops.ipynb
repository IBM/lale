{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDD 2022 Hands-on Tutorial on \"Gradual AutoML using Lale\"\n",
    "\n",
    "# 8. Add a New Operator\n",
    "\n",
    "Lale comes with several library operators, so you do not need to write\n",
    "your own. But if you want to use Lale for AutoML that includes your custom models,\n",
    "you will need to create a new Lale operator that wraps the custom model.\n",
    "\n",
    "This tutorial illustrates this using ResNet-50 for image classification as a running example.\n",
    "The following four steps highlight the process to add a new operator:\n",
    "\n",
    "-   [8.1 Create Implementation Class](#impl)\n",
    "-   [8.2 Add Hyperparameter Schema](#hyperparam_schema)\n",
    "-   [8.3 Register a New Lale Operator](#make_operator)\n",
    "-   [8.4 Test and Use the New operator](#use_operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"impl\"></a>\n",
    "## 8.1 Create Implementation Class\n",
    "\n",
    "The implementation class of an operator needs to have methods `__init__`,\n",
    "`fit`, and `predict` or `transform`. Any other compatibility with\n",
    "scikit-learn such as `get_params` or `set_params` is optional, and so\n",
    "is extending from `sklearn.base.BaseEstimator`.\n",
    "\n",
    "This section illustrates how to implement this class for our new operator `ResNet50`.\n",
    "We call this class `_ResNet50Impl` and it uses  ResNet-50 implementation available in the torchvision library (`torchvision.models.resnet50`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://kakate%40us.ibm.com:****@na.artifactory.swg-devops.com/artifactory/api/pypi/wcp-nlp-pypi-virtual/simple\n",
      "Requirement already satisfied: torch in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (1.12.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from torch) (4.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/Users/kakateus.ibm.com/venv/lale_tutorial/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://kakate%40us.ibm.com:****@na.artifactory.swg-devops.com/artifactory/api/pypi/wcp-nlp-pypi-virtual/simple\n",
      "Requirement already satisfied: torchvision in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (0.13.1)\n",
      "Requirement already satisfied: requests in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: numpy in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from torchvision) (1.23.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from torchvision) (4.3.0)\n",
      "Requirement already satisfied: torch==1.12.1 in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from torchvision) (1.12.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from requests->torchvision) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from requests->torchvision) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from torchvision) (4.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/Users/kakateus.ibm.com/venv/lale_tutorial/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Install dependencies required by this tutorial\n",
    "!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "class _ResNet50Impl():\n",
    "    \n",
    "    #__init__ that takes the hyperparameters as keyword arguments.\n",
    "    def __init__(self, num_classes=10,\n",
    "            num_epochs = 2, batch_size = 128, learning_rate_init=0.1,\n",
    "            lr_scheduler = 'constant'):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1).to(self.device)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        \"\"\"Fit method for ResNet50.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Pytorch Dataset object.\n",
    "          Pytorch dataset that contains the training data and targets.\n",
    "        y : optional\n",
    "          This is ignored.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ResNet50Impl\n",
    "          A new object that is trained.\n",
    "        \"\"\"\n",
    "        trainloader = torch.utils.data.DataLoader(X, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "        self.model.train()\n",
    "        \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "            \n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for epoch in range(self.num_epochs):\n",
    "            learning_rate = self.calculate_learning_rate(epoch)\n",
    "            optimizer = optim.SGD(self.model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "            print('\\n=> Training Epoch %d, LR=%.4f' %(epoch, learning_rate))\n",
    "            for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(inputs)         \n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.data.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "                \n",
    "                print('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
    "                        %(epoch, self.num_epochs, batch_idx+1,\n",
    "                            (len(X)//self.batch_size)+1, loss.data.item(), 100.*correct/total))\n",
    "\n",
    "        return _ResNet50Impl(self.num_classes, self.model, self.num_epochs, self.batch_size,\n",
    "                            self.lr_scheduler) \n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        dataloader = torch.utils.data.DataLoader(X, batch_size=self.batch_size, shuffle=False, num_workers=2)\n",
    "        predicted_X = None\n",
    "        for batch_idx, data in enumerate(dataloader):\n",
    "            if isinstance(data, list) or isinstance(data, tuple):\n",
    "                inputs = data[0] #For standard datasets from torchvision, data is a list with X and y\n",
    "            else:\n",
    "                inputs = data\n",
    "            inputs = inputs.to(self.device)\n",
    "            outputs = self.model(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            predicted = predicted.detach().cpu().numpy()\n",
    "            predicted = np.reshape(predicted, (predicted.shape[0], 1))\n",
    "            if predicted_X is None:\n",
    "                predicted_X = predicted\n",
    "            else:\n",
    "                predicted_X = np.vstack((predicted_X, predicted))\n",
    "        self.model.train()\n",
    "        return predicted_X\n",
    "\n",
    "    def calculate_learning_rate(self, epoch):\n",
    "        import math\n",
    "        if self.lr_scheduler == 'constant':\n",
    "            return self.learning_rate_init\n",
    "        elif self.lr_scheduler == 'decay':\n",
    "            optim_factor = 0\n",
    "            if(epoch > 160):\n",
    "                optim_factor = 3\n",
    "            elif(epoch > 120):\n",
    "                optim_factor = 2\n",
    "            elif(epoch > 60):\n",
    "                optim_factor = 1\n",
    "\n",
    "            return self.learning_rate_init*math.pow(0.2, optim_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code first imports the relevant torchvision and torch packages. Then, it declares\n",
    "a new class for wrapping `torchvision.models.resnet50`. The\n",
    "Lale approach for wrapping new operators carefully avoids depending too much\n",
    "on the Python language or any particular Python library. Hence, the\n",
    "`_ResNet50Impl` class does not need to inherit from anything, but it does need to\n",
    "follow certain conventions:\n",
    "\n",
    "* It has a constructor, `__init__`, whose arguments are the\n",
    "  hyperparameters as keyword arguments.\n",
    "\n",
    "* It has a training method, `fit`, with an argument `X` containing the\n",
    "  training examples and, in the case of supervised models, an argument `y`\n",
    "  containing labels. Since the torch Dataset has both data and targets for this case,\n",
    "  the default value of y is None and it is ignored during training.\n",
    "  The `fit` method trains the neural network by looping for epochs and batches, \n",
    "  and returns the wrapper object with the trained model.\n",
    "\n",
    "* It has a prediction method, `predict` for an estimator or `transform` for\n",
    "  a transformer. The method has an argument `X` containing the test examples\n",
    "  and returns the labels for `predict` or the transformed data for\n",
    "  `transform`.\n",
    "\n",
    "These conventions are designed to be similar to those of scikit-learn.\n",
    "However, they avoid a code dependency upon scikit-learn.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"hyperparam_schema\"></a>\n",
    "## 8.2 Add Hyperparameter Schema\n",
    "\n",
    "Lale uses hyperparameter schemas both for error-checking and for generating search\n",
    "spaces for hyperparameter optimization.\n",
    "A hyperparameter schema specifies the space of valid types and values for\n",
    "hyperparameters and also the ranges for hyperparameter optimization.\n",
    "Schemas are also used to specify types for the arguments to `fit` and `predict` or `transform`,\n",
    "and for the output of `predict` or `transform`. All the schemas are optional,\n",
    "and Lale will skip the corresponding functionality such as error checking or search\n",
    "space generation if they are not specified.\n",
    "For meaningful hyperparameter tuning, a Lale operator needs a hyperparameter schema at the minimum.\n",
    "\n",
    "\n",
    "In this section, we will focus on creating a hyperparameter schema for our ResNet50 operator.\n",
    "To keep the schemas independent of the Python programming language, they are expressed as\n",
    "[JSON Schema](https://json-schema.org/understanding-json-schema/reference/).\n",
    "JSON Schema is currently a draft standard and is already being widely\n",
    "adopted and implemented, for instance, as part of specifying\n",
    "[Swagger APIs](https://www.openapis.org/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_hyperparams_schema = {\n",
    "  '$schema': 'http://json-schema.org/draft-04/schema#',\n",
    "  'description': 'Hyperparameter schema.',\n",
    "  'allOf': [\n",
    "    { 'description':\n",
    "        'This first sub-object lists all constructor arguments with their '\n",
    "        'types, one at a time, omitting cross-argument constraints.',\n",
    "      'type': 'object',\n",
    "      'additionalProperties': False,\n",
    "      'required': ['num_classes', 'num_epochs',\n",
    "       'batch_size', 'lr_scheduler', 'learning_rate_init'],\n",
    "      'relevantToOptimizer': ['num_epochs', 'batch_size', 'lr_scheduler'],\n",
    "      'properties': {\n",
    "        'num_classes': {\n",
    "          'description': 'Number of classes.',\n",
    "          'type': 'integer',\n",
    "          'default': 10,\n",
    "          'minimum': 2},\n",
    "        'num_epochs':{\n",
    "          'description': 'The number of epochs used for training.',\n",
    "          'type': 'integer',\n",
    "          'default': 2,\n",
    "          'minimum': 0,\n",
    "          'distribution': 'uniform',\n",
    "          'minimumForOptimizer': 2,\n",
    "          'maximumForOptimizer': 200},\n",
    "        'batch_size':{\n",
    "          'description': 'Batch size used for training and prediction.',\n",
    "          'type': 'integer',\n",
    "          'default': 64,\n",
    "          'minimum': 1,\n",
    "          'distribution': 'uniform',\n",
    "          'minimumForOptimizer': 32,\n",
    "          'maximumForOptimizer': 128},\n",
    "        'lr_scheduler':{\n",
    "          'description': 'Learning rate scheduler for training.',\n",
    "          'enum': ['constant', 'decay'],\n",
    "          'default': 'constant'},           \n",
    "        'learning_rate_init':{\n",
    "          'description': 'Initial value of learning rate for training.',\n",
    "          'type': 'number',\n",
    "          'default': 0.1,\n",
    "          'minimum': 0,\n",
    "          'distribution': 'loguniform',\n",
    "          'minimumForOptimizer': 1e-05,\n",
    "          'maximumForOptimizer': 0.1\n",
    "        }}}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `num_classes`, `num_epochs` and `batch_size` are integer hyperparameters, `lr_scheduler` is a\n",
    "categorical hyperparameter, and `learning_rate_init` is a float. \n",
    "For all the hyperparameters, the schema\n",
    "includes a `type` which indicates the data type, \n",
    "`description` which is used for interactive documentation, and a\n",
    "`default` value. Note that the JSON Schema type for float is `number`. \n",
    "For integer and float hyperparameters, a `minimum` value is specified.\n",
    "The categorical hyperparameter is specified as an enumeration of its allowed values.\n",
    "\n",
    "The schema has a list called `relevantToOptimizer` which includes hyperparameters to be tuned\n",
    "during AutoML.\n",
    "For such hyperparameters, the schema\n",
    "includes additional information such as its `distribution`, `minimumForOptimizer`, and\n",
    "`maximumForOptimizer` guiding the optimizer to limit its search space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"make_operator\"></a>\n",
    "## 8.3 Register a New Lale Operator\n",
    "\n",
    "We can now register `_ResNet50Impl` as a new Lale operator `ResNet50`. This is done by calling `make_operator` and passing the implementation class and a dictionary with information such as the `description` of the operator, `tags` which include additional information about what kind of operator it is, and the hyperparameter schema given by key `hyperparams`. For `tags`, it is required to have a key `op` indicating whether it is an `estimator` or `transformer` per scikit-learn terminology. Information such as `classifier` or `regressor` is encouraged but optional and `pre` and `post` are optional as well. Please refer to `lale.lib` for more examples of `tags`.\n",
    "\n",
    "As mentioned earlier, the other schemas such as `input_fit`, `input_predict`, `output_fit`, and `output_predict` specify data types of input/output of fit and predict. We will omit those for this tutorial, but `lale.lib` has many examples to refer to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lale.operators import make_operator\n",
    "\n",
    "_combined_schemas = {\n",
    "  '$schema': 'http://json-schema.org/draft-04/schema#',\n",
    "  'description': 'Combined schema for expected data and hyperparameters for a transformer for'\n",
    "                ' pytorch implementation of ResNet50 for image classification.',\n",
    "  'type': 'object',\n",
    "  'tags': {'pre': ['images'], 'op': ['estimator', 'classifier'], 'post': []},\n",
    "  'properties': {\n",
    "    'hyperparams': _hyperparams_schema}}\n",
    "\n",
    "ResNet50 = make_operator(_ResNet50Impl, _combined_schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"use_operator\"></a>\n",
    "## 8.4 Testing and Using the New Operator\n",
    "\n",
    "Once your operator implementation and schema definitions are ready,\n",
    "you can test it with Lale as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the new Operator\n",
    "\n",
    "Before demonstrating the new `ResNet50` operator, the following code loads the\n",
    "CIFAR10 dataset from torchvision.datasets. The tutorial uses only the first 1000 rows for training \n",
    "and 500 rows for test to speed-up the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[n/255.\n",
    "                    for n in [129.3, 124.1, 112.4]], std=[n/255. for n in [68.2,  65.4,  70.4]])\n",
    "]) # meanstd transformation\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[n/255.\n",
    "                    for n in [129.3, 124.1, 112.4]], std=[n/255. for n in [68.2,  65.4,  70.4]])\n",
    "])\n",
    "\n",
    "data_train = datasets.CIFAR10(root = \"/tmp/\", download = True, transform = transform_train)\n",
    "data_train.data = data_train.data[0:1000, :]\n",
    "data_train.targets = data_train.targets[0:1000]\n",
    "\n",
    "data_test = datasets.CIFAR10(root = \"/tmp/\", download = True, train = False, transform = transform_test)\n",
    "data_test.data = data_test.data[0:500, :]\n",
    "data_test.targets = data_test.targets[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate the ResNet50 operator with `num_classes` as 10 and `num_epochs` as 1 and call `fit` to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=> Training Epoch 0, LR=0.1000\n",
      "| Epoch [  0/  1] Iter[  1/ 16]\t\tLoss: 13.4841 Acc@1: 0.000%\n",
      "| Epoch [  0/  1] Iter[  2/ 16]\t\tLoss: 6.7819 Acc@1: 4.688%\n",
      "| Epoch [  0/  1] Iter[  3/ 16]\t\tLoss: 6.9210 Acc@1: 5.729%\n",
      "| Epoch [  0/  1] Iter[  4/ 16]\t\tLoss: 7.0799 Acc@1: 7.031%\n",
      "| Epoch [  0/  1] Iter[  5/ 16]\t\tLoss: 6.4457 Acc@1: 7.500%\n",
      "| Epoch [  0/  1] Iter[  6/ 16]\t\tLoss: 6.4661 Acc@1: 7.552%\n",
      "| Epoch [  0/  1] Iter[  7/ 16]\t\tLoss: 10.4703 Acc@1: 9.375%\n",
      "| Epoch [  0/  1] Iter[  8/ 16]\t\tLoss: 6.9008 Acc@1: 8.984%\n",
      "| Epoch [  0/  1] Iter[  9/ 16]\t\tLoss: 3.9730 Acc@1: 8.681%\n",
      "| Epoch [  0/  1] Iter[ 10/ 16]\t\tLoss: 2.6823 Acc@1: 9.375%\n",
      "| Epoch [  0/  1] Iter[ 11/ 16]\t\tLoss: 3.3830 Acc@1: 9.659%\n",
      "| Epoch [  0/  1] Iter[ 12/ 16]\t\tLoss: 7.4459 Acc@1: 10.156%\n",
      "| Epoch [  0/  1] Iter[ 13/ 16]\t\tLoss: 8.7007 Acc@1: 10.096%\n",
      "| Epoch [  0/  1] Iter[ 14/ 16]\t\tLoss: 9.6212 Acc@1: 10.268%\n",
      "| Epoch [  0/  1] Iter[ 15/ 16]\t\tLoss: 8.1019 Acc@1: 10.208%\n",
      "| Epoch [  0/  1] Iter[ 16/ 16]\t\tLoss: 6.3694 Acc@1: 10.700%\n"
     ]
    }
   ],
   "source": [
    "clf = ResNet50(num_classes=10,num_epochs = 1)\n",
    "fitted_clf = clf.fit(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `predict` to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[779]\n",
      " [814]\n",
      " [536]\n",
      " [860]\n",
      " [953]\n",
      " [996]\n",
      " [625]\n",
      " [845]\n",
      " [956]\n",
      " [586]]\n"
     ]
    }
   ],
   "source": [
    "predicted = fitted_clf.predict(data_test)\n",
    "print(predicted[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \n",
      "=> Training Epoch 0, LR=0.1000\n",
      "| Epoch [  0/  1] Iter[  1/ 22]\t\tLoss: 12.6265 Acc@1: 0.000%                                                            \n",
      "| Epoch [  0/  1] Iter[  2/ 22]\t\tLoss: 8.3455 Acc@1: 5.435%                                                             \n",
      "| Epoch [  0/  1] Iter[  3/ 22]\t\tLoss: 5.3283 Acc@1: 9.420%                                                             \n",
      "| Epoch [  0/  1] Iter[  4/ 22]\t\tLoss: 3.4839 Acc@1: 10.870%                                                            \n",
      "| Epoch [  0/  1] Iter[  5/ 22]\t\tLoss: 10.5996 Acc@1: 10.435%                                                           \n",
      "| Epoch [  0/  1] Iter[  6/ 22]\t\tLoss: 5.1930 Acc@1: 9.058%                                                             \n",
      "| Epoch [  0/  1] Iter[  7/ 22]\t\tLoss: 8.3661 Acc@1: 9.627%                                                             \n",
      "| Epoch [  0/  1] Iter[  8/ 22]\t\tLoss: 7.3044 Acc@1: 10.326%                                                            \n",
      "| Epoch [  0/  1] Iter[  9/ 22]\t\tLoss: 8.1651 Acc@1: 10.628%                                                            \n",
      "| Epoch [  0/  1] Iter[ 10/ 22]\t\tLoss: 5.4314 Acc@1: 10.000%                                                            \n",
      "| Epoch [  0/  1] Iter[ 11/ 22]\t\tLoss: 10.3705 Acc@1: 10.277%                                                           \n",
      "| Epoch [  0/  1] Iter[ 12/ 22]\t\tLoss: 11.7713 Acc@1: 9.964%                                                            \n",
      "| Epoch [  0/  1] Iter[ 13/ 22]\t\tLoss: 7.0805 Acc@1: 10.535%                                                            \n",
      "| Epoch [  0/  1] Iter[ 14/ 22]\t\tLoss: 19.2849 Acc@1: 11.180%                                                           \n",
      "| Epoch [  0/  1] Iter[ 15/ 22]\t\tLoss: 9.3718 Acc@1: 11.159%                                                            \n",
      "| Epoch [  0/  1] Iter[ 16/ 22]\t\tLoss: 11.3188 Acc@1: 10.870%                                                           \n",
      "| Epoch [  0/  1] Iter[ 17/ 22]\t\tLoss: 4.6541 Acc@1: 10.486%                                                            \n",
      "| Epoch [  0/  1] Iter[ 18/ 22]\t\tLoss: 8.2381 Acc@1: 10.990%                                                            \n",
      "| Epoch [  0/  1] Iter[ 19/ 22]\t\tLoss: 7.3090 Acc@1: 11.442%                                                            \n",
      "| Epoch [  0/  1] Iter[ 20/ 22]\t\tLoss: 9.4942 Acc@1: 11.413%                                                            \n",
      "| Epoch [  0/  1] Iter[ 21/ 22]\t\tLoss: 2.9025 Acc@1: 11.387%                                                            \n",
      "| Epoch [  0/  1] Iter[ 22/ 22]\t\tLoss: 9.0954 Acc@1: 11.300%                                                            \n",
      "                                                                                                                        \n",
      "=> Training Epoch 0, LR=0.1000\n",
      "| Epoch [  0/  1] Iter[  1/ 12]\t\tLoss: 13.4596 Acc@1: 0.000%                                                            \n",
      "| Epoch [  0/  1] Iter[  2/ 12]\t\tLoss: 6.1674 Acc@1: 6.111%                                                             \n",
      "| Epoch [  0/  1] Iter[  3/ 12]\t\tLoss: 5.5224 Acc@1: 8.148%                                                             \n",
      "| Epoch [  0/  1] Iter[  4/ 12]\t\tLoss: 6.2777 Acc@1: 9.167%                                                             \n",
      "| Epoch [  0/  1] Iter[  5/ 12]\t\tLoss: 3.1131 Acc@1: 8.000%                                                             \n",
      "| Epoch [  0/  1] Iter[  6/ 12]\t\tLoss: 6.2952 Acc@1: 9.074%                                                             \n",
      "| Epoch [  0/  1] Iter[  7/ 12]\t\tLoss: 5.3193 Acc@1: 9.206%                                                             \n",
      "| Epoch [  0/  1] Iter[  8/ 12]\t\tLoss: 3.6627 Acc@1: 9.028%                                                             \n",
      "| Epoch [  0/  1] Iter[  9/ 12]\t\tLoss: 2.5448 Acc@1: 9.383%                                                             \n",
      "| Epoch [  0/  1] Iter[ 10/ 12]\t\tLoss: 2.7843 Acc@1: 9.778%                                                             \n",
      "| Epoch [  0/  1] Iter[ 11/ 12]\t\tLoss: 5.1601 Acc@1: 9.697%                                                             \n",
      "| Epoch [  0/  1] Iter[ 12/ 12]\t\tLoss: 4.8604 Acc@1: 9.700%                                                             \n",
      "                                                                                                                        \n",
      "=> Training Epoch 0, LR=0.1000\n",
      "| Epoch [  0/  1] Iter[  1/ 13]\t\tLoss: 13.0875 Acc@1: 0.000%                                                            \n",
      "| Epoch [  0/  1] Iter[  2/ 13]\t\tLoss: 6.6415 Acc@1: 3.012%                                                             \n",
      "| Epoch [  0/  1] Iter[  3/ 13]\t\tLoss: 4.0719 Acc@1: 6.827%                                                             \n",
      "| Epoch [  0/  1] Iter[  4/ 13]\t\tLoss: 3.4728 Acc@1: 9.036%                                                             \n",
      "| Epoch [  0/  1] Iter[  5/ 13]\t\tLoss: 4.7206 Acc@1: 8.434%                                                             \n",
      "| Epoch [  0/  1] Iter[  6/ 13]\t\tLoss: 2.5274 Acc@1: 9.237%                                                             \n",
      "| Epoch [  0/  1] Iter[  7/ 13]\t\tLoss: 5.0201 Acc@1: 9.466%                                                             \n",
      "| Epoch [  0/  1] Iter[  8/ 13]\t\tLoss: 5.1951 Acc@1: 9.187%                                                             \n",
      "| Epoch [  0/  1] Iter[  9/ 13]\t\tLoss: 3.1847 Acc@1: 8.969%                                                             \n",
      "| Epoch [  0/  1] Iter[ 10/ 13]\t\tLoss: 6.6334 Acc@1: 8.675%                                                             \n",
      "| Epoch [  0/  1] Iter[ 11/ 13]\t\tLoss: 9.2046 Acc@1: 8.653%                                                             \n",
      "| Epoch [  0/  1] Iter[ 12/ 13]\t\tLoss: 11.2686 Acc@1: 8.434%                                                            \n",
      "| Epoch [  0/  1] Iter[ 13/ 13]\t\tLoss: 20.2701 Acc@1: 8.400%                                                            \n",
      "100%|██████████████████████████████████████████████████████████████████| 3/3 [02:28<00:00, 49.37s/trial, best loss: 0.0]\n",
      "\n",
      "=> Training Epoch 0, LR=0.1000\n",
      "| Epoch [  0/  1] Iter[  1/ 22]\t\tLoss: 12.7016 Acc@1: 0.000%\n",
      "| Epoch [  0/  1] Iter[  2/ 22]\t\tLoss: 7.6211 Acc@1: 3.261%\n",
      "| Epoch [  0/  1] Iter[  3/ 22]\t\tLoss: 8.3257 Acc@1: 4.348%\n",
      "| Epoch [  0/  1] Iter[  4/ 22]\t\tLoss: 6.1589 Acc@1: 3.804%\n",
      "| Epoch [  0/  1] Iter[  5/ 22]\t\tLoss: 4.1197 Acc@1: 5.652%\n",
      "| Epoch [  0/  1] Iter[  6/ 22]\t\tLoss: 5.2478 Acc@1: 6.522%\n",
      "| Epoch [  0/  1] Iter[  7/ 22]\t\tLoss: 5.9002 Acc@1: 7.453%\n",
      "| Epoch [  0/  1] Iter[  8/ 22]\t\tLoss: 4.8375 Acc@1: 8.696%\n",
      "| Epoch [  0/  1] Iter[  9/ 22]\t\tLoss: 2.6697 Acc@1: 8.937%\n",
      "| Epoch [  0/  1] Iter[ 10/ 22]\t\tLoss: 6.0500 Acc@1: 9.130%\n",
      "| Epoch [  0/  1] Iter[ 11/ 22]\t\tLoss: 7.6280 Acc@1: 9.289%\n",
      "| Epoch [  0/  1] Iter[ 12/ 22]\t\tLoss: 3.3041 Acc@1: 9.058%\n",
      "| Epoch [  0/  1] Iter[ 13/ 22]\t\tLoss: 3.1656 Acc@1: 9.365%\n",
      "| Epoch [  0/  1] Iter[ 14/ 22]\t\tLoss: 2.4963 Acc@1: 9.627%\n",
      "| Epoch [  0/  1] Iter[ 15/ 22]\t\tLoss: 5.4670 Acc@1: 9.710%\n",
      "| Epoch [  0/  1] Iter[ 16/ 22]\t\tLoss: 2.4482 Acc@1: 9.783%\n",
      "| Epoch [  0/  1] Iter[ 17/ 22]\t\tLoss: 4.5297 Acc@1: 10.486%\n",
      "| Epoch [  0/  1] Iter[ 18/ 22]\t\tLoss: 2.6115 Acc@1: 10.507%\n",
      "| Epoch [  0/  1] Iter[ 19/ 22]\t\tLoss: 8.5956 Acc@1: 10.412%\n",
      "| Epoch [  0/  1] Iter[ 20/ 22]\t\tLoss: 2.9860 Acc@1: 10.652%\n",
      "| Epoch [  0/  1] Iter[ 21/ 22]\t\tLoss: 2.0665 Acc@1: 11.284%\n",
      "| Epoch [  0/  1] Iter[ 22/ 22]\t\tLoss: 5.7937 Acc@1: 11.100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Hyperopt(estimator=lale.operators.TrainableIndividualOp(_lale_name='ResNet50', _lale_schemas={'tags': {'op': ['estimator', 'classifier'], 'post': [], 'pre': ['images']}, 'type': 'object', 'description': 'Combined schema for expected data and hyperparameters for a transformer for pytorch implementation of ResNet50 for image classification.', '$schema': 'http://json-schema.org/draft-04/schema#', 'properties': {'hyperparams': {'description': 'Hyperparameter schema.', '$schema': 'http://json-schema.org/draft-04/schema#', 'allOf': [{'description': 'This first sub-object lists all constructor arguments with their types, one at a time, omitting cross-argument constraints.', 'type': 'object', 'required': ['num_classes', 'num_epochs', 'batch_size', 'lr_scheduler', 'learning_rate_init'], 'additionalProperties': False, 'relevantToOptimizer': ['num_epochs', 'batch_size', 'lr_scheduler'], 'properties': {'num_classes': {'minimum': 2, 'type': 'integer', 'default': 10, 'description': 'Number of classes.'}, 'batch_size': {'minimum': 1, 'distribution': 'uniform', 'type': 'integer', 'description': 'Batch size used for training and prediction.', 'default': 64, 'maximumForOptimizer': 128, 'minimumForOptimizer': 32}, 'learning_rate_init': {'description': 'Initial value of learning rate for training.', 'minimum': 0, 'distribution': 'loguniform', 'default': 0.1, 'maximumForOptimizer': 0.1, 'minimumForOptimizer': 1e-05, 'type': 'number'}, 'lr_scheduler': {'default': 'constant', 'description': 'Learning rate scheduler for training.', 'enum': ['constant', 'decay']}, 'num_epochs': {'maximumForOptimizer': 200, 'minimum': 0, 'default': 2, 'distribution': 'uniform', 'type': 'integer', 'description': 'The number of epochs used for training.', 'minimumForOptimizer': 2}}}]}}}, _lale_impl=lale.operators._WithoutGetParams(), num_classes=10, num_epochs=1, batch_size=64, lr_scheduler='constant', learning_rate_init=0.1, _lale_frozen_hyperparameters=['num_classes', 'num_epochs']), max_evals=3, cv=None, verbose=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lale.lib.lale import Hyperopt\n",
    "\n",
    "opt = Hyperopt(estimator=clf, max_evals=3, cv= None, verbose=True)\n",
    "opt.fit(data_train, y=None, X_valid=data_test, y_valid=data_test.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, we have learned how to write an operator implementation class and hyperparameter\n",
    "schema; how to register the Lale operator; and how to use the Lale operator for\n",
    "manual as well as automated machine-learning.\n",
    "\n",
    "The next section has more details on options in JSON Schema and can be used as a reference when creating Lale schemas.\n",
    "\n",
    "\n",
    "## 8.5 Appendix\n",
    "\n",
    "This section documents features of JSON Schema that Lale uses, as well as\n",
    "extensions that Lale adds to JSON schema for information specific to the\n",
    "machine-learning domain. For a more comprehensive introduction to JSON\n",
    "Schema, refer to its\n",
    "[Reference](https://json-schema.org/understanding-json-schema/reference/).\n",
    "\n",
    "The following table lists kinds of schemas in JSON Schema:\n",
    "\n",
    "| Kind of schema | Corresponding type in Python/Lale |\n",
    "| ---------------| ---------------------------- |\n",
    "| `null`         | `NoneType`, value `None` |\n",
    "| `boolean`      | `bool`, values `True` or `False` |\n",
    "| `string`       | `str` |\n",
    "| `enum`         | See discussion below. |\n",
    "| `number`       | `float`, .e.g, `0.1` |\n",
    "| `integer`      | `int`, e.g., `42` |\n",
    "| `array`        | See discussion below. |\n",
    "| `object`       | `dict` with string keys |\n",
    "| `anyOf`, `allOf`, `not` | See discussion below. |\n",
    "\n",
    "The use of `null`, `boolean`, and `string` is fairly straightforward.  The\n",
    "following paragraphs discuss the other kinds of schemas one by one.\n",
    "\n",
    "### 8.5.1 enum\n",
    "\n",
    "In JSON Schema, an enum can contain assorted values including strings,\n",
    "numbers, or even `null`. Lale uses enums of strings for categorical\n",
    "hyperparameters, such as `'lr_scheduler': {'enum': ['constant', 'decay']}` in the earlier\n",
    "example. In that case, Lale also automatically declares a corresponding\n",
    "Python `enum`.\n",
    "When Lale uses enums of other types, it is usually to restrict a\n",
    "hyperparameter to a single value, such as `'enum': [None]`.\n",
    "\n",
    "### 8.5.2 number, integer\n",
    "\n",
    "In schemas with `type` set to `number` or `integer`, JSON schema lets users\n",
    "specify `minimum`, `maximum`,\n",
    "`exclusiveMinimum`, and `exclusiveMaximum`. Lale further extends JSON schema\n",
    "with `minimumForOptimizer`, `maximumForOptimizer`, and `distribution`.\n",
    "Possible values for the `distribution` are `'uniform'` (the default) and\n",
    "`'loguniform'`. In the case of integers, Lale quantizes the distributions\n",
    "accordingly.\n",
    "\n",
    "### 8.5.3 array\n",
    "\n",
    "Lale schemas for input and output data make heavy use of the JSON Schema\n",
    "`array` type. In this case, Lale schemas are intended to capture logical\n",
    "schemas, not physical representations, similar to how relational databases\n",
    "hide physical representations behind a well-formalized abstraction layer.\n",
    "Therefore, Lale uses arrays from JSON Schema for several types in Python.\n",
    "The most obvious one is a Python `list`. Another common one is a numpy\n",
    "[ndarray](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html),\n",
    "where Lale uses nested arrays to represent each of the dimensions of a\n",
    "multi-dimensional array. Lale also has support for `pandas.DataFrame` and\n",
    "`pandas.Series`, for which it again uses JSON Schema arrays.\n",
    "\n",
    "For arrays, JSON schema lets users specify `items`, `minItems`, and\n",
    "`maxItems`. Lale further extends JSON schema with `minItemsForOptimizer` and\n",
    "`maxItemsForOptimizer`. Furthermore, Lale supports a `laleType`,\n",
    "which can be `'Any'` to locally disable a subschema check, or`'tuple'` to support cases where the Python code requires a\n",
    "tuple instead of a list.\n",
    "\n",
    "### 8.5.4 object\n",
    "\n",
    "For objects, JSON schema lets users specify a list `required` of properties\n",
    "that must be present, a dictionary `properties` of sub-schemas, and a flag\n",
    "`additionalProperties` to indicate whether the object can have additional\n",
    "properties beyond the keys of the `properties` dictionary. Lale further\n",
    "extends JSON schema with a `relevantToOptimizer` list of properties that\n",
    "hyperparameter optimizers should search over.\n",
    "\n",
    "For individual properties, Lale supports a `default`, which is inspired by\n",
    "and consistent with web API specification practice. It also supports a\n",
    "`forOptimizer` flag which defaults to `True` but can be set to `False` to\n",
    "hide a particular subschema from the hyperparameter optimizer. For example,\n",
    "the number of components for PCA in scikit-learn can be specified as an\n",
    "integer or a floating point number, but an optimizer should only explore one\n",
    "of these choices. Lale supports a Boolean flag `transient` that, if true,\n",
    "elides a hyperparameter during pretty-printing, visualization, or in JSON.\n",
    "\n",
    "### 8.5.5 allOf, anyOf, not\n",
    "\n",
    "In JSON schema, `allOf` is a logical \"and\", `anyOf` is\n",
    "a logical \"or\", and `not` is a logical negation. The running example from\n",
    "earlier illustrated `allOf` at the top level in the hyperparameter schema.\n",
    "A use-case that takes advantage of `anyOf` is for\n",
    "expressing union types, which arise frequently in scikit-learn. For example,\n",
    "here is the schema for `n_components` from PCA:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"n_components\": {\n",
    "    \"anyOf\": [\n",
    "        {\n",
    "            \"description\": \"If not set, keep all components.\",\n",
    "            \"enum\": [None],\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"Use Minka's MLE to guess the dimension.\",\n",
    "            \"enum\": [\"mle\"],\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"\"\"Select the number of components such that the amount of variance that needs to be explained is greater than the specified percentage.\"\"\",\n",
    "            \"type\": \"number\",\n",
    "            \"minimum\": 0.0,\n",
    "            \"exclusiveMinimum\": True,\n",
    "            \"maximum\": 1.0,\n",
    "            \"exclusiveMaximum\": True,\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"Number of components to keep.\",\n",
    "            \"type\": \"integer\",\n",
    "            \"minimum\": 1,\n",
    "            \"laleMaximum\": \"X/items/maxItems\",  # number of columns\n",
    "            \"forOptimizer\": False,\n",
    "        },\n",
    "    ],\n",
    "    \"default\": None,\n",
    "},"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
