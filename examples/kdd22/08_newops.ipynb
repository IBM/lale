{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDD 2022 Hands-on Tutorial on \"Gradual AutoML using Lale\"\n",
    "\n",
    "# 8. Add a New Operator\n",
    "\n",
    "Lale comes with several library operators, so you do not need to write\n",
    "your own. But if you want to use Lale for AutoML that includes your custom models,\n",
    "you will need to create a new Lale operator that wraps the custom model.\n",
    "\n",
    "This tutorial illustrates this using ResNet-50 for image classification as a running example.\n",
    "The following four steps highlight the process to add a new operator:\n",
    "\n",
    "-   [8.1 Create Implementation Class](#impl)\n",
    "-   [8.2 Add Hyperparameter Schema](#hyperparam_schema)\n",
    "-   [8.3 Register a New Lale Operator](#make_operator)\n",
    "-   [8.4 Test and Use the New operator](#use_operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"impl\"></a>\n",
    "## 8.1 Create Implementation Class\n",
    "\n",
    "The implementation class of an operator needs to have methods `__init__`,\n",
    "`fit`, and `predict` or `transform`. Any other compatibility with\n",
    "scikit-learn such as `get_params` or `set_params` is optional, and so\n",
    "is extending from `sklearn.base.BaseEstimator`.\n",
    "\n",
    "This section illustrates how to implement this class for our new operator `ResNet50`.\n",
    "We call this class `_ResNet50Impl` and it uses  ResNet-50 implementation available in the torchvision library (`torchvision.models.resnet50`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://kakate%40us.ibm.com:****@na.artifactory.swg-devops.com/artifactory/api/pypi/wcp-nlp-pypi-virtual/simple\n",
      "Requirement already satisfied: torch in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (1.12.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from torch) (4.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/Users/kakateus.ibm.com/venv/lale_tutorial/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://kakate%40us.ibm.com:****@na.artifactory.swg-devops.com/artifactory/api/pypi/wcp-nlp-pypi-virtual/simple\n",
      "Requirement already satisfied: torchvision in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (0.13.1)\n",
      "Requirement already satisfied: requests in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from torchvision) (4.3.0)\n",
      "Requirement already satisfied: numpy in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from torchvision) (1.23.1)\n",
      "Requirement already satisfied: torch==1.12.1 in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from torchvision) (1.12.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from requests->torchvision) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from requests->torchvision) (1.26.11)\n",
      "Requirement already satisfied: typing-extensions in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from torchvision) (4.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/Users/kakateus.ibm.com/venv/lale_tutorial/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Install dependencies required by this tutorial\n",
    "!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "class _ResNet50Impl():\n",
    "    \n",
    "    #__init__ that takes the hyperparameters as keyword arguments.\n",
    "    def __init__(self, num_classes=10,\n",
    "            num_epochs = 2, batch_size = 128, learning_rate_init=0.1,\n",
    "            lr_scheduler = 'constant'):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1).to(self.device)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        \"\"\"Fit method for ResNet50.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Pytorch Dataset object.\n",
    "          Pytorch dataset that contains the training data and targets.\n",
    "        y : optional\n",
    "          This is ignored.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ResNet50Impl\n",
    "          A new object that is trained.\n",
    "        \"\"\"\n",
    "        trainloader = torch.utils.data.DataLoader(X, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "        self.model.train()\n",
    "        \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "            \n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for epoch in range(self.num_epochs):\n",
    "            learning_rate = self.calculate_learning_rate(epoch)\n",
    "            optimizer = optim.SGD(self.model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "            print('\\n=> Training Epoch %d, LR=%.4f' %(epoch, learning_rate))\n",
    "            for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(inputs)         \n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.data.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "                \n",
    "                print('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
    "                        %(epoch, self.num_epochs, batch_idx+1,\n",
    "                            (len(X)//self.batch_size)+1, loss.data.item(), 100.*correct/total))\n",
    "\n",
    "        return _ResNet50Impl(self.num_classes, self.model, self.num_epochs, self.batch_size,\n",
    "                            self.lr_scheduler) \n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        dataloader = torch.utils.data.DataLoader(X, batch_size=self.batch_size, shuffle=False, num_workers=2)\n",
    "        predicted_X = None\n",
    "        for batch_idx, data in enumerate(dataloader):\n",
    "            if isinstance(data, list) or isinstance(data, tuple):\n",
    "                inputs = data[0] #For standard datasets from torchvision, data is a list with X and y\n",
    "            else:\n",
    "                inputs = data\n",
    "            inputs = inputs.to(self.device)\n",
    "            outputs = self.model(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            predicted = predicted.detach().cpu().numpy()\n",
    "            predicted = np.reshape(predicted, (predicted.shape[0], 1))\n",
    "            if predicted_X is None:\n",
    "                predicted_X = predicted\n",
    "            else:\n",
    "                predicted_X = np.vstack((predicted_X, predicted))\n",
    "        self.model.train()\n",
    "        return predicted_X\n",
    "\n",
    "    def calculate_learning_rate(self, epoch):\n",
    "        import math\n",
    "        if self.lr_scheduler == 'constant':\n",
    "            return self.learning_rate_init\n",
    "        elif self.lr_scheduler == 'decay':\n",
    "            optim_factor = 0\n",
    "            if(epoch > 160):\n",
    "                optim_factor = 3\n",
    "            elif(epoch > 120):\n",
    "                optim_factor = 2\n",
    "            elif(epoch > 60):\n",
    "                optim_factor = 1\n",
    "\n",
    "            return self.learning_rate_init*math.pow(0.2, optim_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code first imports the relevant torchvision and torch packages. Then, it declares\n",
    "a new class for wrapping `torchvision.models.resnet50`. The\n",
    "Lale approach for wrapping new operators carefully avoids depending too much\n",
    "on the Python language or any particular Python library. Hence, the\n",
    "`_ResNet50Impl` class does not need to inherit from anything, but it does need to\n",
    "follow certain conventions:\n",
    "\n",
    "* It has a constructor, `__init__`, whose arguments are the\n",
    "  hyperparameters as keyword arguments.\n",
    "\n",
    "* It has a training method, `fit`, with an argument `X` containing the\n",
    "  training examples and, in the case of supervised models, an argument `y`\n",
    "  containing labels. Since the torch Dataset has both data and targets for this case,\n",
    "  the default value of y is None and it is ignored during training.\n",
    "  The `fit` method trains the neural network by looping for epochs and batches, \n",
    "  and returns the wrapper object with the trained model.\n",
    "\n",
    "* It has a prediction method, `predict` for an estimator or `transform` for\n",
    "  a transformer. The method has an argument `X` containing the test examples\n",
    "  and returns the labels for `predict` or the transformed data for\n",
    "  `transform`.\n",
    "\n",
    "These conventions are designed to be similar to those of scikit-learn.\n",
    "However, they avoid a code dependency upon scikit-learn.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"hyperparam_schema\"></a>\n",
    "## 8.2 Add Hyperparameter Schema\n",
    "\n",
    "Lale uses hyperparameter schemas both for error-checking and for generating search\n",
    "spaces for hyperparameter optimization.\n",
    "A hyperparameter schema specifies the space of valid types and values for\n",
    "hyperparameters and also the ranges for hyperparameter optimization.\n",
    "Schemas are also used to specify types for the arguments to `fit` and `predict` or `transform`,\n",
    "and for the output of `predict` or `transform`. All the schemas are optional,\n",
    "and Lale will skip the corresponding functionality such as error checking or search\n",
    "space generation if they are not specified.\n",
    "For meaningful hyperparameter tuning, a Lale operator needs a hyperparameter schema at the minimum.\n",
    "\n",
    "\n",
    "In this section, we will focus on creating a hyperparameter schema for our ResNet50 operator.\n",
    "To keep the schemas independent of the Python programming language, they are expressed as\n",
    "[JSON Schema](https://json-schema.org/understanding-json-schema/reference/).\n",
    "JSON Schema is currently a draft standard and is already being widely\n",
    "adopted and implemented, for instance, as part of specifying\n",
    "[Swagger APIs](https://www.openapis.org/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_hyperparams_schema = {\n",
    "  '$schema': 'http://json-schema.org/draft-04/schema#',\n",
    "  'description': 'Hyperparameter schema.',\n",
    "  'allOf': [\n",
    "    { 'description':\n",
    "        'This first sub-object lists all constructor arguments with their '\n",
    "        'types, one at a time, omitting cross-argument constraints.',\n",
    "      'type': 'object',\n",
    "      'additionalProperties': False,\n",
    "      'required': ['num_classes', 'num_epochs',\n",
    "       'batch_size', 'lr_scheduler', 'learning_rate_init'],\n",
    "      'relevantToOptimizer': ['num_epochs', 'batch_size', 'lr_scheduler'],\n",
    "      'properties': {\n",
    "        'num_classes': {\n",
    "          'description': 'Number of classes.',\n",
    "          'type': 'integer',\n",
    "          'default': 10,\n",
    "          'minimum': 2},\n",
    "        'num_epochs':{\n",
    "          'description': 'The number of epochs used for training.',\n",
    "          'type': 'integer',\n",
    "          'default': 2,\n",
    "          'minimum': 0,\n",
    "          'distribution': 'uniform',\n",
    "          'minimumForOptimizer': 2,\n",
    "          'maximumForOptimizer': 200},\n",
    "        'batch_size':{\n",
    "          'description': 'Batch size used for training and prediction.',\n",
    "          'type': 'integer',\n",
    "          'default': 64,\n",
    "          'minimum': 1,\n",
    "          'distribution': 'uniform',\n",
    "          'minimumForOptimizer': 32,\n",
    "          'maximumForOptimizer': 128},\n",
    "        'lr_scheduler':{\n",
    "          'description': 'Learning rate scheduler for training.',\n",
    "          'enum': ['constant', 'decay'],\n",
    "          'default': 'constant'},           \n",
    "        'learning_rate_init':{\n",
    "          'description': 'Initial value of learning rate for training.',\n",
    "          'type': 'number',\n",
    "          'default': 0.1,\n",
    "          'minimum': 0,\n",
    "          'distribution': 'loguniform',\n",
    "          'minimumForOptimizer': 1e-05,\n",
    "          'maximumForOptimizer': 0.1\n",
    "        }}}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `num_classes`, `num_epochs` and `batch_size` are integer hyperparameters, `lr_scheduler` is a\n",
    "categorical hyperparameter, and `learning_rate_init` is a float. \n",
    "For all the hyperparameters, the schema\n",
    "includes a `type` which indicates the data type, \n",
    "`description` which is used for interactive documentation, and a\n",
    "`default` value. Note that the JSON Schema type for float is `number`. \n",
    "For integer and float hyperparameters, a `minimum` value is specified.\n",
    "The categorical hyperparameter is specified as an enumeration of its allowed values.\n",
    "\n",
    "The schema has a list called `relevantToOptimizer` which includes hyperparameters to be tuned\n",
    "during AutoML.\n",
    "For such hyperparameters, the schema\n",
    "includes additional information such as its `distribution`, `minimumForOptimizer`, and\n",
    "`maximumForOptimizer` guiding the optimizer to limit its search space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"make_operator\"></a>\n",
    "## 8.3 Register a New Lale Operator\n",
    "\n",
    "We can now register `_ResNet50Impl` as a new Lale operator `ResNet50`. This is done by calling `make_operator` and passing the implementation class and a dictionary with information such as the `description` of the operator, `tags` which include additional information about what kind of operator it is, and the hyperparameter schema given by key `hyperparams`. For `tags`, it is required to have a key `op` indicating whether it is an `estimator` or `transformer` per scikit-learn terminology. Information such as `classifier` or `regressor` is encouraged but optional and `pre` and `post` are optional as well. Please refer to `lale.lib` for more examples of `tags`.\n",
    "\n",
    "As mentioned earlier, the other schemas such as `input_fit`, `input_predict`, `output_fit`, and `output_predict` specify data types of input/output of fit and predict. We will omit those for this tutorial, but `lale.lib` has many examples to refer to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lale.operators import make_operator\n",
    "\n",
    "_combined_schemas = {\n",
    "  '$schema': 'http://json-schema.org/draft-04/schema#',\n",
    "  'description': 'Combined schema for expected data and hyperparameters for a transformer for'\n",
    "                ' pytorch implementation of ResNet50 for image classification.',\n",
    "  'type': 'object',\n",
    "  'tags': {'pre': ['images'], 'op': ['estimator', 'classifier'], 'post': []},\n",
    "  'properties': {\n",
    "    'hyperparams': _hyperparams_schema}}\n",
    "\n",
    "ResNet50 = make_operator(_ResNet50Impl, _combined_schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"use_operator\"></a>\n",
    "## 8.4 Testing and Using the New Operator\n",
    "\n",
    "Once your operator implementation and schema definitions are ready,\n",
    "you can test it with Lale as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.1 Train and Use the New Operator\n",
    "\n",
    "Before demonstrating the new `ResNet50` operator, the following code loads the\n",
    "CIFAR10 dataset from torchvision.datasets. The tutorial uses only the first 1000 rows for training \n",
    "and 500 rows for test to speed-up the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[n/255.\n",
    "                    for n in [129.3, 124.1, 112.4]], std=[n/255. for n in [68.2,  65.4,  70.4]])\n",
    "]) # meanstd transformation\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[n/255.\n",
    "                    for n in [129.3, 124.1, 112.4]], std=[n/255. for n in [68.2,  65.4,  70.4]])\n",
    "])\n",
    "\n",
    "data_train = datasets.CIFAR10(root = \"/tmp/\", download = True, transform = transform_train)\n",
    "data_train.data = data_train.data[0:1000, :]\n",
    "data_train.targets = data_train.targets[0:1000]\n",
    "\n",
    "data_test = datasets.CIFAR10(root = \"/tmp/\", download = True, train = False, transform = transform_test)\n",
    "data_test.data = data_test.data[0:500, :]\n",
    "data_test.targets = data_test.targets[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate the ResNet50 operator with `num_classes` as 10 and `num_epochs` as 1 and call `fit` to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=> Training Epoch 0, LR=0.1000\n",
      "| Epoch [  0/  1] Iter[  1/ 16]\t\tLoss: 12.5136 Acc@1: 0.000%\n",
      "| Epoch [  0/  1] Iter[  2/ 16]\t\tLoss: 5.9717 Acc@1: 6.250%\n",
      "| Epoch [  0/  1] Iter[  3/ 16]\t\tLoss: 5.8434 Acc@1: 9.896%\n",
      "| Epoch [  0/  1] Iter[  4/ 16]\t\tLoss: 4.2084 Acc@1: 9.766%\n",
      "| Epoch [  0/  1] Iter[  5/ 16]\t\tLoss: 3.6952 Acc@1: 11.250%\n",
      "| Epoch [  0/  1] Iter[  6/ 16]\t\tLoss: 5.7219 Acc@1: 11.458%\n",
      "| Epoch [  0/  1] Iter[  7/ 16]\t\tLoss: 5.4148 Acc@1: 10.714%\n",
      "| Epoch [  0/  1] Iter[  8/ 16]\t\tLoss: 4.8464 Acc@1: 10.352%\n",
      "| Epoch [  0/  1] Iter[  9/ 16]\t\tLoss: 5.2711 Acc@1: 11.111%\n",
      "| Epoch [  0/  1] Iter[ 10/ 16]\t\tLoss: 5.2699 Acc@1: 10.938%\n",
      "| Epoch [  0/  1] Iter[ 11/ 16]\t\tLoss: 6.2654 Acc@1: 11.222%\n",
      "| Epoch [  0/  1] Iter[ 12/ 16]\t\tLoss: 4.9063 Acc@1: 10.938%\n",
      "| Epoch [  0/  1] Iter[ 13/ 16]\t\tLoss: 4.0157 Acc@1: 10.938%\n",
      "| Epoch [  0/  1] Iter[ 14/ 16]\t\tLoss: 2.5661 Acc@1: 11.272%\n",
      "| Epoch [  0/  1] Iter[ 15/ 16]\t\tLoss: 2.6410 Acc@1: 10.938%\n",
      "| Epoch [  0/  1] Iter[ 16/ 16]\t\tLoss: 8.7660 Acc@1: 10.800%\n"
     ]
    }
   ],
   "source": [
    "clf = ResNet50(num_classes=10,num_epochs = 1)\n",
    "fitted_clf = clf.fit(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `predict` to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[779]\n",
      " [814]\n",
      " [536]\n",
      " [860]\n",
      " [953]\n",
      " [996]\n",
      " [625]\n",
      " [845]\n",
      " [956]\n",
      " [586]]\n"
     ]
    }
   ],
   "source": [
    "predicted = fitted_clf.predict(data_test)\n",
    "print(predicted[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.2 Configure the New Operator Using AutoML\n",
    "\n",
    "We can use an optimizer from Lale to search over ResNet50's hyperparameter ranges and find the best configuration. In this example, we will use [Hyperopt](https://lale.readthedocs.io/en/latest/modules/lale.lib.lale.hyperopt.html?highlight=Hyperopt#lale.lib.lale.hyperopt.Hyperopt) as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \n",
      "=> Training Epoch 0, LR=0.1000\n",
      "| Epoch [  0/  1] Iter[  1/ 22]\t\tLoss: 13.1677 Acc@1: 0.000%                                                            \n",
      "| Epoch [  0/  1] Iter[  2/ 22]\t\tLoss: 8.5692 Acc@1: 2.174%                                                             \n",
      "| Epoch [  0/  1] Iter[  3/ 22]\t\tLoss: 6.6609 Acc@1: 6.522%                                                             \n",
      "| Epoch [  0/  1] Iter[  4/ 22]\t\tLoss: 3.0092 Acc@1: 6.522%                                                             \n",
      "| Epoch [  0/  1] Iter[  5/ 22]\t\tLoss: 4.2045 Acc@1: 8.261%                                                             \n",
      "| Epoch [  0/  1] Iter[  6/ 22]\t\tLoss: 8.5942 Acc@1: 9.783%                                                             \n",
      "| Epoch [  0/  1] Iter[  7/ 22]\t\tLoss: 5.5907 Acc@1: 9.938%                                                             \n",
      "| Epoch [  0/  1] Iter[  8/ 22]\t\tLoss: 4.5967 Acc@1: 9.239%                                                             \n",
      "| Epoch [  0/  1] Iter[  9/ 22]\t\tLoss: 3.4519 Acc@1: 9.903%                                                             \n",
      "| Epoch [  0/  1] Iter[ 10/ 22]\t\tLoss: 4.3757 Acc@1: 9.783%                                                             \n",
      "| Epoch [  0/  1] Iter[ 11/ 22]\t\tLoss: 2.3765 Acc@1: 9.684%                                                             \n",
      "| Epoch [  0/  1] Iter[ 12/ 22]\t\tLoss: 2.6716 Acc@1: 10.145%                                                            \n",
      "| Epoch [  0/  1] Iter[ 13/ 22]\t\tLoss: 6.3464 Acc@1: 10.535%                                                            \n",
      "| Epoch [  0/  1] Iter[ 14/ 22]\t\tLoss: 6.8596 Acc@1: 10.093%                                                            \n",
      "| Epoch [  0/  1] Iter[ 15/ 22]\t\tLoss: 17.5595 Acc@1: 9.710%                                                            \n",
      "| Epoch [  0/  1] Iter[ 16/ 22]\t\tLoss: 12.0208 Acc@1: 9.511%                                                            \n",
      "| Epoch [  0/  1] Iter[ 17/ 22]\t\tLoss: 11.2524 Acc@1: 9.335%                                                            \n",
      "| Epoch [  0/  1] Iter[ 18/ 22]\t\tLoss: 3.9910 Acc@1: 9.058%                                                             \n",
      "| Epoch [  0/  1] Iter[ 19/ 22]\t\tLoss: 14.6504 Acc@1: 9.382%                                                            \n",
      "| Epoch [  0/  1] Iter[ 20/ 22]\t\tLoss: 11.3939 Acc@1: 9.457%                                                            \n",
      "| Epoch [  0/  1] Iter[ 21/ 22]\t\tLoss: 7.2020 Acc@1: 9.317%                                                             \n",
      "| Epoch [  0/  1] Iter[ 22/ 22]\t\tLoss: 5.2891 Acc@1: 9.500%                                                             \n",
      "                                                                                                                        \n",
      "=> Training Epoch 0, LR=0.1000\n",
      "| Epoch [  0/  1] Iter[  1/ 12]\t\tLoss: 12.6987 Acc@1: 0.000%                                                            \n",
      "| Epoch [  0/  1] Iter[  2/ 12]\t\tLoss: 8.8268 Acc@1: 5.000%                                                             \n",
      "| Epoch [  0/  1] Iter[  3/ 12]\t\tLoss: 5.3455 Acc@1: 6.296%                                                             \n",
      "| Epoch [  0/  1] Iter[  4/ 12]\t\tLoss: 4.0082 Acc@1: 8.611%                                                             \n",
      "| Epoch [  0/  1] Iter[  5/ 12]\t\tLoss: 4.0248 Acc@1: 8.667%                                                             \n",
      "| Epoch [  0/  1] Iter[  6/ 12]\t\tLoss: 4.6878 Acc@1: 9.259%                                                             \n",
      "| Epoch [  0/  1] Iter[  7/ 12]\t\tLoss: 2.3167 Acc@1: 10.159%                                                            \n",
      "| Epoch [  0/  1] Iter[  8/ 12]\t\tLoss: 5.7445 Acc@1: 11.667%                                                            \n",
      "| Epoch [  0/  1] Iter[  9/ 12]\t\tLoss: 4.0892 Acc@1: 12.099%                                                            \n",
      "| Epoch [  0/  1] Iter[ 10/ 12]\t\tLoss: 7.3019 Acc@1: 11.889%                                                            \n",
      "| Epoch [  0/  1] Iter[ 11/ 12]\t\tLoss: 5.7493 Acc@1: 13.030%                                                            \n",
      "| Epoch [  0/  1] Iter[ 12/ 12]\t\tLoss: 15.7950 Acc@1: 13.100%                                                           \n",
      "                                                                                                                        \n",
      "=> Training Epoch 0, LR=0.1000\n",
      "| Epoch [  0/  1] Iter[  1/ 13]\t\tLoss: 12.4718 Acc@1: 0.000%                                                            \n",
      "| Epoch [  0/  1] Iter[  2/ 13]\t\tLoss: 7.2847 Acc@1: 7.831%                                                             \n",
      "| Epoch [  0/  1] Iter[  3/ 13]\t\tLoss: 5.4385 Acc@1: 10.040%                                                            \n",
      "| Epoch [  0/  1] Iter[  4/ 13]\t\tLoss: 3.4287 Acc@1: 9.639%                                                             \n",
      "| Epoch [  0/  1] Iter[  5/ 13]\t\tLoss: 4.6764 Acc@1: 9.157%                                                             \n",
      "| Epoch [  0/  1] Iter[  6/ 13]\t\tLoss: 5.8584 Acc@1: 10.241%                                                            \n",
      "| Epoch [  0/  1] Iter[  7/ 13]\t\tLoss: 6.8813 Acc@1: 10.843%                                                            \n",
      "| Epoch [  0/  1] Iter[  8/ 13]\t\tLoss: 4.2350 Acc@1: 10.843%                                                            \n",
      "| Epoch [  0/  1] Iter[  9/ 13]\t\tLoss: 5.2460 Acc@1: 10.843%                                                            \n",
      "| Epoch [  0/  1] Iter[ 10/ 13]\t\tLoss: 6.0586 Acc@1: 11.325%                                                            \n",
      "| Epoch [  0/  1] Iter[ 11/ 13]\t\tLoss: 6.1566 Acc@1: 11.829%                                                            \n",
      "| Epoch [  0/  1] Iter[ 12/ 13]\t\tLoss: 5.7809 Acc@1: 11.847%                                                            \n",
      "| Epoch [  0/  1] Iter[ 13/ 13]\t\tLoss: 20.4118 Acc@1: 11.800%                                                           \n",
      "100%|██████████████████████████████████████████████████████████████████| 3/3 [02:27<00:00, 49.07s/trial, best loss: 0.0]\n",
      "\n",
      "=> Training Epoch 0, LR=0.1000\n",
      "| Epoch [  0/  1] Iter[  1/ 22]\t\tLoss: 12.8850 Acc@1: 0.000%\n",
      "| Epoch [  0/  1] Iter[  2/ 22]\t\tLoss: 9.7843 Acc@1: 3.261%\n",
      "| Epoch [  0/  1] Iter[  3/ 22]\t\tLoss: 6.6534 Acc@1: 6.522%\n",
      "| Epoch [  0/  1] Iter[  4/ 22]\t\tLoss: 6.9561 Acc@1: 7.065%\n",
      "| Epoch [  0/  1] Iter[  5/ 22]\t\tLoss: 6.2994 Acc@1: 6.957%\n",
      "| Epoch [  0/  1] Iter[  6/ 22]\t\tLoss: 8.8375 Acc@1: 7.246%\n",
      "| Epoch [  0/  1] Iter[  7/ 22]\t\tLoss: 4.8252 Acc@1: 8.385%\n",
      "| Epoch [  0/  1] Iter[  8/ 22]\t\tLoss: 13.6308 Acc@1: 8.152%\n",
      "| Epoch [  0/  1] Iter[  9/ 22]\t\tLoss: 13.3792 Acc@1: 7.971%\n",
      "| Epoch [  0/  1] Iter[ 10/ 22]\t\tLoss: 18.7606 Acc@1: 8.261%\n",
      "| Epoch [  0/  1] Iter[ 11/ 22]\t\tLoss: 14.1572 Acc@1: 7.708%\n",
      "| Epoch [  0/  1] Iter[ 12/ 22]\t\tLoss: 11.2799 Acc@1: 8.696%\n",
      "| Epoch [  0/  1] Iter[ 13/ 22]\t\tLoss: 10.1107 Acc@1: 9.197%\n",
      "| Epoch [  0/  1] Iter[ 14/ 22]\t\tLoss: 4.0331 Acc@1: 8.696%\n",
      "| Epoch [  0/  1] Iter[ 15/ 22]\t\tLoss: 6.2430 Acc@1: 8.841%\n",
      "| Epoch [  0/  1] Iter[ 16/ 22]\t\tLoss: 12.9002 Acc@1: 8.696%\n",
      "| Epoch [  0/  1] Iter[ 17/ 22]\t\tLoss: 14.5975 Acc@1: 8.696%\n",
      "| Epoch [  0/  1] Iter[ 18/ 22]\t\tLoss: 11.1334 Acc@1: 8.575%\n",
      "| Epoch [  0/  1] Iter[ 19/ 22]\t\tLoss: 9.8441 Acc@1: 8.238%\n",
      "| Epoch [  0/  1] Iter[ 20/ 22]\t\tLoss: 5.1905 Acc@1: 8.261%\n",
      "| Epoch [  0/  1] Iter[ 21/ 22]\t\tLoss: 11.4466 Acc@1: 8.696%\n",
      "| Epoch [  0/  1] Iter[ 22/ 22]\t\tLoss: 4.2233 Acc@1: 8.900%\n"
     ]
    }
   ],
   "source": [
    "from lale.lib.lale import Hyperopt\n",
    "\n",
    "opt = Hyperopt(estimator=clf, max_evals=3, cv= None, verbose=True)\n",
    "trained_opt = opt.fit(data_train, y=None, X_valid=data_test, y_valid=data_test.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"62pt\" height=\"45pt\"\n",
       " viewBox=\"0.00 0.00 62.00 44.77\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40.7696)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-40.7696 58,-40.7696 58,4 -4,4\"/>\n",
       "<!-- (root) -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>(root)</title>\n",
       "<g id=\"a_node1\"><a xlink:title=\"(root) = ResNet50(num_epochs=1, batch_size=46, lr_scheduler=&#39;decay&#39;)\">\n",
       "<ellipse fill=\"#ffffff\" stroke=\"#000000\" cx=\"27\" cy=\"-18.3848\" rx=\"27\" ry=\"18.2703\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-20.5848\" font-family=\"Times,serif\" font-size=\"11.00\" fill=\"#000000\">Res&#45;</text>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-9.5848\" font-family=\"Times,serif\" font-size=\"11.00\" fill=\"#000000\">Net50</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1698af5b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_config = trained_opt.get_pipeline()\n",
    "best_config.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, we have learned how to write an operator implementation class and hyperparameter\n",
    "schema; how to register the Lale operator; and how to use the Lale operator for\n",
    "manual as well as automated machine-learning.\n",
    "\n",
    "Lale can also generate automatic documentation for operators, the notebook [Schemas and Their Uses](./10_schemas.ipynb) describes how to enable it.\n",
    "\n",
    "The reference section of our [documentation for adding new operators](https://github.com/IBM/lale/blob/master/examples/docs_new_operators.ipynb) has more details about JSON Schemas and how to use those when creating schemas for Lale operators."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
