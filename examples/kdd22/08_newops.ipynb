{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDD 2022 Hands-on Tutorial on \"Gradual AutoML using Lale\"\n",
    "\n",
    "# 8. Add a New Operator\n",
    "\n",
    "Lale comes with several library operators, so you do not need to write\n",
    "your own. But if you want to use Lale for AutoML that includes your custom models,\n",
    "you will need to create a new Lale operator that wraps the custom model.\n",
    "\n",
    "This tutorial illustrates this using ResNet-50 for image classification as a running example.\n",
    "The following four steps highlight the process to add a new operator:\n",
    "\n",
    "-   [8.1 Create Implementation Class](#impl)\n",
    "-   [8.2 Add Hyperparameter Schema](#hyperparam_schema)\n",
    "-   [8.3 Register a New Lale Operator](#make_operator)\n",
    "-   [8.4 Test and Use the New operator](#use_operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"impl\"></a>\n",
    "## 8.1 Create Implementation Class\n",
    "\n",
    "The implementation class of an operator needs to have methods `__init__`,\n",
    "`fit`, and `predict` or `transform`. Any other compatibility with\n",
    "scikit-learn such as `get_params` or `set_params` is optional, and so\n",
    "is extending from `sklearn.base.BaseEstimator`.\n",
    "\n",
    "This section illustrates how to implement this class for our new operator `ResNet50`.\n",
    "We call this class `_ResNet50Impl` and it uses  ResNet-50 implementation available in the torchvision library (`torchvision.models.resnet50`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://kakate%40us.ibm.com:****@na.artifactory.swg-devops.com/artifactory/api/pypi/wcp-nlp-pypi-virtual/simple\n",
      "Requirement already satisfied: torch in /Users/kakateus.ibm.com/venv/lale39/lib/python3.9/site-packages (1.12.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/kakateus.ibm.com/venv/lale39/lib/python3.9/site-packages (from torch) (4.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.2.1 is available.\n",
      "You should consider upgrading via the '/Users/kakateus.ibm.com/venv/lale39/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://kakate%40us.ibm.com:****@na.artifactory.swg-devops.com/artifactory/api/pypi/wcp-nlp-pypi-virtual/simple\n",
      "Requirement already satisfied: torchvision in /Users/kakateus.ibm.com/venv/lale39/lib/python3.9/site-packages (0.13.0)\n",
      "Requirement already satisfied: requests in /Users/kakateus.ibm.com/venv/lale39/lib/python3.9/site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: torch==1.12.0 in /Users/kakateus.ibm.com/venv/lale39/lib/python3.9/site-packages (from torchvision) (1.12.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/kakateus.ibm.com/venv/lale39/lib/python3.9/site-packages (from torchvision) (4.0.1)\n",
      "Requirement already satisfied: numpy in /Users/kakateus.ibm.com/venv/lale39/lib/python3.9/site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/kakateus.ibm.com/venv/lale39/lib/python3.9/site-packages (from torchvision) (9.0.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/kakateus.ibm.com/venv/lale39/lib/python3.9/site-packages (from requests->torchvision) (2.0.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/kakateus.ibm.com/venv/lale39/lib/python3.9/site-packages (from requests->torchvision) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kakateus.ibm.com/venv/lale39/lib/python3.9/site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kakateus.ibm.com/venv/lale39/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.2.1 is available.\n",
      "You should consider upgrading via the '/Users/kakateus.ibm.com/venv/lale39/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Install dependencies required by this tutorial\n",
    "!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "class _ResNet50Impl():\n",
    "    \n",
    "    #__init__ that takes the hyperparameters as keyword arguments.\n",
    "    def __init__(self, num_classes=10,\n",
    "            num_epochs = 2, batch_size = 128, learning_rate_init=0.1,\n",
    "            lr_scheduler = 'constant'):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = resnet50(num_classes).to(self.device)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        \"\"\"Fit method for ResNet50.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Pytorch Dataset object.\n",
    "          Pytorch dataset that contains the training data and targets.\n",
    "        y : optional\n",
    "          This is ignored.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ResNet50Impl\n",
    "          A new object that is trained.\n",
    "        \"\"\"\n",
    "        trainloader = torch.utils.data.DataLoader(X, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "        self.model.train()\n",
    "        \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "            \n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for epoch in range(self.num_epochs):\n",
    "            learning_rate = self.calculate_learning_rate(epoch)\n",
    "            optimizer = optim.SGD(self.model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "            print('\\n=> Training Epoch %d, LR=%.4f' %(epoch, learning_rate))\n",
    "            for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(inputs)         \n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.data.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "                \n",
    "                print('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
    "                        %(epoch, self.num_epochs, batch_idx+1,\n",
    "                            (len(X)//self.batch_size)+1, loss.data.item(), 100.*correct/total))\n",
    "\n",
    "        return _ResNet50Impl(self.num_classes, self.model, self.num_epochs, self.batch_size,\n",
    "                            self.lr_scheduler) \n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        dataloader = torch.utils.data.DataLoader(X, batch_size=self.batch_size, shuffle=False, num_workers=2)\n",
    "        predicted_X = None\n",
    "        for batch_idx, data in enumerate(dataloader):\n",
    "            if isinstance(data, list) or isinstance(data, tuple):\n",
    "                inputs = data[0] #For standard datasets from torchvision, data is a list with X and y\n",
    "            else:\n",
    "                inputs = data\n",
    "            inputs = inputs.to(self.device)\n",
    "            outputs = self.model(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            predicted = predicted.detach().cpu().numpy()\n",
    "            predicted = np.reshape(predicted, (predicted.shape[0], 1))\n",
    "            if predicted_X is None:\n",
    "                predicted_X = predicted\n",
    "            else:\n",
    "                predicted_X = np.vstack((predicted_X, predicted))\n",
    "        self.model.train()\n",
    "        return predicted_X\n",
    "\n",
    "    def calculate_learning_rate(self, epoch):\n",
    "        import math\n",
    "        if self.lr_scheduler == 'constant':\n",
    "            return self.learning_rate_init\n",
    "        elif self.lr_scheduler == 'decay':\n",
    "            optim_factor = 0\n",
    "            if(epoch > 160):\n",
    "                optim_factor = 3\n",
    "            elif(epoch > 120):\n",
    "                optim_factor = 2\n",
    "            elif(epoch > 60):\n",
    "                optim_factor = 1\n",
    "\n",
    "            return self.learning_rate_init*math.pow(0.2, optim_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code first imports the relevant torchvision and torch packages. Then, it declares\n",
    "a new class for wrapping `torchvision.models.resnet50`. The\n",
    "Lale approach for wrapping new operators carefully avoids depending too much\n",
    "on the Python language or any particular Python library. Hence, the\n",
    "`_ResNet50Impl` class does not need to inherit from anything, but it does need to\n",
    "follow certain conventions:\n",
    "\n",
    "* It has a constructor, `__init__`, whose arguments are the\n",
    "  hyperparameters as keyword arguments.\n",
    "\n",
    "* It has a training method, `fit`, with an argument `X` containing the\n",
    "  training examples and, in the case of supervised models, an argument `y`\n",
    "  containing labels. Since the torch Dataset has both data and targets for this case,\n",
    "  the default value of y is None and it is ignored during training.\n",
    "  The `fit` method trains the neural network by looping for epochs and batches, \n",
    "  and returns the wrapper object with the trained model.\n",
    "\n",
    "* It has a prediction method, `predict` for an estimator or `transform` for\n",
    "  a transformer. The method has an argument `X` containing the test examples\n",
    "  and returns the labels for `predict` or the transformed data for\n",
    "  `transform`.\n",
    "\n",
    "These conventions are designed to be similar to those of scikit-learn.\n",
    "However, they avoid a code dependency upon scikit-learn.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"hyperparam_schema\"></a>\n",
    "## 8.2 Add Hyperparameter Schema\n",
    "\n",
    "Lale uses hyperparameter schemas both for error-checking and for generating search\n",
    "spaces for hyperparameter optimization.\n",
    "A hyperparameter schema specifies the space of valid types and values for\n",
    "hyperparameters and also the ranges for hyperparameter optimization.\n",
    "Schemas are also used to specify types for the arguments to `fit` and `predict` or `transform`,\n",
    "and for the output of `predict` or `transform`. All the schemas are optional,\n",
    "and Lale will skip the corresponding functionality such as error checking or search\n",
    "space generation if they are not specified.\n",
    "For meaningful hyperparameter tuning, a Lale operator needs a hyperparameter schema at the minimum.\n",
    "\n",
    "\n",
    "In this section, we will focus on creating a hyperparameter schema for our ResNet50 operator.\n",
    "To keep the schemas independent of the Python programming language, they are expressed as\n",
    "[JSON Schema](https://json-schema.org/understanding-json-schema/reference/).\n",
    "JSON Schema is currently a draft standard and is already being widely\n",
    "adopted and implemented, for instance, as part of specifying\n",
    "[Swagger APIs](https://www.openapis.org/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_hyperparams_schema = {\n",
    "  '$schema': 'http://json-schema.org/draft-04/schema#',\n",
    "  'description': 'Hyperparameter schema.',\n",
    "  'allOf': [\n",
    "    { 'description':\n",
    "        'This first sub-object lists all constructor arguments with their '\n",
    "        'types, one at a time, omitting cross-argument constraints.',\n",
    "      'type': 'object',\n",
    "      'additionalProperties': False,\n",
    "      'required': ['num_classes', 'num_epochs',\n",
    "       'batch_size', 'lr_scheduler', 'learning_rate_init'],\n",
    "      'relevantToOptimizer': ['num_epochs', 'batch_size', 'lr_scheduler'],\n",
    "      'properties': {\n",
    "        'num_classes': {\n",
    "          'description': 'Number of classes.',\n",
    "          'type': 'integer',\n",
    "          'default': 10,\n",
    "          'minimum': 2},\n",
    "        'num_epochs':{\n",
    "          'description': 'The number of epochs used for training.',\n",
    "          'type': 'integer',\n",
    "          'default': 2,\n",
    "          'minimum': 0,\n",
    "          'distribution': 'uniform',\n",
    "          'minimumForOptimizer': 2,\n",
    "          'maximumForOptimizer': 200},\n",
    "        'batch_size':{\n",
    "          'description': 'Batch size used for training and prediction.',\n",
    "          'type': 'integer',\n",
    "          'default': 64,\n",
    "          'minimum': 1,\n",
    "          'distribution': 'uniform',\n",
    "          'minimumForOptimizer': 32,\n",
    "          'maximumForOptimizer': 128},\n",
    "        'lr_scheduler':{\n",
    "          'description': 'Learning rate scheduler for training.',\n",
    "          'enum': ['constant', 'decay'],\n",
    "          'default': 'constant'},           \n",
    "        'learning_rate_init':{\n",
    "          'description': 'Initial value of learning rate for training.',\n",
    "          'type': 'number',\n",
    "          'default': 0.1,\n",
    "          'minimum': 0,\n",
    "          'distribution': 'loguniform',\n",
    "          'minimumForOptimizer': 1e-05,\n",
    "          'maximumForOptimizer': 0.1\n",
    "        }}}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `num_classes`, `num_epochs` and `batch_size` are integer hyperparameters, `lr_scheduler` is a\n",
    "categorical hyperparameter, and `learning_rate_init` is a float. \n",
    "For all the hyperparameters, the schema\n",
    "includes a `type` which indicates the data type, \n",
    "`description` which is used for interactive documentation, and a\n",
    "`default` value. Note that the JSON Schema type for float is `number`. \n",
    "For integer and float hyperparameters, a `minimum` value is specified.\n",
    "The categorical hyperparameter is specified as an enumeration of its allowed values.\n",
    "\n",
    "The schema has a list called `relevantToOptimizer` which includes hyperparameters to be tuned\n",
    "during AutoML.\n",
    "For such hyperparameters, the schema\n",
    "includes additional information such as its `distribution`, `minimumForOptimizer`, and\n",
    "`maximumForOptimizer` guiding the optimizer to limit its search space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"make_operator\"></a>\n",
    "## 8.3 Register a New Lale Operator\n",
    "\n",
    "We can now register `_ResNet50Impl` as a new Lale operator `ResNet50`. This is done by calling `make_operator` and passing the implementation class and a dictionary with information such as the `description` of the operator, `tags` which include additional information about what kind of operator it is, and the hyperparameter schema given by key `hyperparams`. For `tags`, it is required to have a key `op` indicating whether it is an `estimator` or `transformer` per scikit-learn terminology. Information such as `classifier` or `regressor` is encouraged but optional and `pre` and `post` are optional as well. Please refer to `lale.lib` for more examples of `tags`.\n",
    "\n",
    "As mentioned earlier, the other schemas such as `input_fit`, `input_predict`, `output_fit`, and `output_predict` specify data types of input/output of fit and predict. We will omit those for this tutorial, but `lale.lib` has many examples to refer to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lale.operators import make_operator\n",
    "\n",
    "_combined_schemas = {\n",
    "  '$schema': 'http://json-schema.org/draft-04/schema#',\n",
    "  'description': 'Combined schema for expected data and hyperparameters for a transformer for'\n",
    "                ' pytorch implementation of ResNet50 for image classification.',\n",
    "  'type': 'object',\n",
    "  'tags': {'pre': ['images'], 'op': ['estimator', 'classifier'], 'post': []},\n",
    "  'properties': {\n",
    "    'hyperparams': _hyperparams_schema}}\n",
    "\n",
    "ResNet50 = make_operator(_ResNet50Impl, _combined_schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"use_operator\"></a>\n",
    "## 8.4 Testing and Using the New Operator\n",
    "\n",
    "Once your operator implementation and schema definitions are ready,\n",
    "you can test it with Lale as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the new Operator\n",
    "\n",
    "Before demonstrating the new `ResNet50` operator, the following code loads the\n",
    "CIFAR10 dataset from torchvision.datasets. The tutorial uses only the first 1000 rows for training \n",
    "and 500 rows for test to speed-up the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[n/255.\n",
    "                    for n in [129.3, 124.1, 112.4]], std=[n/255. for n in [68.2,  65.4,  70.4]])\n",
    "]) # meanstd transformation\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[n/255.\n",
    "                    for n in [129.3, 124.1, 112.4]], std=[n/255. for n in [68.2,  65.4,  70.4]])\n",
    "])\n",
    "\n",
    "data_train = datasets.CIFAR10(root = \"/tmp/\", download = True, transform = transform_train)\n",
    "data_train.data = data_train.data[0:1000, :]\n",
    "data_train.targets = data_train.targets[0:1000]\n",
    "\n",
    "data_test = datasets.CIFAR10(root = \"/tmp/\", download = True, train = False, transform = transform_test)\n",
    "data_test.data = data_test.data[0:500, :]\n",
    "data_test.targets = data_test.targets[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate the ResNet50 operator with `num_classes` as 10 and `num_epochs` as 1 and call `fit` to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kakateus.ibm.com/venv/lale39/lib/python3.9/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and will be removed in 0.15. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/Users/kakateus.ibm.com/venv/lale39/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=> Training Epoch 0, LR=0.1000\n",
      "| Epoch [  0/  1] Iter[  1/ 16]\t\tLoss: 13.1739 Acc@1: 0.000%\n",
      "| Epoch [  0/  1] Iter[  2/ 16]\t\tLoss: 7.8341 Acc@1: 5.469%\n",
      "| Epoch [  0/  1] Iter[  3/ 16]\t\tLoss: 5.0676 Acc@1: 7.292%\n",
      "| Epoch [  0/  1] Iter[  4/ 16]\t\tLoss: 3.4731 Acc@1: 9.375%\n",
      "| Epoch [  0/  1] Iter[  5/ 16]\t\tLoss: 3.2107 Acc@1: 9.375%\n",
      "| Epoch [  0/  1] Iter[  6/ 16]\t\tLoss: 6.1151 Acc@1: 10.417%\n",
      "| Epoch [  0/  1] Iter[  7/ 16]\t\tLoss: 8.2757 Acc@1: 9.821%\n",
      "| Epoch [  0/  1] Iter[  8/ 16]\t\tLoss: 2.7197 Acc@1: 10.938%\n",
      "| Epoch [  0/  1] Iter[  9/ 16]\t\tLoss: 5.3714 Acc@1: 11.285%\n",
      "| Epoch [  0/  1] Iter[ 10/ 16]\t\tLoss: 5.8829 Acc@1: 10.625%\n",
      "| Epoch [  0/  1] Iter[ 11/ 16]\t\tLoss: 4.0505 Acc@1: 10.795%\n",
      "| Epoch [  0/  1] Iter[ 12/ 16]\t\tLoss: 12.3540 Acc@1: 11.068%\n",
      "| Epoch [  0/  1] Iter[ 13/ 16]\t\tLoss: 8.9091 Acc@1: 10.817%\n",
      "| Epoch [  0/  1] Iter[ 14/ 16]\t\tLoss: 3.7056 Acc@1: 10.714%\n",
      "| Epoch [  0/  1] Iter[ 15/ 16]\t\tLoss: 3.7871 Acc@1: 10.104%\n",
      "| Epoch [  0/  1] Iter[ 16/ 16]\t\tLoss: 3.1307 Acc@1: 10.100%\n"
     ]
    }
   ],
   "source": [
    "clf = ResNet50(num_classes=10,num_epochs = 1)\n",
    "fitted_clf = clf.fit(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `predict` to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[779]\n",
      " [814]\n",
      " [536]\n",
      " [860]\n",
      " [953]\n",
      " [996]\n",
      " [625]\n",
      " [845]\n",
      " [956]\n",
      " [586]]\n"
     ]
    }
   ],
   "source": [
    "predicted = fitted_clf.predict(data_test)\n",
    "print(predicted[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kakateus.ibm.com/venv/lale39/lib/python3.9/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and will be removed in 0.15. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/Users/kakateus.ibm.com/venv/lale39/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                    \n",
      "=> Training Epoch 0, LR=0.1000\n",
      "| Epoch [  0/  1] Iter[  1/ 22]\t\tLoss: 12.4900 Acc@1: 0.000%                                                                                                                                        \n",
      "| Epoch [  0/  1] Iter[  2/ 22]\t\tLoss: 9.1250 Acc@1: 1.087%                                                                                                                                         \n",
      "| Epoch [  0/  1] Iter[  3/ 22]\t\tLoss: 6.4047 Acc@1: 5.797%                                                                                                                                         \n",
      "| Epoch [  0/  1] Iter[  4/ 22]\t\tLoss: 3.2886 Acc@1: 7.609%                                                                                                                                         \n",
      "| Epoch [  0/  1] Iter[  5/ 22]\t\tLoss: 6.3268 Acc@1: 7.391%                                                                                                                                         \n",
      "| Epoch [  0/  1] Iter[  6/ 22]\t\tLoss: 2.7232 Acc@1: 9.058%                                                                                                                                         \n",
      "| Epoch [  0/  1] Iter[  7/ 22]\t\tLoss: 2.4921 Acc@1: 9.317%                                                                                                                                         \n",
      "| Epoch [  0/  1] Iter[  8/ 22]\t\tLoss: 8.5407 Acc@1: 10.054%                                                                                                                                        \n",
      "| Epoch [  0/  1] Iter[  9/ 22]\t\tLoss: 6.8186 Acc@1: 10.870%                                                                                                                                        \n",
      "| Epoch [  0/  1] Iter[ 10/ 22]\t\tLoss: 7.0229 Acc@1: 10.870%                                                                                                                                        \n",
      "| Epoch [  0/  1] Iter[ 11/ 22]\t\tLoss: 7.5339 Acc@1: 10.870%                                                                                                                                        \n",
      "| Epoch [  0/  1] Iter[ 12/ 22]\t\tLoss: 7.1085 Acc@1: 10.688%                                                                                                                                        \n",
      "| Epoch [  0/  1] Iter[ 13/ 22]\t\tLoss: 3.4468 Acc@1: 10.702%                                                                                                                                        \n",
      "| Epoch [  0/  1] Iter[ 14/ 22]\t\tLoss: 12.4372 Acc@1: 10.404%                                                                                                                                       \n",
      "| Epoch [  0/  1] Iter[ 15/ 22]\t\tLoss: 6.7403 Acc@1: 9.855%                                                                                                                                         \n",
      "| Epoch [  0/  1] Iter[ 16/ 22]\t\tLoss: 13.8508 Acc@1: 10.054%                                                                                                                                       \n",
      "| Epoch [  0/  1] Iter[ 17/ 22]\t\tLoss: 3.0319 Acc@1: 10.230%                                                                                                                                        \n",
      "| Epoch [  0/  1] Iter[ 18/ 22]\t\tLoss: 7.2646 Acc@1: 10.386%                                                                                                                                        \n",
      "| Epoch [  0/  1] Iter[ 19/ 22]\t\tLoss: 3.6919 Acc@1: 10.755%                                                                                                                                        \n",
      "| Epoch [  0/  1] Iter[ 20/ 22]\t\tLoss: 4.6062 Acc@1: 10.870%                                                                                                                                        \n",
      "| Epoch [  0/  1] Iter[ 21/ 22]\t\tLoss: 6.0512 Acc@1: 10.973%                                                                                                                                        \n",
      "| Epoch [  0/  1] Iter[ 22/ 22]\t\tLoss: 4.9309 Acc@1: 10.900%                                                                                                                                        \n",
      "                                                                                                                                                                                                    \n",
      "=> Training Epoch 0, LR=0.1000\n",
      "| Epoch [  0/  1] Iter[  1/ 12]\t\tLoss: 13.9572 Acc@1: 0.000%                                                                                                                                        \n",
      "| Epoch [  0/  1] Iter[  2/ 12]\t\tLoss: 6.9697 Acc@1: 5.556%                                                                                                                                         \n",
      "| Epoch [  0/  1] Iter[  3/ 12]\t\tLoss: 4.5305 Acc@1: 7.407%                                                                                                                                         \n",
      "| Epoch [  0/  1] Iter[  4/ 12]\t\tLoss: 4.2172 Acc@1: 8.611%                                                                                                                                         \n",
      "| Epoch [  0/  1] Iter[  5/ 12]\t\tLoss: 4.2803 Acc@1: 8.444%                                                                                                                                         \n",
      "| Epoch [  0/  1] Iter[  6/ 12]\t\tLoss: 3.9195 Acc@1: 9.444%                                                                                                                                         \n",
      "| Epoch [  0/  1] Iter[  7/ 12]\t\tLoss: 7.2395 Acc@1: 8.889%                                                                                                                                         \n",
      "| Epoch [  0/  1] Iter[  8/ 12]\t\tLoss: 7.1558 Acc@1: 8.750%                                                                                                                                         \n",
      "| Epoch [  0/  1] Iter[  9/ 12]\t\tLoss: 3.6309 Acc@1: 8.765%                                                                                                                                         \n",
      "| Epoch [  0/  1] Iter[ 10/ 12]\t\tLoss: 5.8267 Acc@1: 9.444%                                                                                                                                         \n",
      "| Epoch [  0/  1] Iter[ 11/ 12]\t\tLoss: 8.5973 Acc@1: 9.192%                                                                                                                                         \n",
      "| Epoch [  0/  1] Iter[ 12/ 12]\t\tLoss: 6.6412 Acc@1: 9.200%                                                                                                                                         \n",
      "                                                                                                                                                                                                    \n",
      "=> Training Epoch 0, LR=0.1000\n",
      "| Epoch [  0/  1] Iter[  1/ 13]\t\tLoss: 12.8990 Acc@1: 0.000%                                                                                                                                        \n",
      "| Epoch [  0/  1] Iter[  2/ 13]\t\tLoss: 7.3902 Acc@1: 5.422%                                                                                                                                         \n",
      "| Epoch [  0/  1] Iter[  3/ 13]\t\tLoss: 5.3940 Acc@1: 6.024%                                                                                                                                         \n",
      "| Epoch [  0/  1] Iter[  4/ 13]\t\tLoss: 3.8495 Acc@1: 8.133%                                                                                                                                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch [  0/  1] Iter[  5/ 13]\t\tLoss: 6.9148 Acc@1: 8.916%                                                                                                                                         \n",
      "| Epoch [  0/  1] Iter[  6/ 13]\t\tLoss: 4.8030 Acc@1: 10.241%                                                                                                                                        \n",
      "| Epoch [  0/  1] Iter[  7/ 13]\t\tLoss: 2.9323 Acc@1: 10.843%                                                                                                                                        \n",
      "| Epoch [  0/  1] Iter[  8/ 13]\t\tLoss: 6.3967 Acc@1: 10.994%                                                                                                                                        \n",
      "| Epoch [  0/  1] Iter[  9/ 13]\t\tLoss: 3.5952 Acc@1: 10.843%                                                                                                                                        \n",
      "| Epoch [  0/  1] Iter[ 10/ 13]\t\tLoss: 4.8311 Acc@1: 10.482%                                                                                                                                        \n",
      "| Epoch [  0/  1] Iter[ 11/ 13]\t\tLoss: 3.7614 Acc@1: 10.186%                                                                                                                                        \n",
      "| Epoch [  0/  1] Iter[ 12/ 13]\t\tLoss: 2.3194 Acc@1: 10.442%                                                                                                                                        \n",
      "| Epoch [  0/  1] Iter[ 13/ 13]\t\tLoss: 6.9638 Acc@1: 10.600%                                                                                                                                        \n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [02:24<00:00, 48.01s/trial, best loss: 45667.341617268845]\n",
      "\n",
      "=> Training Epoch 0, LR=0.1000\n",
      "| Epoch [  0/  1] Iter[  1/ 22]\t\tLoss: 13.2644 Acc@1: 0.000%\n",
      "| Epoch [  0/  1] Iter[  2/ 22]\t\tLoss: 7.6059 Acc@1: 5.435%\n",
      "| Epoch [  0/  1] Iter[  3/ 22]\t\tLoss: 6.8300 Acc@1: 6.522%\n",
      "| Epoch [  0/  1] Iter[  4/ 22]\t\tLoss: 4.4189 Acc@1: 7.609%\n",
      "| Epoch [  0/  1] Iter[  5/ 22]\t\tLoss: 5.2890 Acc@1: 8.696%\n",
      "| Epoch [  0/  1] Iter[  6/ 22]\t\tLoss: 4.8055 Acc@1: 8.333%\n",
      "| Epoch [  0/  1] Iter[  7/ 22]\t\tLoss: 5.1875 Acc@1: 8.075%\n",
      "| Epoch [  0/  1] Iter[  8/ 22]\t\tLoss: 2.4937 Acc@1: 8.967%\n",
      "| Epoch [  0/  1] Iter[  9/ 22]\t\tLoss: 8.2413 Acc@1: 8.454%\n",
      "| Epoch [  0/  1] Iter[ 10/ 22]\t\tLoss: 2.6074 Acc@1: 8.913%\n",
      "| Epoch [  0/  1] Iter[ 11/ 22]\t\tLoss: 5.9518 Acc@1: 8.893%\n",
      "| Epoch [  0/  1] Iter[ 12/ 22]\t\tLoss: 11.1031 Acc@1: 9.239%\n",
      "| Epoch [  0/  1] Iter[ 13/ 22]\t\tLoss: 9.2100 Acc@1: 9.365%\n",
      "| Epoch [  0/  1] Iter[ 14/ 22]\t\tLoss: 13.6165 Acc@1: 9.161%\n",
      "| Epoch [  0/  1] Iter[ 15/ 22]\t\tLoss: 11.1261 Acc@1: 9.275%\n",
      "| Epoch [  0/  1] Iter[ 16/ 22]\t\tLoss: 7.9228 Acc@1: 9.783%\n",
      "| Epoch [  0/  1] Iter[ 17/ 22]\t\tLoss: 8.4336 Acc@1: 9.719%\n",
      "| Epoch [  0/  1] Iter[ 18/ 22]\t\tLoss: 9.5396 Acc@1: 9.541%\n",
      "| Epoch [  0/  1] Iter[ 19/ 22]\t\tLoss: 11.9275 Acc@1: 9.725%\n",
      "| Epoch [  0/  1] Iter[ 20/ 22]\t\tLoss: 4.0334 Acc@1: 9.565%\n",
      "| Epoch [  0/  1] Iter[ 21/ 22]\t\tLoss: 10.0826 Acc@1: 9.524%\n",
      "| Epoch [  0/  1] Iter[ 22/ 22]\t\tLoss: 11.4108 Acc@1: 9.600%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Hyperopt(estimator=lale.operators.TrainableIndividualOp(_lale_name='ResNet50', _lale_schemas={'description': 'Combined schema for expected data and hyperparameters for a transformer for pytorch implementation of ResNet50 for image classification.', '$schema': 'http://json-schema.org/draft-04/schema#', 'properties': {'hyperparams': {'$schema': 'http://json-schema.org/draft-04/schema#', 'allOf': [{'relevantToOptimizer': ['num_epochs', 'batch_size', 'lr_scheduler'], 'properties': {'learning_rate_init': {'minimumForOptimizer': 1e-05, 'type': 'number', 'description': 'Initial value of learning rate for training.', 'minimum': 0, 'distribution': 'loguniform', 'default': 0.1, 'maximumForOptimizer': 0.1}, 'num_epochs': {'maximumForOptimizer': 200, 'default': 2, 'distribution': 'uniform', 'description': 'The number of epochs used for training.', 'minimum': 0, 'minimumForOptimizer': 2, 'type': 'integer'}, 'num_classes': {'minimum': 2, 'default': 10, 'description': 'Number of classes.', 'type': 'integer'}, 'lr_scheduler': {'enum': ['constant', 'decay'], 'default': 'constant', 'description': 'Learning rate scheduler for training.'}, 'batch_size': {'distribution': 'uniform', 'default': 64, 'maximumForOptimizer': 128, 'description': 'Batch size used for training and prediction.', 'type': 'integer', 'minimumForOptimizer': 32, 'minimum': 1}}, 'required': ['num_classes', 'num_epochs', 'batch_size', 'lr_scheduler', 'learning_rate_init'], 'additionalProperties': False, 'description': 'This first sub-object lists all constructor arguments with their types, one at a time, omitting cross-argument constraints.', 'type': 'object'}], 'description': 'Hyperparameter schema.'}}, 'tags': {'op': ['estimator'], 'pre': ['images'], 'post': []}, 'type': 'object'}, _lale_impl=lale.operators._WithoutGetParams(), num_classes=10, num_epochs=1, batch_size=64, lr_scheduler='constant', learning_rate_init=0.1, _lale_frozen_hyperparameters=['num_classes', 'num_epochs']), max_evals=3, cv=None, verbose=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lale.lib.lale import Hyperopt\n",
    "\n",
    "opt = Hyperopt(estimator=clf, max_evals=3, cv= None, verbose=True)\n",
    "opt.fit(data_train, y=None, X_valid=data_test, y_valid=data_test.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, we have learned how to write an operator implementation class and hyperparameter\n",
    "schema; how to register the Lale operator; and how to use the Lale operator for\n",
    "manual as well as automated machine-learning.\n",
    "\n",
    "The next section has more details on options in JSON Schema and can be used as a reference when creating Lale schemas.\n",
    "\n",
    "\n",
    "## 8.5 Appendix\n",
    "\n",
    "This section documents features of JSON Schema that Lale uses, as well as\n",
    "extensions that Lale adds to JSON schema for information specific to the\n",
    "machine-learning domain. For a more comprehensive introduction to JSON\n",
    "Schema, refer to its\n",
    "[Reference](https://json-schema.org/understanding-json-schema/reference/).\n",
    "\n",
    "The following table lists kinds of schemas in JSON Schema:\n",
    "\n",
    "| Kind of schema | Corresponding type in Python/Lale |\n",
    "| ---------------| ---------------------------- |\n",
    "| `null`         | `NoneType`, value `None` |\n",
    "| `boolean`      | `bool`, values `True` or `False` |\n",
    "| `string`       | `str` |\n",
    "| `enum`         | See discussion below. |\n",
    "| `number`       | `float`, .e.g, `0.1` |\n",
    "| `integer`      | `int`, e.g., `42` |\n",
    "| `array`        | See discussion below. |\n",
    "| `object`       | `dict` with string keys |\n",
    "| `anyOf`, `allOf`, `not` | See discussion below. |\n",
    "\n",
    "The use of `null`, `boolean`, and `string` is fairly straightforward.  The\n",
    "following paragraphs discuss the other kinds of schemas one by one.\n",
    "\n",
    "### 8.5.1 enum\n",
    "\n",
    "In JSON Schema, an enum can contain assorted values including strings,\n",
    "numbers, or even `null`. Lale uses enums of strings for categorical\n",
    "hyperparameters, such as `'lr_scheduler': {'enum': ['constant', 'decay']}` in the earlier\n",
    "example. In that case, Lale also automatically declares a corresponding\n",
    "Python `enum`.\n",
    "When Lale uses enums of other types, it is usually to restrict a\n",
    "hyperparameter to a single value, such as `'enum': [None]`.\n",
    "\n",
    "### 8.5.2 number, integer\n",
    "\n",
    "In schemas with `type` set to `number` or `integer`, JSON schema lets users\n",
    "specify `minimum`, `maximum`,\n",
    "`exclusiveMinimum`, and `exclusiveMaximum`. Lale further extends JSON schema\n",
    "with `minimumForOptimizer`, `maximumForOptimizer`, and `distribution`.\n",
    "Possible values for the `distribution` are `'uniform'` (the default) and\n",
    "`'loguniform'`. In the case of integers, Lale quantizes the distributions\n",
    "accordingly.\n",
    "\n",
    "### 8.5.3 array\n",
    "\n",
    "Lale schemas for input and output data make heavy use of the JSON Schema\n",
    "`array` type. In this case, Lale schemas are intended to capture logical\n",
    "schemas, not physical representations, similar to how relational databases\n",
    "hide physical representations behind a well-formalized abstraction layer.\n",
    "Therefore, Lale uses arrays from JSON Schema for several types in Python.\n",
    "The most obvious one is a Python `list`. Another common one is a numpy\n",
    "[ndarray](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html),\n",
    "where Lale uses nested arrays to represent each of the dimensions of a\n",
    "multi-dimensional array. Lale also has support for `pandas.DataFrame` and\n",
    "`pandas.Series`, for which it again uses JSON Schema arrays.\n",
    "\n",
    "For arrays, JSON schema lets users specify `items`, `minItems`, and\n",
    "`maxItems`. Lale further extends JSON schema with `minItemsForOptimizer` and\n",
    "`maxItemsForOptimizer`. Furthermore, Lale supports a `laleType`,\n",
    "which can be `'Any'` to locally disable a subschema check, or`'tuple'` to support cases where the Python code requires a\n",
    "tuple instead of a list.\n",
    "\n",
    "### 8.5.4 object\n",
    "\n",
    "For objects, JSON schema lets users specify a list `required` of properties\n",
    "that must be present, a dictionary `properties` of sub-schemas, and a flag\n",
    "`additionalProperties` to indicate whether the object can have additional\n",
    "properties beyond the keys of the `properties` dictionary. Lale further\n",
    "extends JSON schema with a `relevantToOptimizer` list of properties that\n",
    "hyperparameter optimizers should search over.\n",
    "\n",
    "For individual properties, Lale supports a `default`, which is inspired by\n",
    "and consistent with web API specification practice. It also supports a\n",
    "`forOptimizer` flag which defaults to `True` but can be set to `False` to\n",
    "hide a particular subschema from the hyperparameter optimizer. For example,\n",
    "the number of components for PCA in scikit-learn can be specified as an\n",
    "integer or a floating point number, but an optimizer should only explore one\n",
    "of these choices. Lale supports a Boolean flag `transient` that, if true,\n",
    "elides a hyperparameter during pretty-printing, visualization, or in JSON.\n",
    "\n",
    "### 8.5.5 allOf, anyOf, not\n",
    "\n",
    "In JSON schema, `allOf` is a logical \"and\", `anyOf` is\n",
    "a logical \"or\", and `not` is a logical negation. The running example from\n",
    "earlier illustrated `allOf` at the top level in the hyperparameter schema.\n",
    "A use-case that takes advantage of `anyOf` is for\n",
    "expressing union types, which arise frequently in scikit-learn. For example,\n",
    "here is the schema for `n_components` from PCA:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"n_components\": {\n",
    "    \"anyOf\": [\n",
    "        {\n",
    "            \"description\": \"If not set, keep all components.\",\n",
    "            \"enum\": [None],\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"Use Minka's MLE to guess the dimension.\",\n",
    "            \"enum\": [\"mle\"],\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"\"\"Select the number of components such that the amount of variance that needs to be explained is greater than the specified percentage.\"\"\",\n",
    "            \"type\": \"number\",\n",
    "            \"minimum\": 0.0,\n",
    "            \"exclusiveMinimum\": True,\n",
    "            \"maximum\": 1.0,\n",
    "            \"exclusiveMaximum\": True,\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"Number of components to keep.\",\n",
    "            \"type\": \"integer\",\n",
    "            \"minimum\": 1,\n",
    "            \"laleMaximum\": \"X/items/maxItems\",  # number of columns\n",
    "            \"forOptimizer\": False,\n",
    "        },\n",
    "    ],\n",
    "    \"default\": None,\n",
    "},"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
