{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDD 2022 Hands-on Tutorial on \"Gradual AutoML using Lale\"\n",
    "\n",
    "# 8. Add a New Operator\n",
    "\n",
    "Lale comes with several [library operators](https://lale.readthedocs.io/en/latest/#operator-libraries),\n",
    "so you do not need to write\n",
    "your own. But if you want to use Lale for AutoML that includes your custom models,\n",
    "you will need to create a new Lale operator that wraps the custom model.\n",
    "\n",
    "This tutorial illustrates how to do that using ResNet-50 for image classification as a running example.\n",
    "The following four steps highlight the process to add a new operator:\n",
    "\n",
    "-   [8.1 Create Implementation Class](#impl)\n",
    "-   [8.2 Add Hyperparameter Schema](#hyperparam_schema)\n",
    "-   [8.3 Register a New Lale Operator](#make_operator)\n",
    "-   [8.4 Test and Use the New operator](#use_operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"impl\"></a>\n",
    "## 8.1 Create Implementation Class\n",
    "\n",
    "The implementation class of an operator needs to have methods `__init__`,\n",
    "`fit`, and `predict` or `transform`. Any other compatibility with\n",
    "scikit-learn such as `get_params` or `set_params` is optional, and so\n",
    "is extending from `sklearn.base.BaseEstimator`.\n",
    "\n",
    "This section illustrates how to implement this class for our new operator `ResNet50`.\n",
    "We call this class `_ResNet50Impl` and it uses  ResNet-50 implementation available in the torchvision library (`torchvision.models.resnet50`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://kakate%40us.ibm.com:****@na.artifactory.swg-devops.com/artifactory/api/pypi/wcp-nlp-pypi-virtual/simple\n",
      "Requirement already satisfied: torch in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (1.12.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from torch) (4.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/Users/kakateus.ibm.com/venv/lale_tutorial/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://kakate%40us.ibm.com:****@na.artifactory.swg-devops.com/artifactory/api/pypi/wcp-nlp-pypi-virtual/simple\n",
      "Requirement already satisfied: torchvision in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (0.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from torchvision) (4.3.0)\n",
      "Requirement already satisfied: numpy in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from torchvision) (1.23.1)\n",
      "Requirement already satisfied: requests in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: torch==1.12.1 in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from torchvision) (1.12.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from requests->torchvision) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from requests->torchvision) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/kakateus.ibm.com/venv/lale_tutorial/lib/python3.9/site-packages (from torchvision) (4.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/Users/kakateus.ibm.com/venv/lale_tutorial/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Install dependencies required by this tutorial\n",
    "!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "class _ResNet50Impl():\n",
    "    \n",
    "    #__init__ that takes the hyperparameters as keyword arguments.\n",
    "    def __init__(self, num_classes=10,\n",
    "            num_epochs = 2, batch_size = 128, learning_rate_init=0.1,\n",
    "            lr_scheduler = 'constant'):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1).to(self.device)\n",
    "        self.model.fc = nn.Linear(2048, num_classes)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        \"\"\"Fit method for ResNet50.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Pytorch Dataset object.\n",
    "          Pytorch dataset that contains the training data and targets.\n",
    "        y : optional\n",
    "          This is ignored.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ResNet50Impl\n",
    "          A new object that is trained.\n",
    "        \"\"\"\n",
    "        trainloader = torch.utils.data.DataLoader(X, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "        self.model.train()\n",
    "        \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "            \n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for epoch in range(self.num_epochs):\n",
    "            learning_rate = self.calculate_learning_rate(epoch)\n",
    "            optimizer = optim.SGD(self.model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "            print('\\n=> Training Epoch %d, LR=%.4f' %(epoch, learning_rate))\n",
    "            for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(inputs)         \n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.data.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "                \n",
    "                print('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
    "                        %(epoch, self.num_epochs, batch_idx+1,\n",
    "                            (len(X)//self.batch_size)+1, loss.data.item(), 100.*correct/total))\n",
    "\n",
    "        return _ResNet50Impl(self.num_classes, self.model, self.num_epochs, self.batch_size,\n",
    "                            self.lr_scheduler) \n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        dataloader = torch.utils.data.DataLoader(X, batch_size=self.batch_size, shuffle=False, num_workers=2)\n",
    "        predicted_X = None\n",
    "        for batch_idx, data in enumerate(dataloader):\n",
    "            if isinstance(data, list) or isinstance(data, tuple):\n",
    "                inputs = data[0] #For standard datasets from torchvision, data is a list with X and y\n",
    "            else:\n",
    "                inputs = data\n",
    "            inputs = inputs.to(self.device)\n",
    "            outputs = self.model(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            predicted = predicted.detach().cpu().numpy()\n",
    "            predicted = np.reshape(predicted, (predicted.shape[0], 1))\n",
    "            if predicted_X is None:\n",
    "                predicted_X = predicted\n",
    "            else:\n",
    "                predicted_X = np.vstack((predicted_X, predicted))\n",
    "        self.model.train()\n",
    "        return predicted_X\n",
    "\n",
    "    def calculate_learning_rate(self, epoch):\n",
    "        import math\n",
    "        if self.lr_scheduler == 'constant':\n",
    "            return self.learning_rate_init\n",
    "        elif self.lr_scheduler == 'decay':\n",
    "            optim_factor = 0\n",
    "            if(epoch > 160):\n",
    "                optim_factor = 3\n",
    "            elif(epoch > 120):\n",
    "                optim_factor = 2\n",
    "            elif(epoch > 60):\n",
    "                optim_factor = 1\n",
    "\n",
    "            return self.learning_rate_init*math.pow(0.2, optim_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code first imports the relevant torchvision and torch packages. Then, it declares\n",
    "a new class for wrapping `torchvision.models.resnet50`. The\n",
    "Lale approach for wrapping new operators carefully avoids depending too much\n",
    "on the Python language or any particular Python library. Hence, the\n",
    "`_ResNet50Impl` class does not need to inherit from anything, but it does need to\n",
    "follow certain conventions:\n",
    "\n",
    "* It has a constructor, `__init__`, whose arguments are the\n",
    "  hyperparameters as keyword arguments.\n",
    "\n",
    "* It has a training method, `fit`, with an argument `X` containing the\n",
    "  training examples and, in the case of supervised models, an argument `y`\n",
    "  containing labels. Since the torch Dataset has both data and targets for this case,\n",
    "  the default value of y is None and it is ignored during training.\n",
    "  The `fit` method trains the neural network by looping for epochs and batches, \n",
    "  and returns the wrapper object with the trained model.\n",
    "\n",
    "* It has a prediction method, `predict` for an estimator or `transform` for\n",
    "  a transformer. The method has an argument `X` containing the test examples\n",
    "  and returns the labels for `predict` or the transformed data for\n",
    "  `transform`.\n",
    "\n",
    "These conventions are designed to be similar to those of scikit-learn.\n",
    "However, they avoid a code dependency upon scikit-learn.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"hyperparam_schema\"></a>\n",
    "## 8.2 Add Hyperparameter Schema\n",
    "\n",
    "Lale uses hyperparameter schemas both for error-checking and for generating search\n",
    "spaces for hyperparameter optimization.\n",
    "A hyperparameter schema specifies the space of valid types and values for\n",
    "hyperparameters and also the ranges for hyperparameter optimization.\n",
    "Schemas are also used to specify types for the arguments to `fit` and `predict` or `transform`,\n",
    "and for the output of `predict` or `transform`. All the schemas are optional,\n",
    "and Lale will skip the corresponding functionality such as error checking or search\n",
    "space generation if they are not specified.\n",
    "For meaningful hyperparameter tuning, a Lale operator needs a hyperparameter schema at the minimum.\n",
    "\n",
    "\n",
    "In this section, we will focus on creating a hyperparameter schema for our ResNet50 operator.\n",
    "To keep the schemas independent of the Python programming language, they are expressed as\n",
    "[JSON Schema](https://json-schema.org/understanding-json-schema/reference/).\n",
    "JSON Schema is currently a draft standard and is already being widely\n",
    "adopted and implemented, for instance, as part of specifying\n",
    "[Swagger APIs](https://www.openapis.org/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_hyperparams_schema = {\n",
    "  '$schema': 'http://json-schema.org/draft-04/schema#',\n",
    "  'description': 'Hyperparameter schema.',\n",
    "  'allOf': [\n",
    "    { 'description':\n",
    "        'This first sub-object lists all constructor arguments with their '\n",
    "        'types, one at a time, omitting cross-argument constraints.',\n",
    "      'type': 'object',\n",
    "      'additionalProperties': False,\n",
    "      'required': ['num_classes', 'num_epochs',\n",
    "       'batch_size', 'lr_scheduler', 'learning_rate_init'],\n",
    "      'relevantToOptimizer': ['num_epochs', 'batch_size', 'lr_scheduler'],\n",
    "      'properties': {\n",
    "        'num_classes': {\n",
    "          'description': 'Number of classes.',\n",
    "          'type': 'integer',\n",
    "          'default': 10,\n",
    "          'minimum': 2},\n",
    "        'num_epochs':{\n",
    "          'description': 'The number of epochs used for training.',\n",
    "          'type': 'integer',\n",
    "          'default': 2,\n",
    "          'minimum': 0,\n",
    "          'distribution': 'uniform',\n",
    "          'minimumForOptimizer': 2,\n",
    "          'maximumForOptimizer': 200},\n",
    "        'batch_size':{\n",
    "          'description': 'Batch size used for training and prediction.',\n",
    "          'type': 'integer',\n",
    "          'default': 64,\n",
    "          'minimum': 1,\n",
    "          'distribution': 'uniform',\n",
    "          'minimumForOptimizer': 32,\n",
    "          'maximumForOptimizer': 128},\n",
    "        'lr_scheduler':{\n",
    "          'description': 'Learning rate scheduler for training.',\n",
    "          'enum': ['constant', 'decay'],\n",
    "          'default': 'constant'},           \n",
    "        'learning_rate_init':{\n",
    "          'description': 'Initial value of learning rate for training.',\n",
    "          'type': 'number',\n",
    "          'default': 0.1,\n",
    "          'minimum': 0,\n",
    "          'distribution': 'loguniform',\n",
    "          'minimumForOptimizer': 1e-05,\n",
    "          'maximumForOptimizer': 0.1\n",
    "        }}}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `num_classes`, `num_epochs` and `batch_size` are integer hyperparameters, `lr_scheduler` is a\n",
    "categorical hyperparameter, and `learning_rate_init` is a float. \n",
    "For all the hyperparameters, the schema\n",
    "includes a `type` which indicates the data type, \n",
    "`description` which is used for interactive documentation, and a\n",
    "`default` value. Note that the JSON Schema type for float is `number`. \n",
    "For integer and float hyperparameters, a `minimum` value is specified.\n",
    "The categorical hyperparameter is specified as an enumeration of its allowed values.\n",
    "\n",
    "The schema has a list called `relevantToOptimizer` which includes hyperparameters to be tuned\n",
    "during AutoML.\n",
    "For such hyperparameters, the schema\n",
    "includes additional information such as its `distribution`, `minimumForOptimizer`, and\n",
    "`maximumForOptimizer` guiding the optimizer to limit its search space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"make_operator\"></a>\n",
    "## 8.3 Register a New Lale Operator\n",
    "\n",
    "We can now register `_ResNet50Impl` as a new Lale operator `ResNet50`. This is done by calling `make_operator` and passing the implementation class and a dictionary with information such as the `description` of the operator, `tags` which include additional information about what kind of operator it is, and the hyperparameter schema given by key `hyperparams`. For `tags`, it is required to have a key `op` indicating whether it is an `estimator` or `transformer` per scikit-learn terminology. Information such as `classifier` or `regressor` is encouraged but optional and `pre` and `post` are optional as well. Please refer to `lale.lib` for more examples of `tags`.\n",
    "\n",
    "As mentioned earlier, the other schemas such as `input_fit`, `input_predict`, `output_fit`, and `output_predict` specify data types of input/output of fit and predict. We will omit those for this tutorial, but `lale.lib` has many examples to refer to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lale.operators import make_operator\n",
    "\n",
    "_combined_schemas = {\n",
    "  '$schema': 'http://json-schema.org/draft-04/schema#',\n",
    "  'description': 'Combined schema for expected data and hyperparameters for a transformer for'\n",
    "                ' pytorch implementation of ResNet50 for image classification.',\n",
    "  'type': 'object',\n",
    "  'tags': {'pre': ['images'], 'op': ['estimator'], 'post': []},\n",
    "  'properties': {\n",
    "    'hyperparams': _hyperparams_schema}}\n",
    "\n",
    "ResNet50 = make_operator(_ResNet50Impl, _combined_schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"use_operator\"></a>\n",
    "## 8.4 Testing and Using the New Operator\n",
    "\n",
    "Once your operator implementation and schema definitions are ready,\n",
    "you can test it with Lale as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.1 Train and Use the New Operator\n",
    "\n",
    "Before demonstrating the new `ResNet50` operator, the following code loads the\n",
    "CIFAR10 dataset from torchvision.datasets. The tutorial uses only the first 1000 rows for training \n",
    "and 500 rows for test to speed-up the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[n/255.\n",
    "                    for n in [129.3, 124.1, 112.4]], std=[n/255. for n in [68.2,  65.4,  70.4]])\n",
    "]) # meanstd transformation\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[n/255.\n",
    "                    for n in [129.3, 124.1, 112.4]], std=[n/255. for n in [68.2,  65.4,  70.4]])\n",
    "])\n",
    "\n",
    "data_train = datasets.CIFAR10(root = \"/tmp/\", download = True, transform = transform_train)\n",
    "data_train.data = data_train.data[0:1000, :]\n",
    "data_train.targets = data_train.targets[0:1000]\n",
    "\n",
    "data_test = datasets.CIFAR10(root = \"/tmp/\", download = True, train = False, transform = transform_test)\n",
    "data_test.data = data_test.data[0:500, :]\n",
    "data_test.targets = data_test.targets[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate the ResNet50 operator with `num_classes` as 10 and `num_epochs` as 1 and call `fit` to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=> Training Epoch 0, LR=0.1000\n",
      "| Epoch [  0/  1] Iter[  1/ 16]\t\tLoss: 2.3903 Acc@1: 12.500%\n",
      "| Epoch [  0/  1] Iter[  2/ 16]\t\tLoss: 4.1925 Acc@1: 11.719%\n",
      "| Epoch [  0/  1] Iter[  3/ 16]\t\tLoss: 4.8753 Acc@1: 11.979%\n",
      "| Epoch [  0/  1] Iter[  4/ 16]\t\tLoss: 6.7370 Acc@1: 13.281%\n",
      "| Epoch [  0/  1] Iter[  5/ 16]\t\tLoss: 6.8356 Acc@1: 12.188%\n",
      "| Epoch [  0/  1] Iter[  6/ 16]\t\tLoss: 9.2414 Acc@1: 10.677%\n",
      "| Epoch [  0/  1] Iter[  7/ 16]\t\tLoss: 4.7938 Acc@1: 10.491%\n",
      "| Epoch [  0/  1] Iter[  8/ 16]\t\tLoss: 9.2306 Acc@1: 10.352%\n",
      "| Epoch [  0/  1] Iter[  9/ 16]\t\tLoss: 5.3865 Acc@1: 10.764%\n",
      "| Epoch [  0/  1] Iter[ 10/ 16]\t\tLoss: 4.0829 Acc@1: 10.469%\n",
      "| Epoch [  0/  1] Iter[ 11/ 16]\t\tLoss: 17.3871 Acc@1: 10.511%\n",
      "| Epoch [  0/  1] Iter[ 12/ 16]\t\tLoss: 12.2102 Acc@1: 10.807%\n",
      "| Epoch [  0/  1] Iter[ 13/ 16]\t\tLoss: 6.5490 Acc@1: 10.457%\n",
      "| Epoch [  0/  1] Iter[ 14/ 16]\t\tLoss: 7.7431 Acc@1: 10.156%\n",
      "| Epoch [  0/  1] Iter[ 15/ 16]\t\tLoss: 4.5282 Acc@1: 9.792%\n",
      "| Epoch [  0/  1] Iter[ 16/ 16]\t\tLoss: 11.8048 Acc@1: 9.800%\n"
     ]
    }
   ],
   "source": [
    "clf = ResNet50(num_classes=10,num_epochs = 1)\n",
    "fitted_clf = clf.fit(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `predict` to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7]\n",
      " [9]\n",
      " [9]\n",
      " [9]\n",
      " [9]\n",
      " [1]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [5]]\n"
     ]
    }
   ],
   "source": [
    "predicted = fitted_clf.predict(data_test)\n",
    "print(predicted[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.2 Configure the New Operator Using AutoML\n",
    "\n",
    "We can use an optimizer from Lale to search over ResNet50's hyperparameter ranges and find the best configuration. In this example, we will use [Hyperopt](https://lale.readthedocs.io/en/latest/modules/lale.lib.lale.hyperopt.html?highlight=Hyperopt#lale.lib.lale.hyperopt.Hyperopt) as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \n",
      "=> Training Epoch 0, LR=0.1000\n",
      "| Epoch [  0/  1] Iter[  1/ 22]\t\tLoss: 2.4664 Acc@1: 8.696%                                                             \n",
      "| Epoch [  0/  1] Iter[  2/ 22]\t\tLoss: 5.0420 Acc@1: 9.783%                                                             \n",
      "| Epoch [  0/  1] Iter[  3/ 22]\t\tLoss: 7.7805 Acc@1: 10.145%                                                            \n",
      "| Epoch [  0/  1] Iter[  4/ 22]\t\tLoss: 9.5116 Acc@1: 11.413%                                                            \n",
      "| Epoch [  0/  1] Iter[  5/ 22]\t\tLoss: 10.3371 Acc@1: 12.174%                                                           \n",
      "| Epoch [  0/  1] Iter[  6/ 22]\t\tLoss: 12.8799 Acc@1: 11.957%                                                           \n",
      "| Epoch [  0/  1] Iter[  7/ 22]\t\tLoss: 14.4987 Acc@1: 11.180%                                                           \n",
      "| Epoch [  0/  1] Iter[  8/ 22]\t\tLoss: 12.0768 Acc@1: 10.598%                                                           \n",
      "| Epoch [  0/  1] Iter[  9/ 22]\t\tLoss: 15.0667 Acc@1: 9.903%                                                            \n",
      "| Epoch [  0/  1] Iter[ 10/ 22]\t\tLoss: 18.3155 Acc@1: 9.783%                                                            \n",
      "| Epoch [  0/  1] Iter[ 11/ 22]\t\tLoss: 14.0785 Acc@1: 10.277%                                                           \n",
      "| Epoch [  0/  1] Iter[ 12/ 22]\t\tLoss: 17.1736 Acc@1: 9.964%                                                            \n",
      "| Epoch [  0/  1] Iter[ 13/ 22]\t\tLoss: 11.7621 Acc@1: 10.033%                                                           \n",
      "| Epoch [  0/  1] Iter[ 14/ 22]\t\tLoss: 6.7419 Acc@1: 10.093%                                                            \n",
      "| Epoch [  0/  1] Iter[ 15/ 22]\t\tLoss: 5.6633 Acc@1: 10.725%                                                            \n",
      "| Epoch [  0/  1] Iter[ 16/ 22]\t\tLoss: 6.8655 Acc@1: 10.462%                                                            \n",
      "| Epoch [  0/  1] Iter[ 17/ 22]\t\tLoss: 4.7357 Acc@1: 10.742%                                                            \n",
      "| Epoch [  0/  1] Iter[ 18/ 22]\t\tLoss: 5.2383 Acc@1: 10.628%                                                            \n",
      "| Epoch [  0/  1] Iter[ 19/ 22]\t\tLoss: 5.2354 Acc@1: 11.098%                                                            \n",
      "| Epoch [  0/  1] Iter[ 20/ 22]\t\tLoss: 10.3524 Acc@1: 10.870%                                                           \n",
      "| Epoch [  0/  1] Iter[ 21/ 22]\t\tLoss: 4.7503 Acc@1: 10.870%                                                            \n",
      "| Epoch [  0/  1] Iter[ 22/ 22]\t\tLoss: 2.9263 Acc@1: 10.900%                                                            \n",
      "                                                                                                                        \n",
      "=> Training Epoch 0, LR=0.1000\n",
      "| Epoch [  0/  1] Iter[  1/ 12]\t\tLoss: 2.4068 Acc@1: 12.222%                                                            \n",
      "| Epoch [  0/  1] Iter[  2/ 12]\t\tLoss: 3.3997 Acc@1: 15.556%                                                            \n",
      "| Epoch [  0/  1] Iter[  3/ 12]\t\tLoss: 4.7538 Acc@1: 11.852%                                                            \n",
      "| Epoch [  0/  1] Iter[  4/ 12]\t\tLoss: 2.9219 Acc@1: 12.500%                                                            \n",
      "| Epoch [  0/  1] Iter[  5/ 12]\t\tLoss: 3.6497 Acc@1: 12.444%                                                            \n",
      "| Epoch [  0/  1] Iter[  6/ 12]\t\tLoss: 4.7373 Acc@1: 12.407%                                                            \n",
      "| Epoch [  0/  1] Iter[  7/ 12]\t\tLoss: 3.6662 Acc@1: 12.857%                                                            \n",
      "| Epoch [  0/  1] Iter[  8/ 12]\t\tLoss: 4.7182 Acc@1: 12.222%                                                            \n",
      "| Epoch [  0/  1] Iter[  9/ 12]\t\tLoss: 3.6481 Acc@1: 12.469%                                                            \n",
      "| Epoch [  0/  1] Iter[ 10/ 12]\t\tLoss: 6.0994 Acc@1: 11.778%                                                            \n",
      "| Epoch [  0/  1] Iter[ 11/ 12]\t\tLoss: 7.4771 Acc@1: 11.919%                                                            \n",
      "| Epoch [  0/  1] Iter[ 12/ 12]\t\tLoss: 17.1753 Acc@1: 11.800%                                                           \n",
      "                                                                                                                        \n",
      "=> Training Epoch 0, LR=0.1000\n",
      "| Epoch [  0/  1] Iter[  1/ 13]\t\tLoss: 2.4020 Acc@1: 4.819%                                                             \n",
      "| Epoch [  0/  1] Iter[  2/ 13]\t\tLoss: 4.5377 Acc@1: 7.229%                                                             \n",
      "| Epoch [  0/  1] Iter[  3/ 13]\t\tLoss: 4.3889 Acc@1: 10.442%                                                            \n",
      "| Epoch [  0/  1] Iter[  4/ 13]\t\tLoss: 6.0713 Acc@1: 11.446%                                                            \n",
      "| Epoch [  0/  1] Iter[  5/ 13]\t\tLoss: 5.8682 Acc@1: 12.771%                                                            \n",
      "| Epoch [  0/  1] Iter[  6/ 13]\t\tLoss: 4.5630 Acc@1: 13.855%                                                            \n",
      "| Epoch [  0/  1] Iter[  7/ 13]\t\tLoss: 9.7708 Acc@1: 13.597%                                                            \n",
      "| Epoch [  0/  1] Iter[  8/ 13]\t\tLoss: 5.7600 Acc@1: 13.253%                                                            \n",
      "| Epoch [  0/  1] Iter[  9/ 13]\t\tLoss: 4.7119 Acc@1: 12.718%                                                            \n",
      "| Epoch [  0/  1] Iter[ 10/ 13]\t\tLoss: 22.9783 Acc@1: 12.048%                                                           \n",
      "| Epoch [  0/  1] Iter[ 11/ 13]\t\tLoss: 6.0256 Acc@1: 11.939%                                                            \n",
      "| Epoch [  0/  1] Iter[ 12/ 13]\t\tLoss: 17.4224 Acc@1: 11.446%                                                           \n",
      "| Epoch [  0/  1] Iter[ 13/ 13]\t\tLoss: 9.0023 Acc@1: 11.500%                                                            \n",
      "100%|███████████████████████████████████████████████████████████████| 3/3 [02:24<00:00, 48.08s/trial, best loss: -0.106]\n",
      "\n",
      "=> Training Epoch 0, LR=0.1000\n",
      "| Epoch [  0/  1] Iter[  1/ 13]\t\tLoss: 2.4335 Acc@1: 7.229%\n",
      "| Epoch [  0/  1] Iter[  2/ 13]\t\tLoss: 3.5311 Acc@1: 7.831%\n",
      "| Epoch [  0/  1] Iter[  3/ 13]\t\tLoss: 4.8742 Acc@1: 9.237%\n",
      "| Epoch [  0/  1] Iter[  4/ 13]\t\tLoss: 5.3256 Acc@1: 9.940%\n",
      "| Epoch [  0/  1] Iter[  5/ 13]\t\tLoss: 5.3993 Acc@1: 10.602%\n",
      "| Epoch [  0/  1] Iter[  6/ 13]\t\tLoss: 4.9392 Acc@1: 10.040%\n",
      "| Epoch [  0/  1] Iter[  7/ 13]\t\tLoss: 4.3182 Acc@1: 9.639%\n",
      "| Epoch [  0/  1] Iter[  8/ 13]\t\tLoss: 2.7743 Acc@1: 10.090%\n",
      "| Epoch [  0/  1] Iter[  9/ 13]\t\tLoss: 5.7511 Acc@1: 10.174%\n",
      "| Epoch [  0/  1] Iter[ 10/ 13]\t\tLoss: 6.7804 Acc@1: 10.361%\n",
      "| Epoch [  0/  1] Iter[ 11/ 13]\t\tLoss: 3.7746 Acc@1: 10.624%\n",
      "| Epoch [  0/  1] Iter[ 12/ 13]\t\tLoss: 2.2941 Acc@1: 11.245%\n",
      "| Epoch [  0/  1] Iter[ 13/ 13]\t\tLoss: 54.3695 Acc@1: 11.200%\n"
     ]
    }
   ],
   "source": [
    "from lale.lib.lale import Hyperopt\n",
    "\n",
    "opt = Hyperopt(estimator=clf, max_evals=3, cv= None, verbose=True)\n",
    "trained_opt = opt.fit(data_train, y=None, X_valid=data_test, y_valid=data_test.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"62pt\" height=\"45pt\"\n",
       " viewBox=\"0.00 0.00 62.00 44.77\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40.7696)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-40.7696 58,-40.7696 58,4 -4,4\"/>\n",
       "<!-- (root) -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>(root)</title>\n",
       "<g id=\"a_node1\"><a xlink:title=\"(root) = ResNet50(num_epochs=1, batch_size=83)\">\n",
       "<ellipse fill=\"#ffffff\" stroke=\"#000000\" cx=\"27\" cy=\"-18.3848\" rx=\"27\" ry=\"18.2703\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-20.5848\" font-family=\"Times,serif\" font-size=\"11.00\" fill=\"#000000\">Res&#45;</text>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-9.5848\" font-family=\"Times,serif\" font-size=\"11.00\" fill=\"#000000\">Net50</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x16d4da0a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "pipeline = ResNet50(num_epochs=1, batch_size=83)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_config = trained_opt.get_pipeline()\n",
    "best_config.visualize()\n",
    "best_config.pretty_print(ipython_display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, we have learned how to write an operator implementation class and hyperparameter\n",
    "schema; how to register the Lale operator; and how to use the Lale operator for\n",
    "manual as well as automated machine-learning.\n",
    "Lale can also generate automatic documentation for operators, the notebook [Schemas and Their Uses](./10_schemas.ipynb) describes how to enable it.\n",
    "The reference section of our [documentation for adding new operators](https://github.com/IBM/lale/blob/master/examples/docs_new_operators.ipynb) has more details about JSON Schemas and how to use those when creating schemas for Lale operators."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
