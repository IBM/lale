{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lale: Types and Auto-ML for Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an introductory guide to\n",
    "[Lale](https://github.com/ibm/lale) for scikit-learn users.\n",
    "[Scikit-learn](https://scikit-learn.org) is a popular, easy-to-use,\n",
    "and comprehensive data science library for Python. This notebook aims\n",
    "to show how Lale can make scikit-learn even better in two areas: type\n",
    "checking and auto-ML.  First, when you pass hyperparameters or\n",
    "datasets to scikit-learn, Lale checks that these are\n",
    "type-correct. Second, if you do not want to manually select all\n",
    "algorithms or tune all hyperparameters, you can leave it to Lale to do\n",
    "that for you automatically. For both type-checking and auto-ML, Lale\n",
    "uses a single source of truth: machine-readable schemas associated\n",
    "with scikit-learn compatible transformers and estimators. Rather than\n",
    "invent a new schema specification language, Lale uses [JSON\n",
    "Schema](https://json-schema.org/understanding-json-schema/), because\n",
    "it is popular, widely-supported, and makes it easy to store or send\n",
    "hyperparameters as JSON objects. Furthermore, by using the same\n",
    "schemas both for type-checking and for auto-ML, Lale ensures that\n",
    "type-checking is consistent with auto-ML while also making schemas\n",
    "easier to maintain.\n",
    "\n",
    "Lale is an open-source Python library and you can install it by doing\n",
    "`pip install lale`. See\n",
    "[installation](https://github.com/IBM/lale/blob/master/docs/installation.rst)\n",
    "for further instructions. Lale uses the term *operator* to refer to\n",
    "what scikit-learn calls machine-learning transformer or estimator.\n",
    "Lale provides schemas for 144\n",
    "[operators](https://github.com/IBM/lale/tree/master/lale/lib). Most of\n",
    "these operators come from scikit-learn itself, but there are also\n",
    "operators from other frameworks such as XGBoost, PyTorch, and even [R\n",
    "and Weka](https://github.com/IBM/lale-gpl/tree/master/lalegpl/lib).\n",
    "If Lale does not yet support your favorite operator, you can add it\n",
    "yourself by following this\n",
    "[guide](https://nbviewer.jupyter.org/github/IBM/lale/blob/master/examples/docs_new_operators.ipynb). If\n",
    "you do add a new operator, please consider contributing it back to\n",
    "Lale!\n",
    "\n",
    "The rest of this notebook first introduces schemas, then demonstrates\n",
    "how to use them for type-checking, and finally demonstrates how to use\n",
    "the same schemas for auto-ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Schemas from a Notebook\n",
    "\n",
    "When writing data science code, I sometimes don't remember all the API\n",
    "information about what hyperparameters and datasets an operator\n",
    "expects. Lale attaches this information to the operators, so we can\n",
    "look it up interactively in a notebook. To demonstrate, let's first\n",
    "import two scikit-learn operators, a decision tree estimator and an\n",
    "NMF transformer. The function `wrap_imported_operators` augments these\n",
    "two operators with schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lale.lib.sklearn import DecisionTreeRegressor as Tree\n",
    "from sklearn.decomposition import NMF\n",
    "import lale.helpers\n",
    "lale.helpers.wrap_imported_operators()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has excellent online documentation, and Lale stores a\n",
    "link to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n"
     ]
    }
   ],
   "source": [
    "print(Tree.documentation_url())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lale provides a helper function `ipython_display` that pretty-prints\n",
    "JSON documents and JSON schemas in a Jupyter notebook. You can get a\n",
    "quick overview of the constructor arguments of an operator by calling\n",
    "the `hyperparam_defaults` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "{\n",
       "    'criterion': 'mse',\n",
       "    'splitter': 'best',\n",
       "    'max_depth': null,\n",
       "    'min_samples_split': 2,\n",
       "    'min_samples_leaf': 1,\n",
       "    'min_weight_fraction_leaf': 0.0,\n",
       "    'max_features': null,\n",
       "    'random_state': null,\n",
       "    'max_leaf_nodes': null,\n",
       "    'min_impurity_decrease': 0.0,\n",
       "    'min_impurity_split': null,\n",
       "    'presort': false}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lale.pretty_print import ipython_display\n",
    "ipython_display(Tree.hyperparam_defaults())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters can be categorical (meaning they accept a few\n",
    "discrete values) or continuous (integers or real numbers).\n",
    "As an example for a categorical hyperparameter, let's look at the\n",
    "`criterion`. JSON Schema can encode categoricals as an `enum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "{\n",
       "    'description': 'Function to measure the quality of a split.',\n",
       "    'enum': ['mse', 'friedman_mse', 'mae'],\n",
       "    'default': 'mse'}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ipython_display(Tree.hyperparam_schema('criterion'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example for a continuous hyperparameter, let's look at\n",
    "`max_depth`. The decision tree regressor in scikit-learn accepts\n",
    "either an integer for that, or `None`, which has its own meaning.\n",
    "JSON Schema can express these two choices as an `anyOf`.  Also, while\n",
    "any positive integer is a valid value, in the context of auto-AI,\n",
    "Lale specifies a bounded range for the optimizer to search over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "{\n",
       "    'description': 'Maximum depth of the tree.',\n",
       "    'default': null,\n",
       "    'anyOf': [\n",
       "    {   'type': 'integer',\n",
       "        'minimum': 1,\n",
       "        'minimumForOptimizer': 3,\n",
       "        'maximumForOptimizer': 5},\n",
       "    {   'enum': [null],\n",
       "        'description': 'If None, then nodes are expanded until all leaves are pure, or until all leaves contain less than min_samples_split samples.'}]}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ipython_display(Tree.hyperparam_schema('max_depth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides hyperparameter schemas, Lale also provides dataset\n",
    "schemas. For exampe, NMF requires a non-negative matrix as `X`. In\n",
    "JSON Schema, we express this as an array of arrays of numbers with\n",
    "`minimum: 0`.  While NMF also accepts a second argument `y`, it does\n",
    "not use that argument. Therefore, Lale gives `y` the empty schema\n",
    "`{}`, which permits any values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "{\n",
       "    'type': 'object',\n",
       "    'required': ['X'],\n",
       "    'additionalProperties': false,\n",
       "    'properties': {\n",
       "        'X': {\n",
       "            'type': 'array',\n",
       "            'items': {\n",
       "                'type': 'array',\n",
       "                'items': {\n",
       "                    'type': 'number',\n",
       "                    'minimum': 0.0}}},\n",
       "        'y': {}}}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ipython_display(NMF.input_schema_fit())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Error Example in Scikit-Learn\n",
    "\n",
    "To set the stage for how Lale uses schemas for error checking, let's\n",
    "look at how scikit-learn does error-checking without schemas. First,\n",
    "we import a few things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "from sklearn import pipeline, feature_selection, ensemble, tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `make_pipeline` to compose a pipeline of two steps: an RFE\n",
    "transformer and a decision tree regressor. RFE performs recursive\n",
    "feature elimination, keeping only those features of the input data\n",
    "that are the most useful for its `estimator` argument. For RFE's\n",
    "estimator argument, the following code uses a random forest with 10\n",
    "trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_hyperparam_error = sklearn.pipeline.make_pipeline(\n",
    "    sklearn.feature_selection.RFE(\n",
    "        estimator=sklearn.ensemble.RandomForestRegressor(n_estimators=10)),\n",
    "    sklearn.tree.DecisionTreeRegressor(max_depth=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the `max_depth` argument for a decision tree cannot be a\n",
    "negative number. Hence, the above code actually contains a bug: it\n",
    "sets `max_depth=-1`. Scikit-learn does not check for this mistake from\n",
    "the `__init__` method, otherwise we would have seen an error message\n",
    "already. Instead, scikit-learn checks for this mistake during `fit`.\n",
    "In order to call `fit`, we must first load a dataset. We load the\n",
    "California Housing dataset and display the first few rows to get a\n",
    "feeling for the data. Lale can process both Pandas dataframes and\n",
    "Numpy ndarrays; here we use dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.2596</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5.017657</td>\n",
       "      <td>1.006421</td>\n",
       "      <td>2300.0</td>\n",
       "      <td>3.691814</td>\n",
       "      <td>32.71</td>\n",
       "      <td>-117.03</td>\n",
       "      <td>1.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.8125</td>\n",
       "      <td>49.0</td>\n",
       "      <td>4.473545</td>\n",
       "      <td>1.041005</td>\n",
       "      <td>1314.0</td>\n",
       "      <td>1.738095</td>\n",
       "      <td>33.77</td>\n",
       "      <td>-118.16</td>\n",
       "      <td>3.821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.1563</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.645833</td>\n",
       "      <td>0.985119</td>\n",
       "      <td>915.0</td>\n",
       "      <td>2.723214</td>\n",
       "      <td>34.66</td>\n",
       "      <td>-120.48</td>\n",
       "      <td>1.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.9425</td>\n",
       "      <td>36.0</td>\n",
       "      <td>4.002817</td>\n",
       "      <td>1.033803</td>\n",
       "      <td>1418.0</td>\n",
       "      <td>3.994366</td>\n",
       "      <td>32.69</td>\n",
       "      <td>-117.11</td>\n",
       "      <td>0.934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.5542</td>\n",
       "      <td>43.0</td>\n",
       "      <td>6.268421</td>\n",
       "      <td>1.134211</td>\n",
       "      <td>874.0</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>36.78</td>\n",
       "      <td>-119.80</td>\n",
       "      <td>0.965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  3.2596      33.0  5.017657   1.006421      2300.0  3.691814     32.71   \n",
       "1  3.8125      49.0  4.473545   1.041005      1314.0  1.738095     33.77   \n",
       "2  4.1563       4.0  5.645833   0.985119       915.0  2.723214     34.66   \n",
       "3  1.9425      36.0  4.002817   1.033803      1418.0  3.994366     32.69   \n",
       "4  3.5542      43.0  6.268421   1.134211       874.0  2.300000     36.78   \n",
       "\n",
       "   Longitude  target  \n",
       "0    -117.03   1.030  \n",
       "1    -118.16   3.821  \n",
       "2    -120.48   1.726  \n",
       "3    -117.11   0.934  \n",
       "4    -119.80   0.965  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lale.datasets\n",
    "(train_X, train_y), (test_X, test_y) = lale.datasets.california_housing_df()\n",
    "pd.concat([train_X.head(), train_y.head()], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have data, we can train our pipeline. As expected, since\n",
    "we deliberately had a mistake in the hyperparameters, training fails.\n",
    "However, it takes a few seconds to get the exception, because\n",
    "scikit-learn first trains the RFE transformer and uses it to transform\n",
    "the data. Only then does it pass the data to the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.17 s, sys: 78.1 ms, total: 4.25 s\n",
      "Wall time: 4.28 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_depth must be greater than zero. \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    sklearn_hyperparam_error.fit(train_X, train_y)\n",
    "except ValueError as e:\n",
    "    message = str(e)\n",
    "print(message, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert message.startswith(\"max_depth must be greater than zero.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, this error message is pretty clear. Scikit-learn\n",
    "implements the error check imperatively, using Python if-statements\n",
    "to raise an exception when hyperparameters are configured wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Hyperparameters with Types\n",
    "\n",
    "Lale performs the same error checks, but using JSON Schema validation\n",
    "instead of Python if-statements and raise-statements. First, we import\n",
    "a few things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lale.operators\n",
    "import jsonschema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the exact same pipeline as before, but written in Lale\n",
    "instead of directly in scikit-learn. In both cases, the underlying\n",
    "implementation is in scikit-learn; Lale only adds thin wrappers to\n",
    "support type checking and auto-ML. This example uses fully-qualified\n",
    "names (e.g., `lale.lib.sklearn.RFE`) to make it clear which version of\n",
    "the operator is being used. The fully-qualified names are for\n",
    "presentation purposes only; in typical usage of either scikit-learn or\n",
    "Lale, these would be simple names (e.g. just `RFE`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.2 ms, sys: 0 ns, total: 31.2 ms\n",
      "Wall time: 25 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid configuration for DecisionTreeRegressor(max_depth=-1) due to invalid value max_depth=-1.\n",
      "Schema of argument max_depth: {\n",
      "    'description': 'Maximum depth of the tree.',\n",
      "    'default': null,\n",
      "    'anyOf': [\n",
      "    {   'type': 'integer',\n",
      "        'minimum': 1,\n",
      "        'minimumForOptimizer': 3,\n",
      "        'maximumForOptimizer': 5},\n",
      "    {   'enum': [null],\n",
      "        'description': 'If None, then nodes are expanded until all leaves are pure, or until all leaves contain less than min_samples_split samples.'}]}\n",
      "Value: -1\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    lale_hyperparam_error = lale.operators.make_pipeline(\n",
    "        lale.lib.sklearn.RFE(\n",
    "            estimator=lale.lib.sklearn.RandomForestRegressor(n_estimators=10)),\n",
    "        lale.lib.sklearn.DecisionTreeRegressor(max_depth=-1))\n",
    "except jsonschema.ValidationError as e:\n",
    "    message = e.message\n",
    "print(message, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert message.startswith(\"Invalid configuration for DecisionTreeRegressor(max_depth=-1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the scikit-learn example, the error message in the Lale\n",
    "example also pin-points the problem as passing `max_depth=-1` to the\n",
    "decision tree. It does so in a more stylized way, printing the\n",
    "relevant JSON schema for this hyperparameter.  Lale detects the error\n",
    "already when the wrong hyperparameter is being passed as an argument,\n",
    "thus reducing the amount of code you have to look at to find the root\n",
    "cause.  Furthermore, Lale takes only tens of milliseconds to detect\n",
    "the error, because it does not attempt to train the RFE transformer\n",
    "first. In this example, that only saves a few seconds, which may not\n",
    "be significant. But there are situations with larger time savings,\n",
    "such as when using larger datasets, slower operators, or when auto-ML\n",
    "tries out many pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Error Example in Scikit-Learn\n",
    "\n",
    "Above, we saw an example for detecting a hyperparameter error in\n",
    "scikit-learn and in Lale. Next, we look at an analogous example for a\n",
    "dataset error. Again, let's first look at the experience with\n",
    "scikit-learn and then the same thing with Lale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use scikit-learn to compose a pipeline of two steps: an RFE\n",
    "transformer as before, this time followed by an NMF transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_dataset_error = sklearn.pipeline.make_pipeline(\n",
    "    sklearn.feature_selection.RFE(\n",
    "        estimator=sklearn.ensemble.RandomForestRegressor(n_estimators=10)),\n",
    "    sklearn.decomposition.NMF())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF, or non-negative matrix factorization, does not allow any negative\n",
    "numbers in its input matrix. The California Housing dataset contains\n",
    "some negative numbers and the RFE does not eliminate those features.\n",
    "To detect the mistake, scikit-learn must first train the RFE and\n",
    "transform the data with it, which takes a few seconds. Then, NMF\n",
    "detects the error and throws an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.31 s, sys: 109 ms, total: 4.42 s\n",
      "Wall time: 4.58 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Negative values in data passed to NMF (input X)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    sklearn_dataset_error.fit(train_X, train_y)\n",
    "except ValueError as e:\n",
    "    message = str(e)\n",
    "print(message, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert message.startswith(\"Negative values in data passed to NMF (input X)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types for Dataset Checking\n",
    "\n",
    "Lale uses types (as expressed using JSON schemas) to check dataset\n",
    "related mistakes. Below is the same pipeline as before, using thin\n",
    "Lale wrappers around scikit-learn operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lale_dataset_error = lale.operators.make_pipeline(\n",
    "    lale.lib.sklearn.RFE(\n",
    "        estimator=lale.lib.sklearn.RandomForestRegressor(n_estimators=10)),\n",
    "    lale.lib.sklearn.NMF())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of calling `fit` on the pipeline, we call `validate_schema`.\n",
    "This does not perform training, but rather, just checks that the\n",
    "schema is correct at each step of the pipeline. In other words, it\n",
    "checks whether the schema of the input data is valid for the first\n",
    "step of the pipeline, and that the schema of the output from each step\n",
    "is valid for the next step. By saving the time for training the RFE,\n",
    "this completes in tens of milliseconds instead of seconds as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 78.1 ms, sys: 15.6 ms, total: 93.8 ms\n",
      "Wall time: 92.4 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected to_schema(data) to be a subschema of NMF.input_schema_fit().\n",
      "to_schema(data) = {\n",
      "    'type': 'object',\n",
      "    'additionalProperties': false,\n",
      "    'required': ['X', 'y'],\n",
      "    'properties': {\n",
      "        'X': {\n",
      "            'type': 'array',\n",
      "            'items': {\n",
      "                'type': 'array',\n",
      "                'items': {\n",
      "                    'type': 'number'}}},\n",
      "        'y': {\n",
      "            'type': 'array',\n",
      "            'minItems': 16512,\n",
      "            'maxItems': 16512,\n",
      "            'items': {\n",
      "                'description': 'target',\n",
      "                'type': 'number'}}}}\n",
      "NMF.input_schema_fit() = {\n",
      "    'type': 'object',\n",
      "    'required': ['X'],\n",
      "    'additionalProperties': false,\n",
      "    'properties': {\n",
      "        'X': {\n",
      "            'type': 'array',\n",
      "            'items': {\n",
      "                'type': 'array',\n",
      "                'items': {\n",
      "                    'type': 'number',\n",
      "                    'minimum': 0.0}}},\n",
      "        'y': {}}}\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    lale_dataset_error.validate_schema(train_X, train_y)\n",
    "except lale.helpers.SubschemaError as e:\n",
    "    message = str(e)\n",
    "print(message, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert message.startswith('Expected to_schema(data) to be a subschema of NMF.input_schema_fit().')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the schemas for `X` differ: whereas the data is an\n",
    "array of arrays of unconstrained numbers, NMF expects an array of\n",
    "arrays of only non-negative numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types for Hyperparameter Tuning\n",
    "\n",
    "We have already seen that Lale uses schemas for interactive\n",
    "documentation and for error-checking. Now we will see how Lale uses\n",
    "those same schemas for auto-ML. Specifically, we will start by looking\n",
    "at hyperparameter tuning, which is an important sub-task of auto-ML.\n",
    "First, let's import PCA, another transformer from scikit-learn, and\n",
    "HyperoptRegressor, a Lale interface to the\n",
    "[hyperopt](http://hyperopt.github.io/hyperopt/) auto-ML library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from lale.lib.lale import HyperoptRegressor\n",
    "lale.helpers.wrap_imported_operators()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a two-step pipeline of PCA and a decision tree\n",
    "regressor. Note that we are not specifying any hyperparameters for\n",
    "these two operators. The code just says `PCA, Tree` instead of\n",
    "`PCA(...), Tree(...)`. This is on purpose: rather than binding\n",
    "hyperparameters by hand, we leave them free to be tuned by hyperopt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_tree_planned = lale.operators.make_pipeline(PCA, Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we create an instance of `HyperoptRegressor`, we pass in the\n",
    "pipeline as its `estimator` argument. Then, we call `fit` to search\n",
    "for a good pipeline. In this case, the search uses 10 trials. Each\n",
    "trial draws values for the hyperparameters from the ranges specified\n",
    "by the JSON schemas associated with the operators in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  2.12it/s, best loss: -0.4696652802026405]\n"
     ]
    }
   ],
   "source": [
    "pca_tree_hyperopt = HyperoptRegressor(estimator=pca_tree_planned, cv=3, max_evals=10)\n",
    "pca_tree_trained = pca_tree_hyperopt.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end result is the pipeline that performed best in cross-validation\n",
    "out of the 10 trials. In addition to the cross-val score, we can also\n",
    "evaluate this best pipeline against the test data. We simply reuse the\n",
    "existing R2 score metric from scikit-learn for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score 0.47\n"
     ]
    }
   ],
   "source": [
    "predicted = pca_tree_trained.predict(test_X)\n",
    "print(f'R2 score {sklearn.metrics.r2_score(test_y, predicted):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Results of Automation\n",
    "\n",
    "In the previous example, the automation picked hyperparameter values\n",
    "for PCA and the decision tree. We know the values were valid and we\n",
    "know how well the pipeline performed with them. But we might also want\n",
    "to know exactly which values were picked. One way to do that is by a\n",
    "tooltip in the visualization. If you are looking at this notebook in a\n",
    "viewer that supports tooltips, you can hover the mouse pointer over\n",
    "either one of the operators to see its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"152pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 152.00 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-40 148,-40 148,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<g id=\"a_node1\"><a xlink:href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\" xlink:title=\"PCA(whiten=True)\">\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-15.2\" font-family=\"Times,serif\" font-size=\"11.00\">PCA</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\n",
       "<g id=\"a_node2\"><a xlink:href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\" xlink:title=\"Tree(max_depth=3, max_features=&#39;log2&#39;, min_samples_leaf=13, min_samples_split=10)\">\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"117\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"117\" y=\"-15.2\" font-family=\"Times,serif\" font-size=\"11.00\">Tree</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.4029,-18C62.3932,-18 71.3106,-18 79.8241,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"79.919,-21.5001 89.919,-18 79.919,-14.5001 79.919,-21.5001\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f71942bceb8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lale.helpers.to_graphviz(pca_tree_trained._impl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to view the results of hyperparameter tuning in Lale is by\n",
    "pretty-printing the pipeline as Python source code. The code contains\n",
    "the hyperparameters. It also uses the `>>` symbol, which is just\n",
    "syntactic sugar for calling the `make_pipeline` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "pca = PCA(whiten=True)\n",
       "tree = Tree(max_depth=3, max_features='log2', min_samples_leaf=13, min_samples_split=10)\n",
       "pipeline = pca >> tree\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ipython_display(pca_tree_trained._impl, show_imports=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Lale with GridSearchCV\n",
    "\n",
    "Lale supports multiple auto-ML tools, not just hyperopt. For instance,\n",
    "you can also use\n",
    "[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "from scikit-learn. Just like with hyperopt, Lale uses your planned\n",
    "pipeline along with the schemas for its operators to generate a search\n",
    "space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lale.search.GridSearchCV import LaleGridSearchCV\n",
    "grid_search_grid = LaleGridSearchCV(\n",
    "    pca_tree_planned, lale_num_samples=1, lale_num_grids=1, cv=2,\n",
    "    scoring=sklearn.metrics.make_scorer(sklearn.metrics.r2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the search, you call `fit` and provide the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.52 s, sys: 3.53 s, total: 12 s\n",
      "Wall time: 5.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_search_best = grid_search_grid.fit(train_X, train_y).best_estimator_.to_lale()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect the results, for instance, by pretty-printing the best\n",
    "pipeline found by grid search back as Python source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "pca = PCA(n_components=5e-324, svd_solver='full', whiten=True)\n",
       "tree = Tree(max_depth=3, max_features=5e-324, min_samples_split=0.01, splitter='random')\n",
       "pipeline = pca >> tree\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ipython_display(grid_search_best, show_imports=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, you can use the best pipeline found for scoring and evaluate the\n",
    "quality of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score -0.00\n"
     ]
    }
   ],
   "source": [
    "predicted = grid_search_best.predict(test_X)\n",
    "print(f'R2 score {sklearn.metrics.r2_score(test_y, predicted):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Combinators\n",
    "\n",
    "We already saw that `>>` is syntactic sugar for `make_pipeline`.  Lale\n",
    "refers to `>>` as the *pipe* combinator. Besides `>>`, Lale supports\n",
    "two additional combinators. Before we introduce them, we will import a\n",
    "few more things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lale.lib.lale import NoOp\n",
    "from lale.lib.lale import ConcatFeatures as HStack\n",
    "from sklearn.linear_model import LinearRegression as LinReg\n",
    "from xgboost import XGBRegressor as XGBoost\n",
    "lale.helpers.wrap_imported_operators()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's compose a pipeline using scikit-learn style functions.\n",
    "We already saw `make_pipeline`. Function `make_union` also exists in\n",
    "scikit-learn. It composes multiple sub-pipelines to run on the same\n",
    "data, then concatenates the features. In other words, `make_union`\n",
    "produces a horizontal stack of the data transformed by its\n",
    "sub-pipelines. Function `make_choice` does not exist in scikit-learn.\n",
    "It specifies an algorithmic choice, which auto-ML can resolve. In\n",
    "other words, `make_choice` creates a search space for automated\n",
    "algorithm selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"323pt\" height=\"102pt\"\n",
       " viewBox=\"0.00 0.00 323.07 101.80\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 97.799)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-97.799 319.074,-97.799 319.074,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<g id=\"a_node1\"><a xlink:href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\" xlink:title=\"PCA\">\n",
       "<ellipse fill=\"#7ec0ee\" stroke=\"black\" cx=\"27\" cy=\"-75.799\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-72.999\" font-family=\"Times,serif\" font-size=\"11.00\">PCA</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\n",
       "<g id=\"a_node3\"><a xlink:href=\"https://lale.readthedocs.io/en/latest/modules/lale.lib.lale.concat_features.html\" xlink:title=\"HStack\">\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"117\" cy=\"-47.799\" rx=\"27\" ry=\"19.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"117\" y=\"-50.999\" font-family=\"Times,serif\" font-size=\"11.00\">H&#45;</text>\n",
       "<text text-anchor=\"middle\" x=\"117\" y=\"-38.999\" font-family=\"Times,serif\" font-size=\"11.00\">Stack</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M51.5824,-68.2955C61.0734,-65.2756 72.193,-61.7376 82.4969,-58.4591\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"83.5782,-61.788 92.0463,-55.4206 81.4558,-55.1175 83.5782,-61.788\"/>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\n",
       "<g id=\"a_node2\"><a xlink:href=\"https://lale.readthedocs.io/en/latest/modules/lale.lib.lale.no_op.html\" xlink:title=\"NoOp\">\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"27\" cy=\"-19.799\" rx=\"27\" ry=\"19.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-22.999\" font-family=\"Times,serif\" font-size=\"11.00\">No&#45;</text>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-10.999\" font-family=\"Times,serif\" font-size=\"11.00\">Op</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.0464,-27.4501C61.3591,-30.4132 72.1847,-33.8578 82.2522,-37.061\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"81.3922,-40.4603 91.9827,-40.1571 83.5147,-33.7898 81.3922,-40.4603\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\n",
       "<g id=\"a_node4\"><a xlink:title=\"Tree | LinReg | XGBoost\">\n",
       "<ellipse fill=\"#7ec0ee\" stroke=\"black\" cx=\"247.537\" cy=\"-47.799\" rx=\"67.5738\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"247.537\" y=\"-44.999\" font-family=\"Times,serif\" font-size=\"11.00\">Tree | LinReg | XGBoost</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M144.011,-47.799C151.715,-47.799 160.572,-47.799 169.784,-47.799\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"169.883,-51.2991 179.883,-47.799 169.883,-44.2991 169.883,-51.2991\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f710ddca240>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dag_with_functions = lale.operators.make_pipeline(\n",
    "    lale.operators.make_union(PCA, NoOp),\n",
    "    lale.operators.make_choice(Tree, LinReg, XGBoost))\n",
    "lale.helpers.to_graphviz(dag_with_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization shows `make_union` as multiple sub-pipelines feeding\n",
    "into `HStack`, and it shows `make_choice` using an `|` combinator. In\n",
    "total, Lale has three combinators. We already saw `>>` (pipe) and `|`\n",
    "(or). The third combinator is `&` (and). The next example shows the\n",
    "exact same pipeline as before, but written using combinators instead\n",
    "of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"323pt\" height=\"102pt\"\n",
       " viewBox=\"0.00 0.00 323.07 101.80\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 97.799)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-97.799 319.074,-97.799 319.074,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<g id=\"a_node1\"><a xlink:href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\" xlink:title=\"PCA\">\n",
       "<ellipse fill=\"#7ec0ee\" stroke=\"black\" cx=\"27\" cy=\"-75.799\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-72.999\" font-family=\"Times,serif\" font-size=\"11.00\">PCA</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\n",
       "<g id=\"a_node3\"><a xlink:href=\"https://lale.readthedocs.io/en/latest/modules/lale.lib.lale.concat_features.html\" xlink:title=\"HStack\">\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"117\" cy=\"-47.799\" rx=\"27\" ry=\"19.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"117\" y=\"-50.999\" font-family=\"Times,serif\" font-size=\"11.00\">H&#45;</text>\n",
       "<text text-anchor=\"middle\" x=\"117\" y=\"-38.999\" font-family=\"Times,serif\" font-size=\"11.00\">Stack</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M51.5824,-68.2955C61.0734,-65.2756 72.193,-61.7376 82.4969,-58.4591\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"83.5782,-61.788 92.0463,-55.4206 81.4558,-55.1175 83.5782,-61.788\"/>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\n",
       "<g id=\"a_node2\"><a xlink:href=\"https://lale.readthedocs.io/en/latest/modules/lale.lib.lale.no_op.html\" xlink:title=\"NoOp\">\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"27\" cy=\"-19.799\" rx=\"27\" ry=\"19.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-22.999\" font-family=\"Times,serif\" font-size=\"11.00\">No&#45;</text>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-10.999\" font-family=\"Times,serif\" font-size=\"11.00\">Op</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.0464,-27.4501C61.3591,-30.4132 72.1847,-33.8578 82.2522,-37.061\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"81.3922,-40.4603 91.9827,-40.1571 83.5147,-33.7898 81.3922,-40.4603\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\n",
       "<g id=\"a_node4\"><a xlink:title=\"Tree | LinReg | XGBoost\">\n",
       "<ellipse fill=\"#7ec0ee\" stroke=\"black\" cx=\"247.537\" cy=\"-47.799\" rx=\"67.5738\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"247.537\" y=\"-44.999\" font-family=\"Times,serif\" font-size=\"11.00\">Tree | LinReg | XGBoost</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M144.011,-47.799C151.715,-47.799 160.572,-47.799 169.784,-47.799\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"169.883,-51.2991 179.883,-47.799 169.883,-44.2991 169.883,-51.2991\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f710f5e54e0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dag_with_combinators = (PCA & NoOp) >> HStack >> (Tree | LinReg | XGBoost)\n",
    "lale.helpers.to_graphviz(dag_with_combinators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Algorithm Selection and Hyperparameter Optimization\n",
    "\n",
    "Since the `dag_with_functions` specifies an algorithm choice, when we\n",
    "feed it to a `HyperoptRegressor`, Hyperopt will do algorithm selection\n",
    "for us. And since some of the operators in the dag do not have all\n",
    "their hyperparameters bound, hyperopt will also tune their free\n",
    "hyperparameters for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:37<00:00,  8.46s/it, best loss: -0.7778059793292099]\n"
     ]
    }
   ],
   "source": [
    "multi_alg_hyperopt = HyperoptRegressor(estimator=dag_with_functions, cv=3, max_evals=10)\n",
    "multi_alg_trained = multi_alg_hyperopt.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the resulting trained pipeline to see what algorithms\n",
    "hyperopt chose. Also, recall that you can hover the mouse pointer over\n",
    "operators in the visualization to see a tooltip with their\n",
    "hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"242pt\" height=\"102pt\"\n",
       " viewBox=\"0.00 0.00 242.00 101.80\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 97.799)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-97.799 238,-97.799 238,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<g id=\"a_node1\"><a xlink:href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\" xlink:title=\"PCA(n_components=&#39;mle&#39;, svd_solver=&#39;full&#39;)\">\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"27\" cy=\"-75.799\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-72.999\" font-family=\"Times,serif\" font-size=\"11.00\">PCA</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\n",
       "<g id=\"a_node3\"><a xlink:href=\"https://lale.readthedocs.io/en/latest/modules/lale.lib.lale.concat_features.html\" xlink:title=\"HStack\">\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"117\" cy=\"-47.799\" rx=\"27\" ry=\"19.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"117\" y=\"-50.999\" font-family=\"Times,serif\" font-size=\"11.00\">H&#45;</text>\n",
       "<text text-anchor=\"middle\" x=\"117\" y=\"-38.999\" font-family=\"Times,serif\" font-size=\"11.00\">Stack</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M51.5824,-68.2955C61.0734,-65.2756 72.193,-61.7376 82.4969,-58.4591\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"83.5782,-61.788 92.0463,-55.4206 81.4558,-55.1175 83.5782,-61.788\"/>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\n",
       "<g id=\"a_node2\"><a xlink:href=\"https://lale.readthedocs.io/en/latest/modules/lale.lib.lale.no_op.html\" xlink:title=\"NoOp\">\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"27\" cy=\"-19.799\" rx=\"27\" ry=\"19.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-22.999\" font-family=\"Times,serif\" font-size=\"11.00\">No&#45;</text>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-10.999\" font-family=\"Times,serif\" font-size=\"11.00\">Op</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.0464,-27.4501C61.3591,-30.4132 72.1847,-33.8578 82.2522,-37.061\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"81.3922,-40.4603 91.9827,-40.1571 83.5147,-33.7898 81.3922,-40.4603\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\n",
       "<g id=\"a_node4\"><a xlink:title=\"XGBoost(colsample_bylevel=0.8125953277265251, colsample_bytree=0.7432005864418613, learning_rate=0.336632702036662, max_depth=4, min_child_weight=8, n_estimators=1012, reg_alpha=0.02223895190558911, reg_lambda=0.02984096814184546, subsample=0.498939586...\">\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"207\" cy=\"-47.799\" rx=\"27\" ry=\"19.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"207\" y=\"-50.999\" font-family=\"Times,serif\" font-size=\"11.00\">XG&#45;</text>\n",
       "<text text-anchor=\"middle\" x=\"207\" y=\"-38.999\" font-family=\"Times,serif\" font-size=\"11.00\">Boost</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M144.403,-47.799C152.393,-47.799 161.311,-47.799 169.824,-47.799\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"169.919,-51.2991 179.919,-47.799 169.919,-44.2991 169.919,-51.2991\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f710f618da0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lale.helpers.to_graphviz(multi_alg_trained._impl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the trained pipeline can be used for predictions as usual,\n",
    "and we can use scikit-learn metrics to evaluate those predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score 0.79\n"
     ]
    }
   ],
   "source": [
    "predicted = multi_alg_trained.predict(test_X)\n",
    "print(f'R2 score {sklearn.metrics.r2_score(test_y, predicted):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Constraint Example in Scikit-Learn\n",
    "\n",
    "Sometimes, the validity of hyperparameters cannot be checked in\n",
    "isolation. Instead, the value of one hyperparameter can restrict\n",
    "which values are valid for another hyperparameter. For example,\n",
    "scikit-learn imposes a conditional hyperparameter constraint between\n",
    "the `svd_solver` and `n_components` arguments to PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_constraint_error = sklearn.pipeline.make_pipeline(\n",
    "    sklearn.feature_selection.RFE(\n",
    "        estimator=sklearn.ensemble.RandomForestRegressor(n_estimators=10)),\n",
    "    sklearn.decomposition.PCA(svd_solver='arpack', n_components='mle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above notebook cell completed successfully, because scikit-learn\n",
    "did not yet check for the constraint. To observe the error message\n",
    "with scikit-learn, we must attempt to fit the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.25 s, sys: 125 ms, total: 4.38 s\n",
      "Wall time: 4.46 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n_components='mle' cannot be a string with svd_solver='arpack'\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "message=None\n",
    "try:\n",
    "    sklearn_constraint_error.fit(train_X, train_y)\n",
    "except ValueError as e:\n",
    "    message = str(e)\n",
    "print(message, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert message.startswith(\"n_components='mle' cannot be a string with svd_solver='arpack'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn implements constraint-checking as Python code with\n",
    "if-statements and raise-statements. After a few seconds, we get an\n",
    "exception, and the error message explains what went wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types for Constraint Checking\n",
    "\n",
    "Lale specifies constraints using JSON Schemas. When you configure an\n",
    "operator with actual hyperparameters, Lale immediately validates them\n",
    "against their schema including constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.9 ms, sys: 0 ns, total: 46.9 ms\n",
      "Wall time: 44.8 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid configuration for PCA(svd_solver='arpack', n_components='mle') due to constraint option n_components mle can only be set for svd_solver full or auto.\n",
      "Schema of constraint 1: {\n",
      "    'description': 'Option n_components mle can only be set for svd_solver full or auto.',\n",
      "    'anyOf': [\n",
      "    {   'type': 'object',\n",
      "        'properties': {\n",
      "            'n_components': {\n",
      "                'not': {\n",
      "                    'enum': ['mle']}}}},\n",
      "    {   'type': 'object',\n",
      "        'properties': {\n",
      "            'svd_solver': {\n",
      "                'enum': ['full', 'auto']}}}]}\n",
      "Value: {'svd_solver': 'arpack', 'n_components': 'mle', 'copy': True, 'whiten': False, 'tol': 0.0, 'iterated_power': 'auto', 'random_state': None}\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    lale_constraint_error = lale.operators.make_pipeline(\n",
    "        lale.lib.sklearn.RFE(\n",
    "            estimator=lale.lib.sklearn.RandomForestRegressor(n_estimators=10)),\n",
    "        PCA(svd_solver='arpack', n_components='mle'))\n",
    "except jsonschema.ValidationError as e:\n",
    "    message = str(e)\n",
    "print(message, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert message.startswith(\"Invalid configuration for PCA(svd_solver='arpack', n_components='mle')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lale reports the error quicker than scikit-learn, taking only tens of\n",
    "milliseconds instead of multiple seconds. The error message contains\n",
    "both a natural-language description of the constraint and its formal\n",
    "representation in JSON Schema. The `'anyOf'` implements an 'or', so\n",
    "you can read the constraints as\n",
    "\n",
    "```python\n",
    "(not (n_components in ['mle'])) or (svd_solver in ['full', 'auto'])\n",
    "```\n",
    "\n",
    "By basic Boolean algebra, this is equivalent to an implication\n",
    "\n",
    "```python\n",
    "(n_components in ['mle']) implies (svd_solver in ['full', 'auto'])\n",
    "```\n",
    "\n",
    "Since the constraint is specified declaratively in the schema, it gets\n",
    "applied wherever the schema gets used. Specifically, the constraint\n",
    "gets used both for type-checking and for auto-ML. In the context of\n",
    "auto-ML, the constraint prunes the search space: it eliminates some\n",
    "hyperparameter combinations so that the auto-ML tool does not have to\n",
    "try them out. We have observed cases where this pruning makes a big\n",
    "difference in search convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAQ\n",
    "\n",
    "- Q: Can I implement other tasks besides regression?\n",
    "\n",
    "  A: Yes, for instance, you can use HyperoptClassifier for\n",
    "     classification.\n",
    "\n",
    "- Q: How about other modalities besides tables?\n",
    "\n",
    "  A: Besides table, we have successfully used Lale for text, images,\n",
    "     and time-series. In fact, Lale even works for multi-modal data,\n",
    "     using the `&` combinator to specify different preprocessing paths\n",
    "     per modality.\n",
    "\n",
    "- Q: I get an error when I instantiate an operator imported from Lale.\n",
    "\n",
    "  A: Lale raises errors on invalid hyperparameter values or\n",
    "     combinations. This ensures that the operators are used correctly.\n",
    "     So don't be surprised if you get any errors when you initialize\n",
    "     Lale operators with some hyperparameter values. Chances are that\n",
    "     those hyperpameters or combinations of hyperparameters are\n",
    "     invalid. If not, please contact us.\n",
    "\n",
    "- Q: The algorithm I want to use is not present in Lale. Can I still use it?\n",
    "\n",
    "  A: Some of the features of Lale can be used if the algorithm\n",
    "     implementation follows a scikit-learn API of fit/predict or\n",
    "     fit/transform. You can cast the operator into a Lale operator as\n",
    "     follows:\n",
    "\n",
    "     ```\n",
    "     from lale.operators import make_operator\n",
    "     lale_op = make_operator(non_lale_op)\n",
    "     ```\n",
    "\n",
    "- Q: Can I tweak the schema of an operator?\n",
    "\n",
    "  A: Yes, by calling the `customize_schema` method. There is a\n",
    "     separate\n",
    "     [guide](https://github.com/IBM/lale/blob/master/examples/docs_new_operators.ipynb)\n",
    "     for how to do that.\n",
    "\n",
    "- Q: How does the search space generation work?\n",
    "\n",
    "  A: Lale includes a search space generator that takes in a planned\n",
    "     pipeline and the schemas of the operators in that pipeline, and\n",
    "     returns a search space for your auto-ML tool of choice. Our\n",
    "     [arXiv paper](https://arxiv.org/pdf/1906.03957.pdf) describes how\n",
    "     that works in detail.\n",
    "\n",
    "- Q: Does Lale optimize for computational performance?\n",
    "\n",
    "  A: While Lale focuses mostly on types and automation, we have also\n",
    "     done a little bit of work on computational performance. However,\n",
    "     it has not been a major focus. If you encounter major\n",
    "     pain-points, please reach out to us.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
