{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lale.grammar import Grammar, explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple: First example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-08c4768e41e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprim_est\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/lale/grammar.py\u001b[0m in \u001b[0;36mexplore\u001b[0;34m(g, n)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'start'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Rule start must be defined\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munroll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mNoOp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'make_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "from lale.lib.sklearn import LogisticRegression, KNeighborsClassifier, PCA, StandardScaler\n",
    "\n",
    "g = Grammar()\n",
    "\n",
    "g.start       = g.estimator\n",
    "g.estimator   = g.prim_est | g.transformer >> g.prim_est\n",
    "g.transformer = g.prim_tfm | g.prim_tfm >> g.transformer\n",
    "\n",
    "g.prim_tfm    = PCA | StandardScaler\n",
    "g.prim_est = LogisticRegression | KNeighborsClassifier\n",
    "\n",
    "generated = explore(g, 4)\n",
    "generated.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lale.lib.lale import HyperoptCV\n",
    "import lale.datasets\n",
    "(train_X, train_y), (test_X, test_y) = lale.datasets.load_iris_df()\n",
    "\n",
    "trainer = HyperoptCV(estimator=generated, cv=2, max_evals=3, scoring='r2')\n",
    "trained = trainer.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lale.helpers import best_estimator, to_graphviz\n",
    "best_estimator(trained).to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar that exercices all combinators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lale.lib.sklearn import LogisticRegression, KNeighborsClassifier, PCA, StandardScaler\n",
    "from lale.lib.autogen import AdaBoostClassifier\n",
    "from lale.lib.lale import ConcatFeatures\n",
    "\n",
    "g = Grammar()\n",
    "\n",
    "g.start       = g.estimator\n",
    "g.estimator   = g.term_est | g.transformer >> g.term_est\n",
    "g.term_est    = g.prim_est # | g.ensemble      # Todo add higher-order operators\n",
    "# g.ensemble    = g.ensembler ( g.estimator )\n",
    "g.transformer = g.union_tfm | g.union_tfm >> g.transformer\n",
    "g.union_tfm   = g.prim_tfm | g.union_body >> ConcatFeatures\n",
    "g.union_body  = g.transformer | g.transformer & g.union_body\n",
    "\n",
    "g.prim_tfm    = PCA | StandardScaler\n",
    "g.prim_est    = LogisticRegression | KNeighborsClassifier\n",
    "g.ensembler   = AdaBoostClassifier\n",
    "\n",
    "generated = explore(g, 5)\n",
    "generated.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = HyperoptCV(estimator=generated, cv=2, max_evals=3, scoring='r2')\n",
    "trained = trainer.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator(trained).to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recipe: grammar from this [paper](https://link.springer.com/chapter/10.1007/978-3-319-55696-3_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lale.lib.sklearn import SimpleImputer, PCA, FeatureAgglomeration, PolynomialFeatures, RandomForestClassifier, DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from lale.lib.autogen import  GaussianNB, MultinomialNB, BernoulliNB\n",
    "import lale.helpers\n",
    "\n",
    "lale.helpers.wrap_imported_operators()\n",
    "\n",
    "\n",
    "g = Grammar()\n",
    "\n",
    "g.start = g.algorithm | g.preprocessing >> g.algorithm\n",
    "g.preprocessing = g.imputation >> g.dimensionality_definition | g.dimensionality_definition\n",
    "g.dimensionality_definition = g.feature_selection >> g.feature_construction | g.feature_selection | g.feature_construction\n",
    "g.feature_selection = g.unsupervised # | g.supervised \n",
    "g.algorithm = g.naive_bayes | g.trees\n",
    "\n",
    "g.imputation = SimpleImputer\n",
    "# g.supervised = SelectKBest\n",
    "g.unsupervised = PCA | FeatureAgglomeration\n",
    "g.feature_construction = PolynomialFeatures\n",
    "g.naive_bayes = GaussianNB | MultinomialNB | BernoulliNB\n",
    "g.trees = g.DecisionTree | g.RandomForest\n",
    "\n",
    "\n",
    "generated = explore(g, 4)\n",
    "generated.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = HyperoptCV(estimator=generated, cv=2, max_evals=3, scoring='r2')\n",
    "trained = trainer.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_estimator(trained).to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# alphad3m: Grammar from this [paper](https://www.automl.org/wp-content/uploads/2019/06/automlws2019_Paper34.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lale.lib.sklearn import OneHotEncoder, SimpleImputer, Ridge, LinearSVC, PCA, GaussianNB\n",
    "from lale.lib.autogen import OrdinalEncoder, SGDClassifier\n",
    "\n",
    "g = Grammar()\n",
    "\n",
    "g.start  = g.est | g.clean >> g.est | g.tfm >> g.est | g.clean >> g.tfm >> g.est\n",
    "g.clean  = g.clean1 >> g.clean | g.clean1\n",
    "g.tfm    = g.tfm1 >> g.tfm | g.tfm1\n",
    "\n",
    "g.clean1 = SimpleImputer #SkImputer | MissingIndicator\n",
    "g.tfm1   = OneHotEncoder  | PCA # | OrdinalEncoder\n",
    "g.est    = GaussianNB | Ridge  | LinearSVC # | SGDClassifier\n",
    "\n",
    "generated = explore(g, 4)\n",
    "generated.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = HyperoptCV(estimator=generated, cv=2, max_evals=3, scoring='r2')\n",
    "trained = trainer.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator(trained).to_json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
