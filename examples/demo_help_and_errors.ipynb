{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset error example in middle of pipeline\n",
    "\n",
    "Start by loading the California Housing dataset, which is a\n",
    "two-dimensional array of numbers.  One of the columns, longitude,\n",
    "contains negative numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.2596</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5.017657</td>\n",
       "      <td>1.006421</td>\n",
       "      <td>2300.0</td>\n",
       "      <td>3.691814</td>\n",
       "      <td>32.71</td>\n",
       "      <td>-117.03</td>\n",
       "      <td>1.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.8125</td>\n",
       "      <td>49.0</td>\n",
       "      <td>4.473545</td>\n",
       "      <td>1.041005</td>\n",
       "      <td>1314.0</td>\n",
       "      <td>1.738095</td>\n",
       "      <td>33.77</td>\n",
       "      <td>-118.16</td>\n",
       "      <td>3.821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.1563</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.645833</td>\n",
       "      <td>0.985119</td>\n",
       "      <td>915.0</td>\n",
       "      <td>2.723214</td>\n",
       "      <td>34.66</td>\n",
       "      <td>-120.48</td>\n",
       "      <td>1.726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  3.2596      33.0  5.017657   1.006421      2300.0  3.691814     32.71   \n",
       "1  3.8125      49.0  4.473545   1.041005      1314.0  1.738095     33.77   \n",
       "2  4.1563       4.0  5.645833   0.985119       915.0  2.723214     34.66   \n",
       "\n",
       "   Longitude  target  \n",
       "0    -117.03   1.030  \n",
       "1    -118.16   3.821  \n",
       "2    -120.48   1.726  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lale.datasets as ds\n",
    "(train_X, train_y), (test_X, test_y) = ds.california_housing_df()\n",
    "schema_X = ds.data_schemas.to_schema(train_X)\n",
    "schema_y = ds.data_schemas.to_schema(train_y)\n",
    "pd.concat([train_X.head(3), train_y.head(3)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn version\n",
    "\n",
    "Train an RFE (recursive feature elimination). This internally trains\n",
    "its argument, a random forest of 10 trees. Training the forest takes a\n",
    "few seconds. If we used more trees, training the forest would take\n",
    "longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor as Forest\n",
    "trainable_rfe = RFE(estimator=Forest(n_estimators=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.52 s, sys: 188 ms, total: 5.7 s\n",
      "Wall time: 5.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trained_rfe = trainable_rfe.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting features still include the longitude with its\n",
    "negative numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.2596</td>\n",
       "      <td>3.691814</td>\n",
       "      <td>32.71</td>\n",
       "      <td>-117.03</td>\n",
       "      <td>1.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.8125</td>\n",
       "      <td>1.738095</td>\n",
       "      <td>33.77</td>\n",
       "      <td>-118.16</td>\n",
       "      <td>3.821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.1563</td>\n",
       "      <td>2.723214</td>\n",
       "      <td>34.66</td>\n",
       "      <td>-120.48</td>\n",
       "      <td>1.726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  AveOccup  Latitude  Longitude  target\n",
       "0  3.2596  3.691814     32.71    -117.03   1.030\n",
       "1  3.8125  1.738095     33.77    -118.16   3.821\n",
       "2  4.1563  2.723214     34.66    -120.48   1.726"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support = trained_rfe.get_support()\n",
    "columns = [col for i, col in enumerate(train_X.columns) if support[i]]\n",
    "train_X2 = pd.DataFrame(data=trained_rfe.transform(train_X), columns=columns)\n",
    "pd.concat([train_X2.head(3), train_y.head(3)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compose a pipeline, trainable, with two steps, RFE and NMF, where the\n",
    "output of RFE is piped into the input of NMF. NMF is is non-negative\n",
    "matrix factorization and requires a non-negative matrix as its input.\n",
    "When we try to fit the pipeline, scikit-learn first spends some time\n",
    "to fit the upstream RFE. After that, it attempts to fit the NMF on the\n",
    "output from RFE. The output from RFE contains negative numbers, and\n",
    "therefore, NMF throws an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn.pipeline\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable = sklearn.pipeline.make_pipeline(\n",
    "    RFE(estimator=Forest(n_estimators=10)), NMF())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.59 s, sys: 172 ms, total: 4.77 s\n",
      "Wall time: 4.86 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Negative values in data passed to NMF (input X)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    trainable.fit(train_X, train_y)\n",
    "except ValueError as e:\n",
    "    message = str(e)\n",
    "print(message, file=sys.stderr)\n",
    "assert message == 'Negative values in data passed to NMF (input X)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lale version\n",
    "\n",
    "Lale supports JSON schema validation of pipelines. We compose the same\n",
    "trainable pipeline of RFE and NMF as before. But rather than trying to\n",
    "fit it, we call validate_schema. This checks the schemas at each step\n",
    "of the pipeline, and detects that the output from RFE is not a\n",
    "subschema of the input to NMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lale.operators\n",
    "import lale.helpers\n",
    "from lale.lib.sklearn import RFE, NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable = lale.operators.make_pipeline(\n",
    "    RFE(estimator=Forest(n_estimators=10)), NMF())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 93.8 ms, sys: 0 ns, total: 93.8 ms\n",
      "Wall time: 96.6 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected to_schema(data) to be a subschema of NMF.input_schema_fit().\n",
      "to_schema(data) = {\n",
      "    'type': 'object',\n",
      "    'additionalProperties': false,\n",
      "    'required': ['X', 'y'],\n",
      "    'properties': {\n",
      "        'X': {\n",
      "            '$schema': 'http://json-schema.org/draft-04/schema#',\n",
      "            'description': 'The input samples with only the selected features.',\n",
      "            'type': 'array',\n",
      "            'items': {\n",
      "                'type': 'array',\n",
      "                'items': {\n",
      "                    'type': 'number'}}},\n",
      "        'y': {\n",
      "            '$schema': 'http://json-schema.org/draft-04/schema#',\n",
      "            'type': 'array',\n",
      "            'minItems': 16512,\n",
      "            'maxItems': 16512,\n",
      "            'items': {\n",
      "                'description': 'target',\n",
      "                'type': 'number'}}},\n",
      "}\n",
      "NMF.input_schema_fit() = {\n",
      "    '$schema': 'http://json-schema.org/draft-04/schema#',\n",
      "    'type': 'object',\n",
      "    'required': ['X'],\n",
      "    'additionalProperties': false,\n",
      "    'properties': {\n",
      "        'X': {\n",
      "            'type': 'array',\n",
      "            'items': {\n",
      "                'type': 'array',\n",
      "                'items': {\n",
      "                    'type': 'number',\n",
      "                    'minimum': 0.0}},\n",
      "            'description': 'Data matrix to be decomposed.'},\n",
      "        'y': {}},\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    trainable.validate_schema(schema_X, schema_y)\n",
    "except lale.helpers.SubschemaError as e:\n",
    "    message = str(e)\n",
    "print(message, file=sys.stderr)\n",
    "assert message.find('to be a subschema of NMF.input_schema_fit()') != -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter error example in middle of pipeline\n",
    "\n",
    "First, we load the drugs.com dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: train_X (161297, 5), train_y (161297,)\n"
     ]
    }
   ],
   "source": [
    "from lale.datasets.uci.uci_datasets import fetch_drugscom\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "train_X, train_y, test_X, test_y = fetch_drugscom()\n",
    "print(f'shapes: train_X {train_X.shape}, train_y {train_y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn version\n",
    "\n",
    "This example gives a baseline, it uses only scikit-learn, not Lale.\n",
    "First, import a few things from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf\n",
    "from sklearn.linear_model import LogisticRegression as LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, instantiate a trainable pipeline that applies Tfidf on the\n",
    "`'review'` column of the input data, followed by LogisticRegression.\n",
    "Since there is no training happening, this is very fast. However,\n",
    "there is a mistake in this code: LR does not support `solver='adam'`.\n",
    "Unfortunately, scikit-learn does not report the error at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 355 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainable = make_pipeline(\n",
    "    ColumnTransformer([\n",
    "        ('txt', Tfidf(max_features=1000), 'review')]),\n",
    "    LR(solver='adam'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, try to train that pipeline. The Tfidf gets trained first,\n",
    "because training LR requires the data as transformed by the trained\n",
    "Tfidf. In this example, training Tfidf is slow. And because of the\n",
    "mistake with `solver='adam'` in the previous cell, training LR fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.7 s, sys: 1.62 s, total: 15.3 s\n",
      "Wall time: 15.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logistic Regression supports only solvers in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'], got adam.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    trainable.fit(train_X)\n",
    "except ValueError as e:\n",
    "    message = str(e)\n",
    "print(message, file=sys.stderr)\n",
    "assert message.startswith('Logistic Regression supports only solvers in')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lale version\n",
    "\n",
    "First, import the Project operator from Lale, and wrap the imported\n",
    "scikit-learn operators (LR and Tfidf) to augment them with JSON\n",
    "schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lale.lib.lale import Project\n",
    "import lale.helpers\n",
    "lale.helpers.wrap_imported_operators()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, try to instantiate a pipeline as before. In particular, the\n",
    "code has the same mistake as before, passing `solver='adam'` to\n",
    "LR. But unlike in pure scikit-learn, here, the mistake gets caught\n",
    "earlier, using JSON Schema validation when the operators are\n",
    "instantiated. For this example, that saves a lot of time, since\n",
    "there is no need to train Tfidf to catch the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.2 ms, sys: 15.6 ms, total: 46.9 ms\n",
      "Wall time: 35.7 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid configuration for LR(solver='adam') due to invalid value solver=adam.\n",
      "Schema of argument solver: {\n",
      "    'description': 'Algorithm for optimization problem.',\n",
      "    'enum': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
      "    'default': 'liblinear',\n",
      "}\n",
      "Value: adam\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from jsonschema import ValidationError\n",
    "try:\n",
    "    trainable = (Project(columns=['review'])\n",
    "              >> Tfidf(max_features=1000)\n",
    "              >> LR(solver='adam'))\n",
    "except ValidationError as e:\n",
    "    message = e.message\n",
    "print(message, file=sys.stderr)\n",
    "assert message.startswith(\"Invalid configuration for LR(solver='adam')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive documentation\n",
    "\n",
    "Data scientists can interactively query the JSON Schemas of individual\n",
    "operators. For example, they can find out all the hyperparameters\n",
    "along with their defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solver': 'liblinear',\n",
       " 'penalty': 'l2',\n",
       " 'dual': False,\n",
       " 'C': 1.0,\n",
       " 'tol': 0.0001,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1.0,\n",
       " 'class_weight': None,\n",
       " 'random_state': None,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'ovr',\n",
       " 'verbose': 0,\n",
       " 'warm_start': False,\n",
       " 'n_jobs': None}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR.hyperparam_defaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a categorical hyperparameter, which JSON Schema\n",
    "represents using an enum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'Algorithm for optimization problem.',\n",
       " 'enum': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
       " 'default': 'liblinear'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR.hyperparam_schema('solver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a continuous hyperparameter, which JSON Schema\n",
    "represents as a number. Th `'minimum'` specifies what is valid (will\n",
    "not raise an exception), whereas the `'minimumForOptimizer'` and\n",
    "`'maximumForOptimizer'` specify what is relevant (a range that makes\n",
    "sense for automated search tools)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'Inverse regularization strength. Smaller values specify stronger regularization.',\n",
       " 'type': 'number',\n",
       " 'distribution': 'loguniform',\n",
       " 'minimum': 0.0,\n",
       " 'exclusiveMinimum': True,\n",
       " 'default': 1.0,\n",
       " 'minimumForOptimizer': 0.03125,\n",
       " 'maximumForOptimizer': 32768}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR.hyperparam_schema('C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More hyperparameter error examples\n",
    "\n",
    "Since the schema of the `C` hyperparameter of `LR` specifies an\n",
    "exclusive minimum of zero, passing zero is not valid. Lale internally\n",
    "calls an off-the-shelf JSON Schema validator when an operator gets\n",
    "configured with concrete hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid configuration for LR(C=0.0) due to invalid value C=0.0.\n",
      "Schema of argument C: {\n",
      "    'description': 'Inverse regularization strength. Smaller values specify stronger regularization.',\n",
      "    'type': 'number',\n",
      "    'distribution': 'loguniform',\n",
      "    'minimum': 0.0,\n",
      "    'exclusiveMinimum': true,\n",
      "    'default': 1.0,\n",
      "    'minimumForOptimizer': 0.03125,\n",
      "    'maximumForOptimizer': 32768,\n",
      "}\n",
      "Value: 0.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    LR(C=0.0)\n",
    "except ValidationError as e:\n",
    "    message = e.message\n",
    "print(message, file=sys.stderr)\n",
    "assert message.startswith('Invalid configuration for LR(C=0.0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides per-hyperparameter types, there are also conditional\n",
    "inter-hyperparameter constraints. These are checked using the\n",
    "same call to an off-the-shelf JSON Schema validator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid configuration for LR(solver='sag', penalty='l1') due to constraint the newton-cg, sag, and lbfgs solvers support only l2 penalties.\n",
      "Schema of constraint 1: {\n",
      "    'description': 'The newton-cg, sag, and lbfgs solvers support only l2 penalties.',\n",
      "    'anyOf': [{\n",
      "        'type': 'object',\n",
      "        'properties': {\n",
      "            'solver': {\n",
      "                'not': {\n",
      "                    'enum': ['newton-cg', 'sag', 'lbfgs']}}}}, {\n",
      "        'type': 'object',\n",
      "        'properties': {\n",
      "            'penalty': {\n",
      "                'enum': ['l2']}}}],\n",
      "}\n",
      "Value: {'solver': 'sag', 'penalty': 'l1', 'dual': False, 'C': 1.0, 'tol': 0.0001, 'fit_intercept': True, 'intercept_scaling': 1.0, 'class_weight': None, 'random_state': None, 'max_iter': 100, 'multi_class': 'ovr', 'verbose': 0, 'warm_start': False, 'n_jobs': None}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    LR(LR.solver.sag, LR.penalty.l1)\n",
    "except ValidationError as e:\n",
    "    message = e.message\n",
    "print(message, file=sys.stderr)\n",
    "assert message.find('support only l2 penalties') != -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are even constraints that affect three different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid configuration for LR(penalty='l2', solver='sag', dual=True) due to constraint the dual formulation is only implemented for l2 penalty with the liblinear solver.\n",
      "Schema of constraint 2: {\n",
      "    'description': 'The dual formulation is only implemented for l2 penalty with the liblinear solver.',\n",
      "    'anyOf': [{\n",
      "        'type': 'object',\n",
      "        'properties': {\n",
      "            'dual': {\n",
      "                'enum': [false]}}}, {\n",
      "        'type': 'object',\n",
      "        'properties': {\n",
      "            'penalty': {\n",
      "                'enum': ['l2']},\n",
      "            'solver': {\n",
      "                'enum': ['liblinear']}}}],\n",
      "}\n",
      "Value: {'penalty': 'l2', 'solver': 'sag', 'dual': True, 'C': 1.0, 'tol': 0.0001, 'fit_intercept': True, 'intercept_scaling': 1.0, 'class_weight': None, 'random_state': None, 'max_iter': 100, 'multi_class': 'ovr', 'verbose': 0, 'warm_start': False, 'n_jobs': None}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    LR(LR.penalty.l2, LR.solver.sag, dual=True)\n",
    "except ValidationError as e:\n",
    "    message = e.message\n",
    "print(message, file=sys.stderr)\n",
    "assert message.find('dual formulation is only implemented for') != -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset error example for individual operator\n",
    "\n",
    "Lale uses JSON Schema validation not only for hyperparameters but also\n",
    "for data. The dataset `train_X` is multimodal: some columns contain\n",
    "text strings whereas others contain numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$schema': 'http://json-schema.org/draft-04/schema#',\n",
       " 'type': 'array',\n",
       " 'items': {'type': 'array',\n",
       "  'minItems': 5,\n",
       "  'maxItems': 5,\n",
       "  'items': [{'description': 'drugName', 'type': 'string'},\n",
       "   {'description': 'condition',\n",
       "    'anyOf': [{'type': 'string'}, {'enum': [nan]}]},\n",
       "   {'description': 'review', 'type': 'string'},\n",
       "   {'description': 'date', 'type': 'string'},\n",
       "   {'description': 'usefulCount', 'type': 'integer', 'minimum': 0}]},\n",
       " 'minItems': 161297,\n",
       " 'maxItems': 161297}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.data_schemas.to_schema(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `train_X` contains strings but `LR` expects only numbers, the\n",
    "call to `fit` reports a type error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed validating input_schema_fit for LR due to 'Valsartan' is not of type 'number'\n",
      "\n",
      "Failed validating 'type' in schema['properties']['X']['items']['items']:\n",
      "    {'type': 'number'}\n",
      "\n",
      "On instance['X'][0][0]:\n",
      "    'Valsartan'\n"
     ]
    }
   ],
   "source": [
    "trainable_lr = LR()\n",
    "try:\n",
    "    trainable_lr.fit(train_X, train_y)\n",
    "except ValidationError as e:\n",
    "    message = e.message\n",
    "print(message, file=sys.stderr)\n",
    "assert message.startswith('Failed validating input_schema_fit for LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a pure numerical dataset instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.0               3.4                1.6               0.4\n",
       "1                6.3               3.3                4.7               1.6\n",
       "2                5.1               3.4                1.5               0.2\n",
       "3                4.8               3.0                1.4               0.1\n",
       "4                6.7               3.1                4.7               1.5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lale.datasets import load_iris_df\n",
    "(train_X, train_y), (test_X, test_y) = load_iris_df()\n",
    "train_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training LR with the Iris dataset works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trained_lr = trainable_lr.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lifecycle error example\n",
    "\n",
    "Lale encourages separating the lifecycle states, here represented\n",
    "by `trainable_lr` vs. `trained_lr`. The `predict` method should\n",
    "only be called on a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_y    [2, 1, 1, 0, 2, 0, 1, 1, 0, 0, 1, 0, 1, 1, 2, 0, 2, 1, 1, 0, 0, 2, 2, 0, 2, 1, 0, 2, 1, 0]\n",
      "predicted [2, 1, 1, 0, 2, 0, 1, 1, 0, 0, 1, 0, 1, 1, 2, 0, 2, 1, 1, 0, 0, 2, 2, 0, 2, 1, 0, 2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "predicted = trained_lr.predict(test_X)\n",
    "print(f'test_y    {[*test_y]}')\n",
    "print(f'predicted {[*predicted]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, the `predict` method should not be called on a trainable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_y    [2, 1, 1, 0, 2, 0, 1, 1, 0, 0, 1, 0, 1, 1, 2, 0, 2, 1, 1, 0, 0, 2, 2, 0, 2, 1, 0, 2, 1, 0]\n",
      "predicted [2, 1, 1, 0, 2, 0, 1, 1, 0, 0, 1, 0, 1, 1, 2, 0, 2, 1, 1, 0, 0, 2, 2, 0, 2, 1, 0, 2, 1, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `predict` method is deprecated on a trainable operator, because the learned coefficients could be accidentally overwritten by retraining. Call `predict` on the trained operator returned by `fit` instead.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"error\", category=DeprecationWarning)\n",
    "try:\n",
    "    predicted = trainable_lr.predict(test_X)\n",
    "except DeprecationWarning as w:\n",
    "    message = str(w)\n",
    "print(message, file=sys.stderr)\n",
    "assert message.startswith('The `predict` method is deprecated on a trainable')\n",
    "print(f'test_y    {[*test_y]}')\n",
    "print(f'predicted {[*predicted]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
