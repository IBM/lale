{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: train_X (161297, 5), train_y (161297,)\n"
     ]
    }
   ],
   "source": [
    "from lale.datasets.uci.uci_datasets import fetch_drugscom\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "train_X, train_y, test_X, test_y = fetch_drugscom()\n",
    "print(f'shapes: train_X {train_X.shape}, train_y {train_y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn error example\n",
    "\n",
    "This example gives a baseline, it uses only scikit-learn, not Lale.\n",
    "First, import a few things from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf\n",
    "from sklearn.linear_model import LogisticRegression as LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, instantiate a trainable pipeline that applies Tfidf on the\n",
    "`'review'` column of the input data, followed by LogisticRegression.\n",
    "Since there is no training happening, this is very fast. However,\n",
    "there is a mistake in this code: LR does not support `solver='adam'`.\n",
    "Unfortunately, scikit-learn does not report the error at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 259 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainable = make_pipeline(\n",
    "    ColumnTransformer([\n",
    "        ('txt', Tfidf(max_features=1000), 'review')]),\n",
    "    LR(solver='adam'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, try to train that pipeline. The Tfidf gets trained first,\n",
    "because training LR requires the data as transformed by the trained\n",
    "Tfidf. In this example, training Tfidf is slow. And because of the\n",
    "mistake with `solver='adam'` in the previous cell, training LR fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.2 s, sys: 1.2 s, total: 13.4 s\n",
      "Wall time: 13.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logistic Regression supports only solvers in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'], got adam.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    trainable.fit(train_X)\n",
    "except ValueError as e:\n",
    "    print(e, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The same example in Lale\n",
    "\n",
    "First, import the Project operator from Lale, and wrap the imported\n",
    "scikit-learn operators (LR and Tfidf) to augment them with JSON\n",
    "schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lale.lib.lale import Project\n",
    "import lale.helpers\n",
    "lale.helpers.wrap_imported_operators()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, try to instantiate a pipeline as before. In particular, the\n",
    "code has the same mistake as before, passing `solver='adam'` to\n",
    "LR. But unlike in pure scikit-learn, here, the mistake gets caught\n",
    "earlier, using JSON Schema validation when the operators are\n",
    "instantiated. For this example, that saves a lot of time, since\n",
    "there is no need to train Tfidf to catch the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.9 ms, sys: 0 ns, total: 46.9 ms\n",
      "Wall time: 41.2 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid configuration for LR(solver='adam') due to invalid value solver=adam.\n",
      "Schema of argument solver: {\n",
      "    'description': 'Algorithm for optimization problem.',\n",
      "    'enum': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
      "    'default': 'liblinear',\n",
      "}\n",
      "Value: adam\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from jsonschema import ValidationError\n",
    "try:\n",
    "    trainable = (Project(columns=['review'])\n",
    "              >> Tfidf(max_features=1000)\n",
    "              >> LR(solver='adam'))\n",
    "except ValidationError as e:\n",
    "    print(e.message, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive documentation\n",
    "\n",
    "Data scientists can interactively query the JSON Schemas of individual\n",
    "operators. For example, they can find out all the hyperparameters\n",
    "along with their defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solver': 'liblinear',\n",
       " 'penalty': 'l2',\n",
       " 'dual': False,\n",
       " 'C': 1.0,\n",
       " 'tol': 0.0001,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1.0,\n",
       " 'class_weight': None,\n",
       " 'random_state': None,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'ovr',\n",
       " 'verbose': 0,\n",
       " 'warm_start': False,\n",
       " 'n_jobs': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR.hyperparam_defaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a categorical hyperparameter, which JSON Schema\n",
    "represents using an enum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'Algorithm for optimization problem.',\n",
       " 'enum': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
       " 'default': 'liblinear'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR.hyperparam_schema('solver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a continuous hyperparameter, which JSON Schema\n",
    "represents as a number. Th `'minimum'` specifies what is valid (will\n",
    "not raise an exception), whereas the `'minimumForOptimizer'` and\n",
    "`'maximumForOptimizer'` specify what is relevant (a range that makes\n",
    "sense for automated search tools)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'Inverse regularization strength. Smaller values specify stronger regularization.',\n",
       " 'type': 'number',\n",
       " 'distribution': 'loguniform',\n",
       " 'minimum': 0.0,\n",
       " 'exclusiveMinimum': True,\n",
       " 'default': 1.0,\n",
       " 'minimumForOptimizer': 0.03125,\n",
       " 'maximumForOptimizer': 32768}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR.hyperparam_schema('C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More hyperparameter error examples\n",
    "\n",
    "Since the schema of the `C` hyperparameter of `LR` specifies an\n",
    "exclusive minimum of zero, passing zero is not valid. Lale internally\n",
    "calls an off-the-shelf JSON Schema validator when an operator gets\n",
    "configured with concrete hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid configuration for LR(C=0.0) due to invalid value C=0.0.\n",
      "Schema of argument C: {\n",
      "    'description': 'Inverse regularization strength. Smaller values specify stronger regularization.',\n",
      "    'type': 'number',\n",
      "    'distribution': 'loguniform',\n",
      "    'minimum': 0.0,\n",
      "    'exclusiveMinimum': true,\n",
      "    'default': 1.0,\n",
      "    'minimumForOptimizer': 0.03125,\n",
      "    'maximumForOptimizer': 32768,\n",
      "}\n",
      "Value: 0.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    LR(C=0.0)\n",
    "except ValidationError as e:\n",
    "    print(e.message, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides per-hyperparameter types, there are also conditional\n",
    "inter-hyperparameter constraints. These are checked using the\n",
    "same call to an off-the-shelf JSON Schema validator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid configuration for LR(solver='sag', penalty='l1') due to constraint the newton-cg, sag, and lbfgs solvers support only l2 penalties.\n",
      "Schema of constraint 1: {\n",
      "    'description': 'The newton-cg, sag, and lbfgs solvers support only l2 penalties.',\n",
      "    'anyOf': [{\n",
      "        'type': 'object',\n",
      "        'properties': {\n",
      "            'solver': {\n",
      "                'not': {\n",
      "                    'enum': ['newton-cg', 'sag', 'lbfgs']}}}}, {\n",
      "        'type': 'object',\n",
      "        'properties': {\n",
      "            'penalty': {\n",
      "                'enum': ['l2']}}}],\n",
      "}\n",
      "Value: {'solver': 'sag', 'penalty': 'l1', 'dual': False, 'C': 1.0, 'tol': 0.0001, 'fit_intercept': True, 'intercept_scaling': 1.0, 'class_weight': None, 'random_state': None, 'max_iter': 100, 'multi_class': 'ovr', 'verbose': 0, 'warm_start': False, 'n_jobs': None}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    LR(LR.solver.sag, LR.penalty.l1)\n",
    "except ValidationError as e:\n",
    "    print(e.message, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are even constraints that affect three different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid configuration for LR(penalty='l2', solver='sag', dual=True) due to constraint the dual formulation is only implemented for l2 penalty with the liblinear solver.\n",
      "Schema of constraint 2: {\n",
      "    'description': 'The dual formulation is only implemented for l2 penalty with the liblinear solver.',\n",
      "    'anyOf': [{\n",
      "        'type': 'object',\n",
      "        'properties': {\n",
      "            'dual': {\n",
      "                'enum': [false]}}}, {\n",
      "        'type': 'object',\n",
      "        'properties': {\n",
      "            'penalty': {\n",
      "                'enum': ['l2']},\n",
      "            'solver': {\n",
      "                'enum': ['liblinear']}}}],\n",
      "}\n",
      "Value: {'penalty': 'l2', 'solver': 'sag', 'dual': True, 'C': 1.0, 'tol': 0.0001, 'fit_intercept': True, 'intercept_scaling': 1.0, 'class_weight': None, 'random_state': None, 'max_iter': 100, 'multi_class': 'ovr', 'verbose': 0, 'warm_start': False, 'n_jobs': None}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    LR(LR.penalty.l2, LR.solver.sag, dual=True)\n",
    "except ValidationError as e:\n",
    "    print(e.message, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset schema error examples\n",
    "\n",
    "Lale uses JSON Schema validation not only for hyperparameters but also\n",
    "for data. The dataset `train_X` is multimodal: some columns contain\n",
    "text strings whereas others contain numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$schema': 'http://json-schema.org/draft-04/schema#',\n",
       " 'type': 'array',\n",
       " 'items': {'type': 'array',\n",
       "  'minItems': 5,\n",
       "  'maxItems': 5,\n",
       "  'items': [{'description': 'drugName', 'type': 'string'},\n",
       "   {'description': 'condition',\n",
       "    'anyOf': [{'type': 'string'}, {'enum': [nan]}]},\n",
       "   {'description': 'review', 'type': 'string'},\n",
       "   {'description': 'date', 'type': 'string'},\n",
       "   {'description': 'usefulCount', 'type': 'integer', 'minimum': 0}]},\n",
       " 'minItems': 161297,\n",
       " 'maxItems': 161297}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lale.datasets import data_schemas\n",
    "data_schemas.to_schema(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `train_X` contains strings but `LR` expects only numbers, the\n",
    "call to `fit` reports a type error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed validating input_schema_fit for LR due to 'Valsartan' is not of type 'number'\n",
      "\n",
      "Failed validating 'type' in schema['properties']['X']['items']['items']:\n",
      "    {'type': 'number'}\n",
      "\n",
      "On instance['X'][0][0]:\n",
      "    'Valsartan'\n"
     ]
    }
   ],
   "source": [
    "trainable_lr = LR()\n",
    "try:\n",
    "    trainable_lr.fit(train_X, train_y)\n",
    "except ValidationError as e:\n",
    "    print(e.message, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a pure numerical dataset instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.0               3.4                1.6               0.4\n",
       "1                6.3               3.3                4.7               1.6\n",
       "2                5.1               3.4                1.5               0.2\n",
       "3                4.8               3.0                1.4               0.1\n",
       "4                6.7               3.1                4.7               1.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lale.datasets import load_iris_df\n",
    "(train_X, train_y), (test_X, test_y) = load_iris_df()\n",
    "train_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training LR with the Iris dataset works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trained_lr = trainable_lr.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lifecycle error example\n",
    "\n",
    "Lale encourages separating the lifecycle states, here represented\n",
    "by `trainable_lr` vs. `trained_lr`. The `predict` method should\n",
    "only be called on a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_y    [2, 1, 1, 0, 2, 0, 1, 1, 0, 0, 1, 0, 1, 1, 2, 0, 2, 1, 1, 0, 0, 2, 2, 0, 2, 1, 0, 2, 1, 0]\n",
      "predicted [2, 1, 1, 0, 2, 0, 1, 1, 0, 0, 1, 0, 1, 1, 2, 0, 2, 1, 1, 0, 0, 2, 2, 0, 2, 1, 0, 2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "predicted = trained_lr.predict(test_X)\n",
    "print(f'test_y    {[*test_y]}')\n",
    "print(f'predicted {[*predicted]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, the `predict` method should not be called on a trainable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_y    [2, 1, 1, 0, 2, 0, 1, 1, 0, 0, 1, 0, 1, 1, 2, 0, 2, 1, 1, 0, 0, 2, 2, 0, 2, 1, 0, 2, 1, 0]\n",
      "predicted [2, 1, 1, 0, 2, 0, 1, 1, 0, 0, 1, 0, 1, 1, 2, 0, 2, 1, 1, 0, 0, 2, 2, 0, 2, 1, 0, 2, 1, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `predict` method is deprecated on a trainable operator, because the learned coefficients could be accidentally overwritten by retraining. Call `predict` on the trained operator returned by `fit` instead.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"error\", category=DeprecationWarning)\n",
    "try:\n",
    "    predicted = trainable_lr.predict(test_X)\n",
    "except DeprecationWarning as w:\n",
    "    print(str(w), file=sys.stderr)\n",
    "print(f'test_y    {[*test_y]}')\n",
    "print(f'predicted {[*predicted]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
